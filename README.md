<div class="Box-sc-g0xbh4-0 bJMeLZ js-snippet-clipboard-copy-unpositioned" data-hpc="true"><article class="markdown-body entry-content container-lg" itemprop="text"><p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3D 机器学习</font></font></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">近年来，3D机器学习领域取得了巨大进展，它是一个融合了计算机视觉、计算机图形学和机器学习的跨学科领域。</font><font style="vertical-align: inherit;">该存储库源自我的学习笔记，将用作分类新研究论文的地方。</font></font></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">我将使用以下图标来&ZeroWidthSpace;&ZeroWidthSpace;区分 3D 表示：</font></font></p>
<ul dir="auto">
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">📷 多视图图像</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">👾 体积</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">🎲 点云</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">💎 多边形网格</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">💊 基于原始的</font></font></li>
</ul>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">要查找相关论文及其关系，请查看</font></font><a href="https://www.connectedpapers.com/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Connected Papers</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">，它提供了一种以图形表示形式可视化学术领域的简洁方法。</font></font></p>
<div class="markdown-heading" dir="auto"><h2 tabindex="-1" class="heading-element" dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">参与其中</font></font></h2><a id="user-content-get-involved" class="anchor" aria-label="永久链接：参与其中" href="#get-involved"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">要为此 Repo 做出贡献，您可以通过拉取请求添加内容或打开问题让我知道。</font></font></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">⭐ ⭐ ⭐ ⭐ ⭐ ⭐ ⭐ ⭐ ⭐ ⭐ ⭐ ⭐</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
我们还创建了一个 Slack 工作场所，供世界各地的人们提出问题、分享知识和促进合作。</font><font style="vertical-align: inherit;">我确信我们可以共同努力推进这一领域的发展。</font></font><a href="https://join.slack.com/t/3d-machine-learning/shared_invite/zt-4hsgj8zb-G6OKrBcc17QBB9ppYETgCQ" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">通过此链接</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">加入社区</font><font style="vertical-align: inherit;">。
</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">⭐⭐⭐⭐⭐⭐⭐⭐⭐⭐⭐⭐⭐⭐</font></font></p>
<div class="markdown-heading" dir="auto"><h2 tabindex="-1" class="heading-element" dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">目录</font></font></h2><a id="user-content-table-of-contents" class="anchor" aria-label="固定链接：目录" href="#table-of-contents"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li><a href="#courses"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">培训班</font></font></a></li>
<li><a href="#datasets"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">数据集</font></font></a>
<ul dir="auto">
<li><a href="#3d_models"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3D模型</font></font></a></li>
<li><a href="#3d_scenes"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3D场景</font></font></a></li>
</ul>
</li>
<li><a href="#pose_estimation"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3D 姿态估计</font></font></a></li>
<li><a href="#single_classification"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">单一对象分类</font></font></a></li>
<li><a href="#multiple_detection"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">多个物体检测</font></font></a></li>
<li><a href="#segmentation"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">场景/对象语义分割</font></font></a></li>
<li><a href="#3d_synthesis"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3D几何合成/重建</font></font></a>
<ul dir="auto">
<li><a href="#3d_synthesis_model_based"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">基于参数可变形模型的方法</font></font></a></li>
<li><a href="#3d_synthesis_template_based"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">基于部分的模板学习方法</font></font></a></li>
<li><a href="#3d_synthesis_dl_based"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">深度学习方法</font></font></a></li>
</ul>
</li>
<li><a href="#material_synthesis"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">纹理/材料分析与合成</font></font></a></li>
<li><a href="#style_transfer"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">风格学习和迁移</font></font></a></li>
<li><a href="#scene_synthesis"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">场景合成/重建</font></font></a></li>
<li><a href="#scene_understanding"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">场景理解</font></font></a></li>
</ul>
<a name="user-content-courses">
</a><div class="markdown-heading" dir="auto"><a><h2 tabindex="-1" class="heading-element" dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">可用课程</font></font></h2></a><a id="user-content-available-courses" class="anchor" aria-label="永久链接：可用课程" href="#available-courses"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><a href="http://web.stanford.edu/class/cs231a/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">斯坦福CS231A：计算机视觉——从3D重建到识别（2018年冬季）</font></font></a></p>
<p dir="auto"><a href="https://cse291-i.github.io/index.html" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">UCSD CSE291-I00：3D 数据机器学习（2018 年冬季）</font></font></a></p>
<p dir="auto"><a href="http://graphics.stanford.edu/courses/cs468-17-spring/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">斯坦福 CS468：3D 数据机器学习（2017 年春季）</font></font></a></p>
<p dir="auto"><a href="http://groups.csail.mit.edu/gdpgroup/6838_spring_2017.html" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">MIT 6.838：形状分析（2017 年春季）</font></font></a></p>
<p dir="auto"><a href="https://www.cs.princeton.edu/courses/archive/fall10/cos526/syllabus.php" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">普林斯顿 COS 526：高级计算机图形学（2010 年秋季）</font></font></a></p>
<p dir="auto"><a href="https://www.cs.princeton.edu/courses/archive/fall03/cs597D/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">普林斯顿 CS597：几何建模与分析（2003 年秋季）</font></font></a></p>
<p dir="auto"><a href="http://geometricdeeplearning.com/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">几何深度学习</font></font></a></p>
<p dir="auto"><a href="https://www.cs.princeton.edu/courses/archive/spring15/cos598A/cos598A.html#Estimating" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3D 理解论文集</font></font></a></p>
<p dir="auto"><a href="https://geometry.cs.ucl.ac.uk/workshops/creativeai/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">CreativeAI：图形深度学习</font></font></a></p>
<a name="user-content-datasets">
</a><div class="markdown-heading" dir="auto"><a><h2 tabindex="-1" class="heading-element" dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">数据集</font></font></h2></a><a id="user-content-datasets" class="anchor" aria-label="永久链接：数据集" href="#datasets"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">要查看 RGBD 数据集的调查，请查看 Michael Firman 的</font></font><a href="http://www.michaelfirman.co.uk/RGBDdatasets/index.html" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">收藏</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">以及相关论文</font></font><a href="https://arxiv.org/pdf/1604.00999.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">RGBD 数据集：过去、现在和未来</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。</font><font style="vertical-align: inherit;">点云库还有一个很好的数据集</font></font><a href="https://pointclouds.org/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">目录</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。</font></font></p>
<a name="user-content-3d_models">
</a><div class="markdown-heading" dir="auto"><a><h3 tabindex="-1" class="heading-element" dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3D模型</font></font></h3></a><a id="user-content-3d-models" class="anchor" aria-label="永久链接：3D 模型" href="#3d-models"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Princeton Shape Benchmark (2003) </font></font></b> <a href="http://shape.cs.princeton.edu/benchmark/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[链接]</font></font></a>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">从网络收集的 .OFF 格式的 1,814 个模型。</font><font style="vertical-align: inherit;">用于评估基于形状的检索和分析算法。</font></font></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/3D-Machine-Learning/blob/master/imgs/Princeton%20Shape%20Benchmark%20(2003).jpeg"><img width="50%" src="https://github.com/timzhang642/3D-Machine-Learning/raw/master/imgs/Princeton%20Shape%20Benchmark%20(2003).jpeg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">IKEA 3D 模型和对齐图像的数据集 (2013) </font></font></b> <a href="http://ikea.csail.mit.edu/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[链接]</font></font></a>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 759 个图像和 219 个模型，包括 Sketchup (skp) 和 Wavefront (obj) 文件，适合姿势估计。</font></font></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/c0b9e6ba63008552d0bc6a9aae9e2df9c5574cd593890b46a75a21492257fdfa/687474703a2f2f696b65612e637361696c2e6d69742e6564752f7765625f696d672f696b65615f6f626a6563742e706e67"><img width="50%" src="https://camo.githubusercontent.com/c0b9e6ba63008552d0bc6a9aae9e2df9c5574cd593890b46a75a21492257fdfa/687474703a2f2f696b65612e637361696c2e6d69742e6564752f7765625f696d672f696b65615f6f626a6563742e706e67" data-canonical-src="http://ikea.csail.mit.edu/web_img/ikea_object.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">开放表面：带有丰富注释的表面外观目录 (SIGGRAPH 2013) </font></font></b> <a href="http://opensurfaces.cs.cornell.edu/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[链接]</font></font></a>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> OpenSurfaces 是一个大型数据库，其中包含根据真实世界消费者照片创建的带注释表面。</font><font style="vertical-align: inherit;">我们的注释框架利用众包来分割照片中的表面，然后用丰富的表面属性（包括材质、纹理和上下文信息）对其进行注释。</font></font></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/78a824eb6bec2d03b31071f0250cfc824fe1b451fa764e03f1244939f9bb7ac7/687474703a2f2f6f70656e73757266616365732e63732e636f726e656c6c2e6564752f7374617469632f696d672f746561736572342d7765622e6a7067"><img width="50%" src="https://camo.githubusercontent.com/78a824eb6bec2d03b31071f0250cfc824fe1b451fa764e03f1244939f9bb7ac7/687474703a2f2f6f70656e73757266616365732e63732e636f726e656c6c2e6564752f7374617469632f696d672f746561736572342d7765622e6a7067" data-canonical-src="http://opensurfaces.cs.cornell.edu/static/img/teaser4-web.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">PASCAL3D+ (2014) </font></font></b> <a href="http://cvgl.stanford.edu/projects/pascal3d.html" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[链接]</font></font></a>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 12 个类别，平均每个类别 3k+ 对象，用于 3D 对象检测和姿态估计。</font></font></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/3dc5a2ff8ad70d1efd8782ffa17926706a7949828448fe77015665b54bf0a184/687474703a2f2f6376676c2e7374616e666f72642e6564752f70726f6a656374732f70617363616c33642b2f70617363616c33642e706e67"><img width="50%" src="https://camo.githubusercontent.com/3dc5a2ff8ad70d1efd8782ffa17926706a7949828448fe77015665b54bf0a184/687474703a2f2f6376676c2e7374616e666f72642e6564752f70726f6a656374732f70617363616c33642b2f70617363616c33642e706e67" data-canonical-src="http://cvgl.stanford.edu/projects/pascal3d+/pascal3d.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ModelNet (2015) </font></font></b> <a href="http://modelnet.cs.princeton.edu/#" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[链接]</font></font></a>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 662 个类别的 127915 个 3D CAD 模型
</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ModelNet10：10 个类别的 4899 个模型
</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ModelNet40：40 个类别的 12311 个模型，全部统一定向</font></font></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/5c6a28c107e1f52bcb5169cb7d91fa3de24f640430050aa736db04bc525e48b3/687474703a2f2f3364766973696f6e2e7072696e6365746f6e2e6564752f70726f6a656374732f323031342f4d6f64656c4e65742f7468756d626e61696c2e6a7067"><img width="50%" src="https://camo.githubusercontent.com/5c6a28c107e1f52bcb5169cb7d91fa3de24f640430050aa736db04bc525e48b3/687474703a2f2f3364766973696f6e2e7072696e6365746f6e2e6564752f70726f6a656374732f323031342f4d6f64656c4e65742f7468756d626e61696c2e6a7067" data-canonical-src="http://3dvision.princeton.edu/projects/2014/ModelNet/thumbnail.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ShapeNet (2015) </font></font></b> <a href="https://www.shapenet.org/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[链接]</font></font></a>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 300 万+ 模型和 4K+ 类别。</font><font style="vertical-align: inherit;">数据集规模大、组织良好且注释丰富。
</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ShapeNetCore </font></font><a href="http://shapenet.cs.stanford.edu/shrec16/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[链接]</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">：55 个类别的 51300 个模型。</font></font></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/1535a42e8530e375d8d381fcda0f32353b9e3076e82a3e160da6d8da2f3926ff/687474703a2f2f6d73617676612e6769746875622e696f2f66696c65732f73686170656e65742e706e67"><img width="50%" src="https://camo.githubusercontent.com/1535a42e8530e375d8d381fcda0f32353b9e3076e82a3e160da6d8da2f3926ff/687474703a2f2f6d73617676612e6769746875622e696f2f66696c65732f73686170656e65742e706e67" data-canonical-src="http://msavva.github.io/files/shapenet.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">对象扫描大型数据集 (2016) </font></font></b> <a href="http://redwood-data.org/3dscan/index.html" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[链接]</font></font></a>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> RGBD 格式的 10K 扫描 + .PLY 格式的重建 3D 模型。</font></font></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/10abbe37fdf1f02a2429e11a1f509e5e802529c9bf0b2db8405d09a48d51fa26/687474703a2f2f726564776f6f642d646174612e6f72672f33647363616e2f696d672f7465617365722e6a7067"><img width="50%" src="https://camo.githubusercontent.com/10abbe37fdf1f02a2429e11a1f509e5e802529c9bf0b2db8405d09a48d51fa26/687474703a2f2f726564776f6f642d646174612e6f72672f33647363616e2f696d672f7465617365722e6a7067" data-canonical-src="http://redwood-data.org/3dscan/img/teaser.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ObjectNet3D：用于 3D 对象识别的大型数据库 (2016) </font></font></b> <a href="http://cvgl.stanford.edu/projects/objectnet3d/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[链接]</font></font></a>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 100 个类别、90,127 个图像、这些图像中的 201,888 个对象和 44,147 个 3D 形状。
</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">任务：区域提议生成、2D 对象检测、联合 2D 检测和 3D 对象姿态估计以及基于图像的 3D 形状检索</font></font></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/a676dbc689f8b956bf0f40fb33c80291ba724047d0aef090ab5e3f96a3df142d/687474703a2f2f6376676c2e7374616e666f72642e6564752f70726f6a656374732f6f626a6563746e657433642f4f626a6563744e657433442e706e67"><img width="50%" src="https://camo.githubusercontent.com/a676dbc689f8b956bf0f40fb33c80291ba724047d0aef090ab5e3f96a3df142d/687474703a2f2f6376676c2e7374616e666f72642e6564752f70726f6a656374732f6f626a6563746e657433642f4f626a6563744e657433442e706e67" data-canonical-src="http://cvgl.stanford.edu/projects/objectnet3d/ObjectNet3D.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Thingi10K：10,000 个 3D 打印模型的数据集（2016）</font></font></b> <a href="https://ten-thousand-models.appspot.com/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[链接]</font></font></a>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">来自 thingiverse.com 上特色“事物”的 10,000 个模型，适合测试 3D 打印技术，例如结构分析、形状优化或实体几何操作。</font></font></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/38e26b17fc1ff1cace6845d0594d6aeaf381dfa91d3972cd009896d938df85fa/68747470733a2f2f7062732e7477696d672e636f6d2f6d656469612f44526278576e71586b4145454830672e6a70673a6c61726765"><img width="50%" src="https://camo.githubusercontent.com/38e26b17fc1ff1cace6845d0594d6aeaf381dfa91d3972cd009896d938df85fa/68747470733a2f2f7062732e7477696d672e636f6d2f6d656469612f44526278576e71586b4145454830672e6a70673a6c61726765" data-canonical-src="https://pbs.twimg.com/media/DRbxWnqXkAEEH0g.jpg:large" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ABC：用于几何深度学习的大型 CAD 模型数据集</font></font></b> <a href="https://cs.nyu.edu/~zhongshi/publication/abc-dataset/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[链接] </font></font></a><a href="https://arxiv.org/abs/1812.06216" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">这项工作介绍了一个用于几何深度学习的数据集，由超过 100 万个单独（且高质量）的几何模型组成，每个模型都与分解的准确地面实况信息相关联分为补丁、明确的清晰特征注释和分析微分属性。</font></font><br></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/7037b58b9c594b6c5ff6c8756bb5417bf2cfbd6d7db31311850d53e12c23f1d8/68747470733a2f2f63732e6e79752e6564752f7e7a686f6e677368692f696d672f6162632d646174617365742e706e67"><img width="50%" src="https://camo.githubusercontent.com/7037b58b9c594b6c5ff6c8756bb5417bf2cfbd6d7db31311850d53e12c23f1d8/68747470733a2f2f63732e6e79752e6564752f7e7a686f6e677368692f696d672f6162632d646174617365742e706e67" data-canonical-src="https://cs.nyu.edu/~zhongshi/img/abc-dataset.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">🎲 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ScanObjectNN：真实世界数据的新基准数据集和分类模型（ICCV 2019）</font></font></b> <a href="https://hkust-vgd.github.io/scanobjectnn/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[链接]</font></font></a>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
这项工作介绍了 ScanObjectNN，一个基于扫描室内场景数据的新真实世界点云对象数据集。</font><font style="vertical-align: inherit;">这项工作中的综合基准表明，该数据集对现有的点云分类技术提出了巨大的挑战，因为来自现实世界扫描的对象通常与背景杂乱和/或由于遮挡而部分化。</font><font style="vertical-align: inherit;">确定了点云对象分类的三个关键开放问题，并提出了一种新的点云分类神经网络，该网络在对杂乱背景的对象进行分类时实现了最先进的性能。
</font></font><br></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/cce7f52aa3804df95abaf01b3472ee542ed9f8f0fbd9804febfe57a19ff1eb5b/68747470733a2f2f686b7573742d7667642e6769746875622e696f2f7363616e6f626a6563746e6e2f696d616765732f6f626a656374735f7465617365722e706e67"><img width="50%" src="https://camo.githubusercontent.com/cce7f52aa3804df95abaf01b3472ee542ed9f8f0fbd9804febfe57a19ff1eb5b/68747470733a2f2f686b7573742d7667642e6769746875622e696f2f7363616e6f626a6563746e6e2f696d616765732f6f626a656374735f7465617365722e706e67" data-canonical-src="https://hkust-vgd.github.io/scanobjectnn/images/objects_teaser.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">VOCASET：语音 4D 头部扫描数据集（2019 年（</font></font></b> <a href="https://voca.is.tue.mpg.de/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[链接] </font></font></a><a href="https://ps.is.tuebingen.mpg.de/uploads_file/attachment/attachment/510/paper_final.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a>
<br><a href="https://voca.is.tue.mpg.de/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">VOCASET</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">）是一个 4D 人脸数据集，包含以 60 fps 捕获的约 29 分钟的 4D 扫描和同步音频。该数据集包含 12 个主题和 480 个约 3 的序列-4 秒，每个句子从一系列标准协议中选择，最大限度地提高语音多样性。</font></font></p>
<p align="center" dir="auto"><animated-image data-catalyst="" style="width: 50%;"><a target="_blank" rel="noopener noreferrer" href="https://github.com/TimoBolkart/voca/blob/master/gif/vocaset.gif" data-target="animated-image.originalLink"><img src="https://github.com/TimoBolkart/voca/raw/master/gif/vocaset.gif" style="max-width: 100%; display: inline-block;" data-target="animated-image.originalImage"></a>
      <span class="AnimatedImagePlayer" data-target="animated-image.player" hidden="">
        <a data-target="animated-image.replacedLink" class="AnimatedImagePlayer-images" href="https://github.com/TimoBolkart/voca/blob/master/gif/vocaset.gif" target="_blank">
          
        <span data-target="animated-image.imageContainer">
            <img data-target="animated-image.replacedImage" alt="语音集.gif" class="AnimatedImagePlayer-animatedImage" src="https://github.com/TimoBolkart/voca/raw/master/gif/vocaset.gif" style="display: block; opacity: 1;">
          <canvas class="AnimatedImagePlayer-stillImage" aria-hidden="true" width="399" height="71"></canvas></span></a>
        <button data-target="animated-image.imageButton" class="AnimatedImagePlayer-images" tabindex="-1" aria-label="播放 vocaset.gif" hidden=""></button>
        <span class="AnimatedImagePlayer-controls" data-target="animated-image.controls" hidden="">
          <button data-target="animated-image.playButton" class="AnimatedImagePlayer-button" aria-label="播放 vocaset.gif">
            <svg aria-hidden="true" focusable="false" class="octicon icon-play" width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg">
              <path d="M4 13.5427V2.45734C4 1.82607 4.69692 1.4435 5.2295 1.78241L13.9394 7.32507C14.4334 7.63943 14.4334 8.36057 13.9394 8.67493L5.2295 14.2176C4.69692 14.5565 4 14.1739 4 13.5427Z">
            </path></svg>
            <svg aria-hidden="true" focusable="false" class="octicon icon-pause" width="16" height="16" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg">
              <rect x="4" y="2" width="3" height="12" rx="1"></rect>
              <rect x="9" y="2" width="3" height="12" rx="1"></rect>
            </svg>
          </button>
          <a data-target="animated-image.openButton" aria-label="在新窗口中打开 vocaset.gif" class="AnimatedImagePlayer-button" href="https://github.com/TimoBolkart/voca/blob/master/gif/vocaset.gif" target="_blank">
            <svg aria-hidden="true" class="octicon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" width="16" height="16">
              <path fill-rule="evenodd" d="M10.604 1h4.146a.25.25 0 01.25.25v4.146a.25.25 0 01-.427.177L13.03 4.03 9.28 7.78a.75.75 0 01-1.06-1.06l3.75-3.75-1.543-1.543A.25.25 0 0110.604 1zM3.75 2A1.75 1.75 0 002 3.75v8.5c0 .966.784 1.75 1.75 1.75h8.5A1.75 1.75 0 0014 12.25v-3.5a.75.75 0 00-1.5 0v3.5a.25.25 0 01-.25.25h-8.5a.25.25 0 01-.25-.25v-8.5a.25.25 0 01.25-.25h3.5a.75.75 0 000-1.5h-3.5z"></path>
            </svg>
          </a>
        </span>
      </span></animated-image></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3D-FUTURE：具有纹理的 3D 家具形状（2020）</font></font></b> <a href="https://tianchi.aliyun.com/specials/promotion/alibaba-3d-future?spm=5176.14208320.0.0.66293cf7asRnrR" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[链接] </font></font></a>
<br><a href="https://tianchi.aliyun.com/specials/promotion/alibaba-3d-future" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3D-FUTURE</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">包含 5,000 多个不同房间中的 20,000 多个干净、逼真的合成场景，其中包括 10,000 多个独特的高品质 3D 家具实例，其具有高分辨率信息纹理，由专业设计师。</font></font></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/5917280018a19b8c3c713e332789c0758c469799fa08f3820bf5e974bbe529b9/68747470733a2f2f696d672e616c6963646e2e636f6d2f7466732f544231485453667a347631674b306a535a464658586230735858612d313939392d313033372e706e67"><img width="50%" src="https://camo.githubusercontent.com/5917280018a19b8c3c713e332789c0758c469799fa08f3820bf5e974bbe529b9/68747470733a2f2f696d672e616c6963646e2e636f6d2f7466732f544231485453667a347631674b306a535a464658586230735858612d313939392d313033372e706e67" data-canonical-src="https://img.alicdn.com/tfs/TB1HTSfz4v1gK0jSZFFXXb0sXXa-1999-1037.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Fusion 360 Gallery 数据集 (2020) </font></font></b> <a href="https://github.com/AutodeskAILab/Fusion360GalleryDataset"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[链接] </font></font></a><a href="https://arxiv.org/abs/2010.02392" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Fusion </font></font><a href="https://github.com/AutodeskAILab/Fusion360GalleryDataset"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">360 Gallery 数据集</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">包含源自参数化 CAD 模型的丰富 2D 和 3D 几何数据。</font><font style="vertical-align: inherit;">重建数据集提供来自简单“草图和拉伸”设计子集的连续构造序列信息。</font><font style="vertical-align: inherit;">分割数据集提供基于 CAD 建模操作的 3D 模型分割，包括 B-Rep 格式、网格和点云。</font></font></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/AutodeskAILab/Fusion360GalleryDataset/master/docs/images/reconstruction_teaser.jpg"><img width="50%" src="https://raw.githubusercontent.com/AutodeskAILab/Fusion360GalleryDataset/master/docs/images/reconstruction_teaser.jpg" style="max-width: 100%;"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/AutodeskAILab/Fusion360GalleryDataset/master/docs/images/segmentation_example.jpg"><img width="50%" src="https://raw.githubusercontent.com/AutodeskAILab/Fusion360GalleryDataset/master/docs/images/segmentation_example.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">机械零部件基准 (2020) </font></font></b><a href="https://mechanical-components.herokuapp.com" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[链接] </font></font></a><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123630171.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a>
<br><a href="https://mechanical-components.herokuapp.com" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">MCB</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">是机械零部件 3D 对象的大型数据集。</font><font style="vertical-align: inherit;">其机械部件总数为68类，58696个。</font></font></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/c0f029c31149667aa41ad1864182eaf74f68a7fdb74950a33f27ce3157397a54/68747470733a2f2f6d656368616e6963616c2d636f6d706f6e656e74732e6865726f6b756170702e636f6d2f7374617469632f696d672f6d61696e5f6669677572652e706e67"><img width="50%" src="https://camo.githubusercontent.com/c0f029c31149667aa41ad1864182eaf74f68a7fdb74950a33f27ce3157397a54/68747470733a2f2f6d656368616e6963616c2d636f6d706f6e656e74732e6865726f6b756170702e636f6d2f7374617469632f696d672f6d61696e5f6669677572652e706e67" data-canonical-src="https://mechanical-components.herokuapp.com/static/img/main_figure.png" style="max-width: 100%;"></a>
</p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">组合 3D 形状数据集 (2020) </font></font></b> <a href="https://github.com/POSTECH-CVLab/Combinatorial-3D-Shape-Generation"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[链接] </font></font></a><a href="https://arxiv.org/abs/2004.07414" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a>
<br><a href="https://github.com/POSTECH-CVLab/Combinatorial-3D-Shape-Generation"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">组合 3D 形状数据集</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">由 14 个类的 406 个实例组成。</font><font style="vertical-align: inherit;">我们数据集中的每个对象都被认为相当于一系列原始放置。</font><font style="vertical-align: inherit;">与其他 3D 对象数据集相比，我们提出的数据集包含单元基元的组装序列。</font><font style="vertical-align: inherit;">这意味着我们可以快速获得一个顺序生成过程，这是一种人类组装机制。</font><font style="vertical-align: inherit;">此外，在验证采样序列后，我们可以从给定的组合形状中采样有效的随机序列。</font><font style="vertical-align: inherit;">总而言之，我们的组合 3D 形状数据集的特征是 (i) 组合、(ii) 顺序、(iii) 可分解和 (iv) 可操作。</font></font></p>
<p align="center" dir="auto">
<a target="_blank" rel="noopener noreferrer" href="/timzhang642/3D-Machine-Learning/blob/master/imgs/combinatorial_3d_shape_dataset.png"><img width="65%" src="/timzhang642/3D-Machine-Learning/raw/master/imgs/combinatorial_3d_shape_dataset.png" style="max-width: 100%;"></a>
</p>
<a name="user-content-3d_scenes">
</a><div class="markdown-heading" dir="auto"><a><h3 tabindex="-1" class="heading-element" dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3D场景</font></font></h3></a><a id="user-content-3d-scenes" class="anchor" aria-label="永久链接：3D 场景" href="#3d-scenes"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">纽约大学深度数据集 V2 (2012) </font></font></b> <a href="https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[链接]</font></font></a>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 1449 对来自 Kinect 视频序列的各种室内场景的密集标记的对齐 RGB 和深度图像。</font></font></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/116904c3a20b4b291a455911b0d7c08f5dedea1f3d608cba52707c2b60ad525b/68747470733a2f2f63732e6e79752e6564752f7e73696c6265726d616e2f696d616765732f6e79755f64657074685f76325f6c6162656c65642e6a7067"><img width="50%" src="https://camo.githubusercontent.com/116904c3a20b4b291a455911b0d7c08f5dedea1f3d608cba52707c2b60ad525b/68747470733a2f2f63732e6e79752e6564752f7e73696c6265726d616e2f696d616765732f6e79755f64657074685f76325f6c6162656c65642e6a7067" data-canonical-src="https://cs.nyu.edu/~silberman/images/nyu_depth_v2_labeled.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SUNRGB-D 3D 对象检测挑战</font></font></b> <a href="http://rgbd.cs.princeton.edu/challenge.html" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[链接]</font></font></a>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 19 个对象类别，用于预测现实世界维度中的 3D 边界框
</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">训练集：10,355 个 RGB-D 场景图像，测试集：2860 个 RGB-D 图像</font></font></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/cac458d231edcd66db102ada5fe36996a98f99f1d89daf4cf506653a93fe290a/687474703a2f2f726762642e63732e7072696e6365746f6e2e6564752f3364626f782e706e67"><img width="50%" src="https://camo.githubusercontent.com/cac458d231edcd66db102ada5fe36996a98f99f1d89daf4cf506653a93fe290a/687474703a2f2f726762642e63732e7072696e6365746f6e2e6564752f3364626f782e706e67" data-canonical-src="http://rgbd.cs.princeton.edu/3dbox.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SceneNN (2016) </font></font></b> <a href="http://www.scenenn.net/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[链接]</font></font></a>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 100 多个带有逐顶点和逐像素注释的室内场景网格。</font></font></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/51b645b285f8f1ae270a2e9cd67c85045a119832976ddeae32bf13092e3af0a3/68747470733a2f2f63646e2d616b2e662e73742d686174656e612e636f6d2f696d616765732f666f746f6c6966652f722f726f626f6e6368752f32303137303631312f32303137303631313135353632352e706e67"><img width="50%" src="https://camo.githubusercontent.com/51b645b285f8f1ae270a2e9cd67c85045a119832976ddeae32bf13092e3af0a3/68747470733a2f2f63646e2d616b2e662e73742d686174656e612e636f6d2f696d616765732f666f746f6c6966652f722f726f626f6e6368752f32303137303631312f32303137303631313135353632352e706e67" data-canonical-src="https://cdn-ak.f.st-hatena.com/images/fotolife/r/robonchu/20170611/20170611155625.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ScanNet (2017) </font></font></b> <a href="http://www.scan-net.org/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[链接]</font></font></a>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">一个 RGB-D 视频数据集，包含 1500 多次扫描中的 250 万个视图，并用 3D 相机姿势、表面重建和实例级语义分割进行注释。</font></font></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/1b94bc834d428e81fffd66be3e9e1b494c72e75c63ce258c622440dca035d268/687474703a2f2f7777772e7363616e2d6e65742e6f72672f696d672f616e6e6f746174696f6e732e706e67"><img width="50%" src="https://camo.githubusercontent.com/1b94bc834d428e81fffd66be3e9e1b494c72e75c63ce258c622440dca035d268/687474703a2f2f7777772e7363616e2d6e65742e6f72672f696d672f616e6e6f746174696f6e732e706e67" data-canonical-src="http://www.scan-net.org/img/annotations.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Matterport3D：从室内环境中的 RGB-D 数据中学习（2017 年）</font></font></b> <a href="https://niessner.github.io/Matterport/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[链接]</font></font></a>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">来自 90 个建筑规模私人房间场景的 194,400 张 RGB-D 图像的 10,800 个全景视图（RGB 和深度）。</font><font style="vertical-align: inherit;">为区域（客厅、厨房）和对象（沙发、电视）类别提供实例级语义分割。</font></font></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/07dcc72fdda0e8b3da96bcdea1d83ed71545a0bff410e2360274d251aa151d52/68747470733a2f2f6e696573736e65722e6769746875622e696f2f4d6174746572706f72742f7465617365722e706e67"><img width="50%" src="https://camo.githubusercontent.com/07dcc72fdda0e8b3da96bcdea1d83ed71545a0bff410e2360274d251aa151d52/68747470733a2f2f6e696573736e65722e6769746875622e696f2f4d6174746572706f72742f7465617365722e706e67" data-canonical-src="https://niessner.github.io/Matterport/teaser.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SUNCG：室内场景的大型 3D 模型存储库 (2017) </font></font></b> <a href="http://suncg.cs.princeton.edu/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[链接]</font></font></a>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">该数据集包含超过 45K 个不同的场景，以及手动创建的真实房间和家具布局。</font><font style="vertical-align: inherit;">所有场景都在对象级别进行语义注释。</font></font></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/80306adc4ecbb08f65842711042e0b7c9dea8e75b5d3e05558b693d4ef314add/687474703a2f2f73756e63672e63732e7072696e6365746f6e2e6564752f666967757265732f646174615f66756c6c2e706e67"><img width="50%" src="https://camo.githubusercontent.com/80306adc4ecbb08f65842711042e0b7c9dea8e75b5d3e05558b693d4ef314add/687474703a2f2f73756e63672e63732e7072696e6365746f6e2e6564752f666967757265732f646174615f66756c6c2e706e67" data-canonical-src="http://suncg.cs.princeton.edu/figures/data_full.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">MINOS：多模态室内模拟器（2017）</font></font></b> <a href="https://github.com/minosworld/minos"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[链接]</font></font></a>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> MINOS 是一款模拟器，旨在支持开发多感官模型，以在复杂的室内环境中实现目标导向导航。</font><font style="vertical-align: inherit;">MINOS 利用复杂 3D 环境的大型数据集，并支持多模式传感器套件的灵活配置。</font><font style="vertical-align: inherit;">MINOS 支持 SUNCG 和 Matterport3D 场景。</font></font></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/9203183d3989ae5f4c3dfca201ef2d114b9a63dfc65579b1e1677f4d403c7b5b/687474703a2f2f766c61646c656e2e696e666f2f77702d636f6e74656e742f75706c6f6164732f323031372f31322f4d494e4f532e6a7067"><img width="50%" src="https://camo.githubusercontent.com/9203183d3989ae5f4c3dfca201ef2d114b9a63dfc65579b1e1677f4d403c7b5b/687474703a2f2f766c61646c656e2e696e666f2f77702d636f6e74656e742f75706c6f6164732f323031372f31322f4d494e4f532e6a7067" data-canonical-src="http://vladlen.info/wp-content/uploads/2017/12/MINOS.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Facebook House3D：丰富而逼真的 3D 环境（2017）</font></font></b> <a href="https://github.com/facebookresearch/House3D"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[链接]</font></font></a>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> House3D 是一个虚拟 3D 环境，由 45K 个室内场景组成，配备了来自 SUNCG 数据集的各种场景类型、布局和对象。</font><font style="vertical-align: inherit;">所有 3D 对象均带有类别标签完整注释。</font><font style="vertical-align: inherit;">环境中的代理可以访问多种模式的观察结果，包括 RGB 图像、深度、分割掩模和自上而下的 2D 地图视图。</font></font></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/1381301/33509559-87c4e470-d6b7-11e7-8266-27c940d5729a.jpg"><img width="50%" src="https://user-images.githubusercontent.com/1381301/33509559-87c4e470-d6b7-11e7-8266-27c940d5729a.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">HoME：家庭多模式环境 (2017) </font></font></b> <a href="https://home-platform.github.io/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[链接]</font></font></a>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> HoME 基于 SUNCG 数据集集成了超过 45,000 个不同的 3D 房屋布局，该数据集可以促进学习、泛化和迁移。</font><font style="vertical-align: inherit;">HoME 是一个与 OpenAI Gym 兼容的开源平台，可扩展至强化学习、语言基础、基于声音的导航、机器人、多智能体学习等任务。</font></font></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/a1e63bbe8d0bcd2605cfb0396879815fcbb11bcc14e75de6aa5093d8fc197ab7/68747470733a2f2f686f6d652d706c6174666f726d2e6769746875622e696f2f6173736574732f6f766572766965772e706e67"><img width="50%" src="https://camo.githubusercontent.com/a1e63bbe8d0bcd2605cfb0396879815fcbb11bcc14e75de6aa5093d8fc197ab7/68747470733a2f2f686f6d652d706c6174666f726d2e6769746875622e696f2f6173736574732f6f766572766965772e706e67" data-canonical-src="https://home-platform.github.io/assets/overview.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">AI2-THOR：AI 代理的真实感交互环境</font></font></b> <a href="http://ai2thor.allenai.org/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[链接]</font></font></a>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> AI2-THOR 是 AI 代理的真实感交互框架。</font><font style="vertical-align: inherit;">THOR环境1.0版本共有120个场景，涵盖四种不同的房间类别：厨房、客厅、卧室和浴室。</font><font style="vertical-align: inherit;">每个房间都有许多可操作的对象。</font></font></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/3D-Machine-Learning/blob/master/imgs/AI2-Thor.jpeg"><img width="50%" src="https://github.com/timzhang642/3D-Machine-Learning/raw/master/imgs/AI2-Thor.jpeg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">UnrealCV：计算机视觉虚拟世界 (2017) </font></font></b> <a href="http://unrealcv.org/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[链接] </font></font></a><a href="http://www.idm.pku.edu.cn/staff/wangyizhou/papers/ACMMM2017_UnrealCV.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">一个开源项目，旨在帮助计算机视觉研究人员使用虚幻引擎 4 构建虚拟世界。</font></font></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/554bf4e807b63ece8f7cfb77698121a8eba366198a65340ca85c72de937148db/687474703a2f2f756e7265616c63762e6f72672f696d616765732f686f6d65706167655f7465617365722e706e67"><img width="50%" src="https://camo.githubusercontent.com/554bf4e807b63ece8f7cfb77698121a8eba366198a65340ca85c72de937148db/687474703a2f2f756e7265616c63762e6f72672f696d616765732f686f6d65706167655f7465617365722e706e67" data-canonical-src="http://unrealcv.org/images/homepage_teaser.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Gibson 环境：体现代理的真实世界感知 (2018 CVPR) </font></font></b> <a href="http://gibsonenv.stanford.edu/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[链接]</font></font></a>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">该平台提供来自 1000 个点云的 RGB，以及多模态传感器数据：表面法线、深度以及一小部分空间、语义对象注释。</font><font style="vertical-align: inherit;">该环境也已为 RL 做好准备，并集成了物理功能。</font><font style="vertical-align: inherit;">使用此类数据集可以进一步缩小虚拟环境与现实世界之间的差异。</font></font></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/3D-Machine-Learning/blob/master/imgs/Gibson%20Environment-%20Real-World%20Perception%20for%20Embodied%20Agents%20(2018%20CVPR)%20.jpeg"><img width="50%" src="https://github.com/timzhang642/3D-Machine-Learning/raw/master/imgs/Gibson%20Environment-%20Real-World%20Perception%20for%20Embodied%20Agents%20(2018%20CVPR)%20.jpeg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">InteriorNet：超大规模多传感器逼真室内场景数据集</font></font></b> <a href="https://interiornet.org/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[链接]</font></font></a>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">系统概述：用于渲染 RGB-D 惯性基准的端到端管道，用于大规模室内场景理解和映射。</font><font style="vertical-align: inherit;">我们的数据集包含由管道创建的 2000 万张图像：(A) 我们收集了世界领先的家具制造商提供的约 100 万个 CAD 模型。</font><font style="vertical-align: inherit;">这些模型已用于实际生产。</font><font style="vertical-align: inherit;">(B) 基于这些模型，大约 1,100 名专业设计师创建了大约 2200 万种室内布局。</font><font style="vertical-align: inherit;">大多数此类布局已用于现实世界的装饰中。</font><font style="vertical-align: inherit;">(C) 对于每种布局，我们生成许多配置来表示不同的随机照明并模拟日常生活中场景随时间的变化。</font><font style="vertical-align: inherit;">(D) 我们提供交互式模拟器 (ViSim) 来帮助创建地面实况 IMU、事件以及单目或立体相机轨迹，包括手绘、随机行走和基于神经网络的真实轨迹。</font><font style="vertical-align: inherit;">(E) 所有支持的图像序列和地面实况。</font></font></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/70922f5a13badf3969e813493dcfd57b620720fbc1bc05e392cea087c7af1cb9/68747470733a2f2f696e746572696f726e65742e6f72672f6974656d732f496e746572696f724e65742e6a7067"><img width="50%" src="https://camo.githubusercontent.com/70922f5a13badf3969e813493dcfd57b620720fbc1bc05e392cea087c7af1cb9/68747470733a2f2f696e746572696f726e65742e6f72672f6974656d732f496e746572696f724e65742e6a7067" data-canonical-src="https://interiornet.org/items/InteriorNet.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Semantic3D </font></font></b><a href="http://www.semantic3d.net/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[链接]</font></font></a>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">大规模点云分类基准，提供了总计超过 40 亿个点的大型自然场景标记 3D 点云数据集，还涵盖了一系列多样化的城市场景。</font></font></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/7ac9e028c6d93d6839d77e5e93f915d7c5b0b2b671775dc679dfc7612d46a2d9/687474703a2f2f7777772e73656d616e74696333642e6e65742f696d672f66756c6c5f7265736f6c7574696f6e2f736732375f382e6a7067"><img width="50%" src="https://camo.githubusercontent.com/7ac9e028c6d93d6839d77e5e93f915d7c5b0b2b671775dc679dfc7612d46a2d9/687474703a2f2f7777772e73656d616e74696333642e6e65742f696d672f66756c6c5f7265736f6c7574696f6e2f736732375f382e6a7067" data-canonical-src="http://www.semantic3d.net/img/full_resolution/sg27_8.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Structured3D：用于结构化 3D 建模的大型逼真数据集</font></font></b> <a href="https://structured3d-dataset.org/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[链接]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/db954b717d9ba5c3776aafdec2e290b5629b7858abf9de5ba122a932e1a4220a/68747470733a2f2f7374727563747572656433642d646174617365742e6f72672f7374617469632f696d672f7465617365722e706e67"><img width="50%" src="https://camo.githubusercontent.com/db954b717d9ba5c3776aafdec2e290b5629b7858abf9de5ba122a932e1a4220a/68747470733a2f2f7374727563747572656433642d646174617365742e6f72672f7374617469632f696d672f7465617365722e706e67" data-canonical-src="https://structured3d-dataset.org/static/img/teaser.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3D-FRONT：带有布局和语义的 3D 家具房间</font></font></b> <a href="https://tianchi.aliyun.com/specials/promotion/alibaba-3d-scene-dataset" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[链接]</font></font></a>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">包含 10,000 栋房屋（或公寓）和约 70,000 个带有布局信息的房间。</font></font></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/a8f59bb0d3e0cdf625047b6e872f6bb20572c8629770faa040bb1df7a4510bfa/68747470733a2f2f696d672e616c6963646e2e636f6d2f7466732f5442313331584f4a654c32674b306a535a506858586168765858612d323939322d323735312e6a7067"><img width="50%" src="https://camo.githubusercontent.com/a8f59bb0d3e0cdf625047b6e872f6bb20572c8629770faa040bb1df7a4510bfa/68747470733a2f2f696d672e616c6963646e2e636f6d2f7466732f5442313331584f4a654c32674b306a535a506858586168765858612d323939322d323735312e6a7067" data-canonical-src="https://img.alicdn.com/tfs/TB131XOJeL2gK0jSZPhXXahvXXa-2992-2751.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3ThreeDWorld(TDW)：用于交互式物理模拟的高保真、多模态平台</font></font></b> <a href="http://www.threedworld.org/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[链接]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/ca5cc9576d42cdf74f2bbc27904793d996d9c5b7b31a674c3db255ef0193f74d/687474703a2f2f7777772e746872656564776f726c642e6f72672f696d672f67616c6c6572792f67616c6c6572792d312e6a7067"><img width="50%" src="https://camo.githubusercontent.com/ca5cc9576d42cdf74f2bbc27904793d996d9c5b7b31a674c3db255ef0193f74d/687474703a2f2f7777772e746872656564776f726c642e6f72672f696d672f67616c6c6572792f67616c6c6572792d312e6a7067" data-canonical-src="http://www.threedworld.org/img/gallery/gallery-1.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">MINERVAS：大规模室内环境虚拟合成</font></font></b> <a href="https://coohom.github.io/MINERVAS/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[链接]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/f7127a63ec1d2f0e4e69738587adf12932c43163929237a5353e52a71facaf90/68747470733a2f2f636f6f686f6d2e6769746875622e696f2f4d494e45525641532f7374617469632f696d672f7465617365722e706e67"><img width="50%" src="https://camo.githubusercontent.com/f7127a63ec1d2f0e4e69738587adf12932c43163929237a5353e52a71facaf90/68747470733a2f2f636f6f686f6d2e6769746875622e696f2f4d494e45525641532f7374617469632f696d672f7465617365722e706e67" data-canonical-src="https://coohom.github.io/MINERVAS/static/img/teaser.png" style="max-width: 100%;"></a></p>
<a name="user-content-pose_estimation">
</a><div class="markdown-heading" dir="auto"><a><h2 tabindex="-1" class="heading-element" dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3D 姿态估计</font></font></h2></a><a id="user-content-3d-pose-estimation" class="anchor" aria-label="永久链接：3D 姿态估计" href="#3d-pose-estimation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">从单个图像重建特定类别的对象 (2014) </font></font></b> <a href="https://people.eecs.berkeley.edu/~akar/categoryshapes.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/e7331a8442f0855ff051067aed07e9de2a6fbcf5c9a2324d4b7e1bb736cf2e0c/687474703a2f2f70656f706c652e656563732e6265726b656c65792e6564752f7e616b61722f62617369737368617065735f686967687265732e706e67"><img width="50%" src="https://camo.githubusercontent.com/e7331a8442f0855ff051067aed07e9de2a6fbcf5c9a2324d4b7e1bb736cf2e0c/687474703a2f2f70656f706c652e656563732e6265726b656c65792e6564752f7e616b61722f62617369737368617065735f686967687265732e706e67" data-canonical-src="http://people.eecs.berkeley.edu/~akar/basisshapes_highres.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">观点和要点（2015）</font></font></b> <a href="https://people.eecs.berkeley.edu/~shubhtuls/papers/cvpr15vpsKps.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/3D-Machine-Learning/blob/master/imgs/Viewpoints%20and%20Keypoints.jpeg"><img width="50%" src="https://github.com/timzhang642/3D-Machine-Learning/raw/master/imgs/Viewpoints%20and%20Keypoints.jpeg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">CNN 渲染：使用经过渲染 3D 模型视图训练的 CNN 进行图像中的视点估计（2015 ICCV）</font></font></b> <a href="https://shapenet.cs.stanford.edu/projects/RenderForCNN/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/28fa95948b098dde9db0877275d72409344d52b0de9a02c2208a3b19c28cbf81/68747470733a2f2f73686170656e65742e63732e7374616e666f72642e6564752f70726f6a656374732f52656e646572466f72434e4e2f696d616765732f7465617365722e6a7067"><img width="50%" src="https://camo.githubusercontent.com/28fa95948b098dde9db0877275d72409344d52b0de9a02c2208a3b19c28cbf81/68747470733a2f2f73686170656e65742e63732e7374616e666f72642e6564752f70726f6a656374732f52656e646572466f72434e4e2f696d616765732f7465617365722e6a7067" data-canonical-src="https://shapenet.cs.stanford.edu/projects/RenderForCNN/images/teaser.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">PoseNet：用于实时 6-DOF 相机重定位的卷积网络（2015）</font></font></b> <a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Kendall_PoseNet_A_Convolutional_ICCV_2015_paper.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/8c247efad2b90818740fea353d44f5005ea493132030de958320d9f17cb413ef/687474703a2f2f6d692e656e672e63616d2e61632e756b2f70726f6a656374732f72656c6f63616c69736174696f6e2f696d616765732f6d61702e706e67"><img width="50%" src="https://camo.githubusercontent.com/8c247efad2b90818740fea353d44f5005ea493132030de958320d9f17cb413ef/687474703a2f2f6d692e656e672e63616d2e61632e756b2f70726f6a656374732f72656c6f63616c69736174696f6e2f696d616765732f6d61702e706e67" data-canonical-src="http://mi.eng.cam.ac.uk/projects/relocalisation/images/map.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">用于相机重定位的深度学习中的不确定性建模（2016）</font></font></b> <a href="https://arxiv.org/pdf/1509.05909.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/3D-Machine-Learning/blob/master/imgs/Modeling%20Uncertainty%20in%20Deep%20Learning%20for%20Camera%20Relocalization.jpeg"><img width="50%" src="https://github.com/timzhang642/3D-Machine-Learning/raw/master/imgs/Modeling%20Uncertainty%20in%20Deep%20Learning%20for%20Camera%20Relocalization.jpeg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">使用深度学习通过视点分类进行鲁棒相机姿态估计（2016）</font></font></b> <a href="https://link.springer.com/article/10.1007/s41095-016-0067-z" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/3D-Machine-Learning/blob/master/imgs/Robust%20camera%20pose%20estimation%20by%20viewpoint%20classification%20using%20deep%20learning.jpeg"><img width="50%" src="https://github.com/timzhang642/3D-Machine-Learning/raw/master/imgs/Robust%20camera%20pose%20estimation%20by%20viewpoint%20classification%20using%20deep%20learning.jpeg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">使用 lstms 进行结构化特征相关的基于图像的定位（2017 ICCV）</font></font></b> <a href="https://arxiv.org/pdf/1611.07890.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="/timzhang642/3D-Machine-Learning/blob/master/imgs/Image-based localization using LSTMs for structured feature correlation.png"><img width="50%" src="/timzhang642/3D-Machine-Learning/raw/master/imgs/Image-based localization using LSTMs for structured feature correlation.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">使用沙漏网络进行基于图像的定位（2017 ICCV 研讨会）</font></font></b> <a href="https://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w17/Melekhov_Image-Based_Localization_Using_ICCV_2017_paper.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="/timzhang642/3D-Machine-Learning/blob/master/imgs/Image-Based Localization Using Hourglass Networks.png"><img width="50%" src="/timzhang642/3D-Machine-Learning/raw/master/imgs/Image-Based Localization Using Hourglass Networks.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">使用深度学习进行相机姿态回归的几何损失函数（2017 CVPR）</font></font></b> <a href="https://arxiv.org/pdf/1704.00390.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/2fd63ffb088e4740efbc3ff1f03a11d73dbbc736f381713a66c67448bd9c3c81/687474703a2f2f6d692e656e672e63616d2e61632e756b2f7e6369706f6c6c612f696d616765732f706f73652d6e65742e706e67"><img width="50%" src="https://camo.githubusercontent.com/2fd63ffb088e4740efbc3ff1f03a11d73dbbc736f381713a66c67448bd9c3c81/687474703a2f2f6d692e656e672e63616d2e61632e756b2f7e6369706f6c6c612f696d616765732f706f73652d6e65742e706e67" data-canonical-src="http://mi.eng.cam.ac.uk/~cipolla/images/pose-net.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">通过姿势估计和匹配进行通用 3D 表示 (2017) </font></font></b> <a href="http://3drepresentation.stanford.edu/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/3D-Machine-Learning/blob/master/imgs/Generic%203D%20Representation%20via%20Pose%20Estimation%20and%20Matching.jpeg"><img width="50%" src="https://github.com/timzhang642/3D-Machine-Learning/raw/master/imgs/Generic%203D%20Representation%20via%20Pose%20Estimation%20and%20Matching.jpeg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">使用深度学习和几何进行 3D 边界框估计（2017）</font></font></b> <a href="https://arxiv.org/pdf/1612.00496.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/3D-Machine-Learning/blob/master/imgs/3D%20Bounding%20Box%20Estimation%20Using%20Deep%20Learning%20and%20Geometry.png"><img width="50%" src="https://github.com/timzhang642/3D-Machine-Learning/raw/master/imgs/3D%20Bounding%20Box%20Estimation%20Using%20Deep%20Learning%20and%20Geometry.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">来自语义关键点的 6-DoF 对象姿势 (2017) </font></font></b> <a href="https://www.seas.upenn.edu/~pavlakos/projects/object3d/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/0a6461a21ae9ea924b25bee51e00f2ff58ffdea3bec53b5b578697d9399c5977/68747470733a2f2f7777772e736561732e7570656e6e2e6564752f7e7061766c616b6f732f70726f6a656374732f6f626a65637433642f66696c65732f6f626a65637433642d7465617365722e706e67"><img width="50%" src="https://camo.githubusercontent.com/0a6461a21ae9ea924b25bee51e00f2ff58ffdea3bec53b5b578697d9399c5977/68747470733a2f2f7777772e736561732e7570656e6e2e6564752f7e7061766c616b6f732f70726f6a656374732f6f626a65637433642f66696c65732f6f626a65637433642d7465617365722e706e67" data-canonical-src="https://www.seas.upenn.edu/~pavlakos/projects/object3d/files/object3d-teaser.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">使用卷积神经网络进行相对相机姿态估计（2017）</font></font></b> <a href="https://arxiv.org/pdf/1702.01381.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/3D-Machine-Learning/blob/master/imgs/Relative%20Camera%20Pose%20Estimation%20Using%20Convolutional%20Neural%20Networks.png"><img width="50%" src="https://github.com/timzhang642/3D-Machine-Learning/raw/master/imgs/Relative%20Camera%20Pose%20Estimation%20Using%20Convolutional%20Neural%20Networks.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3DMatch：从 RGB-D 重建中学习局部几何描述符（2017）</font></font></b> <a href="http://3dmatch.cs.princeton.edu/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/6eaadbace0a8920da7ec2a33a16fcf42c678077db8e594e981b3ec7c7139c1b4/687474703a2f2f33646d617463682e63732e7072696e6365746f6e2e6564752f696d672f6f766572766965772e6a7067"><img width="50%" src="https://camo.githubusercontent.com/6eaadbace0a8920da7ec2a33a16fcf42c678077db8e594e981b3ec7c7139c1b4/687474703a2f2f33646d617463682e63732e7072696e6365746f6e2e6564752f696d672f6f766572766965772e6a7067" data-canonical-src="http://3dmatch.cs.princeton.edu/img/overview.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">单图像 3D 解释器网络 (2016) </font></font></b> <a href="http://3dinterpreter.csail.mit.edu/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/jiajunwu/3dinn"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/16a85ad12c30ae686479dd38a006075349f28b2b31e2736df09f2842f0f29904/687474703a2f2f3364696e7465727072657465722e637361696c2e6d69742e6564752f696d616765732f73706f746c696768745f3364696e6e5f6c617267652e6a7067"><img width="50%" src="https://camo.githubusercontent.com/16a85ad12c30ae686479dd38a006075349f28b2b31e2736df09f2842f0f29904/687474703a2f2f3364696e7465727072657465722e637361696c2e6d69742e6564752f696d616765732f73706f746c696768745f3364696e6e5f6c617267652e6a7067" data-canonical-src="http://3dinterpreter.csail.mit.edu/images/spotlight_3dinn_large.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">多视图一致性作为学习形状和姿势预测的监督信号（2018 CVPR）</font></font></b> <a href="https://shubhtuls.github.io/mvcSnP/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/a5da4622e393300cd5d23bd7c5dbfcaaeca895457545c55671767315b5fe6c5b/68747470733a2f2f736875626874756c732e6769746875622e696f2f6d7663536e502f7265736f75726365732f696d616765732f7465617365722e706e67"><img width="50%" src="https://camo.githubusercontent.com/a5da4622e393300cd5d23bd7c5dbfcaaeca895457545c55671767315b5fe6c5b/68747470733a2f2f736875626874756c732e6769746875622e696f2f6d7663536e502f7265736f75726365732f696d616765732f7465617365722e706e67" data-canonical-src="https://shubhtuls.github.io/mvcSnP/resources/images/teaser.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">PoseCNN：用于杂乱场景中 6D 物体姿态估计的卷积神经网络（2018）</font></font></b> <a href="https://rse-lab.cs.washington.edu/projects/posecnn/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/e61c932238878d093fda806c98fb4c9729430dd070d80f3a69cfeb25cd7d6cd0/68747470733a2f2f7975786e672e6769746875622e696f2f506f7365434e4e2e706e67"><img width="50%" src="https://camo.githubusercontent.com/e61c932238878d093fda806c98fb4c9729430dd070d80f3a69cfeb25cd7d6cd0/68747470733a2f2f7975786e672e6769746875622e696f2f506f7365434e4e2e706e67" data-canonical-src="https://yuxng.github.io/PoseCNN.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">用于从合成图像中学习快速准确的 3D 姿势推断的特征映射（2018 CVPR）</font></font></b> <a href="https://arxiv.org/pdf/1712.03904.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/d9a9e263f926c58c5fd0df423c0452a442e5bacd6db6689ad73206e553e802bc/68747470733a2f2f656e637279707465642d74626e302e677374617469632e636f6d2f696d616765733f713d74626e3a414e64394763546e7079616a4568626872504d6330597045517a7145384e39453743575f45565759413342786734366f55455946663958766b41"><img width="40%" src="https://camo.githubusercontent.com/d9a9e263f926c58c5fd0df423c0452a442e5bacd6db6689ad73206e553e802bc/68747470733a2f2f656e637279707465642d74626e302e677374617469632e636f6d2f696d616765733f713d74626e3a414e64394763546e7079616a4568626872504d6330597045517a7145384e39453743575f45565759413342786734366f55455946663958766b41" data-canonical-src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTnpyajEhbhrPMc0YpEQzqE8N9E7CW_EVWYA3Bxg46oUEYFf9XvkA" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pix3D：单图像 3D 形状建模的数据集和方法（2018 CVPR）</font></font></b> <a href="http://pix3d.csail.mit.edu/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/500948d9ed72250e72a845caa1851b2557077c4988cf0d12debfc95647ed9551/687474703a2f2f70697833642e637361696c2e6d69742e6564752f696d616765732f73706f746c696768745f70697833642e6a7067"><img width="50%" src="https://camo.githubusercontent.com/500948d9ed72250e72a845caa1851b2557077c4988cf0d12debfc95647ed9551/687474703a2f2f70697833642e637361696c2e6d69742e6564752f696d616765732f73706f746c696768745f70697833642e6a7067" data-canonical-src="http://pix3d.csail.mit.edu/images/spotlight_pix3d.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">野外物体的 3D 姿态估计和 3D 模型检索（2018 CVPR）</font></font></b> <a href="https://arxiv.org/pdf/1803.11493.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/9a7c75937eb7bcf28c989351c958b6bc85c59b74a3187a5cb39c3690b3c5fc02/68747470733a2f2f7777772e74756772617a2e61742f66696c6561646d696e2f757365725f75706c6f61642f496e737469747574652f4943472f446f63756d656e74732f7465616d5f6c6570657469742f696d616765732f677261626e65722f706f73655f72657472696576616c5f6f766572766965772e706e67"><img width="50%" src="https://camo.githubusercontent.com/9a7c75937eb7bcf28c989351c958b6bc85c59b74a3187a5cb39c3690b3c5fc02/68747470733a2f2f7777772e74756772617a2e61742f66696c6561646d696e2f757365725f75706c6f61642f496e737469747574652f4943472f446f63756d656e74732f7465616d5f6c6570657469742f696d616765732f677261626e65722f706f73655f72657472696576616c5f6f766572766965772e706e67" data-canonical-src="https://www.tugraz.at/fileadmin/user_upload/Institute/ICG/Documents/team_lepetit/images/grabner/pose_retrieval_overview.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">用于家用物体语义机器人抓取的深度物体姿势估计（2018）</font></font></b> <a href="https://research.nvidia.com/publication/2018-09_Deep-Object-Pose" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/bcd794f0f444633b12a2d06467028091e61e8751dd37ae1713ee64a712a5b66f/68747470733a2f2f72657365617263682e6e76696469612e636f6d2f73697465732f64656661756c742f66696c65732f7075626c69636174696f6e732f666f7277656273697465315f302e706e67"><img width="50%" src="https://camo.githubusercontent.com/bcd794f0f444633b12a2d06467028091e61e8751dd37ae1713ee64a712a5b66f/68747470733a2f2f72657365617263682e6e76696469612e636f6d2f73697465732f64656661756c742f66696c65732f7075626c69636174696f6e732f666f7277656273697465315f302e706e67" data-canonical-src="https://research.nvidia.com/sites/default/files/publications/forwebsite1_0.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">MocapNET2：一种直接以流行的 Bio Vision Hierarchy (BVH) 格式估计 3D 人体姿势的实时方法 (2021) </font></font></b> <a href="http://users.ics.forth.gr/~argyros/mypapers/2021_01_ICPR_Qammaz.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">，</font></font><a href="https://github.com/FORTH-ModelBasedTracker/MocapNET"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/FORTH-ModelBasedTracker/MocapNET/master/doc/mnet2.png"><img width="50%" src="https://raw.githubusercontent.com/FORTH-ModelBasedTracker/MocapNET/master/doc/mnet2.png" style="max-width: 100%;"></a></p>
<a name="user-content-single_classification">
</a><div class="markdown-heading" dir="auto"><a><h2 tabindex="-1" class="heading-element" dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">单一对象分类</font></font></h2></a><a id="user-content-single-object-classification" class="anchor" aria-label="固定链接：单个对象分类" href="#single-object-classification"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">👾 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3D ShapeNets：体积形状的深度表示（2015）</font></font></b> <a href="http://3dshapenets.cs.princeton.edu/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/3987fb73c58f0ff1cf3ffb2be7ecf0d48536caa6009c88f3d08de78c2b5c5959/68747470733a2f2f6169322d73322d7075626c69632e73332e616d617a6f6e6177732e636f6d2f666967757265732f323031362d31312d30382f336564323333383632383461353633396362336538626161656366343936636161373636653333352f312d466967757265312d312e706e67"><img width="50%" src="https://camo.githubusercontent.com/3987fb73c58f0ff1cf3ffb2be7ecf0d48536caa6009c88f3d08de78c2b5c5959/68747470733a2f2f6169322d73322d7075626c69632e73332e616d617a6f6e6177732e636f6d2f666967757265732f323031362d31312d30382f336564323333383632383461353633396362336538626161656366343936636161373636653333352f312d466967757265312d312e706e67" data-canonical-src="https://ai2-s2-public.s3.amazonaws.com/figures/2016-11-08/3ed23386284a5639cb3e8baaecf496caa766e335/1-Figure1-1.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">👾 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">VoxNet：用于实时对象识别的 3D 卷积神经网络 (2015) </font></font></b> <a href="http://www.dimatura.net/publications/voxnet_maturana_scherer_iros15.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/dimatura/voxnet"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/afa3e54800deeb41c7f3368a752033ba7e3291d77d2c8ec570b4568a523da267/687474703a2f2f7777772e64696d61747572612e6e65742f72657365617263682f766f786e65742f6361725f766f786e65745f736964652e706e67"><img width="50%" src="https://camo.githubusercontent.com/afa3e54800deeb41c7f3368a752033ba7e3291d77d2c8ec570b4568a523da267/687474703a2f2f7777772e64696d61747572612e6e65742f72657365617263682f766f786e65742f6361725f766f786e65745f736964652e706e67" data-canonical-src="http://www.dimatura.net/research/voxnet/car_voxnet_side.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">📷</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">用于 3D 形状识别的多视图卷积神经网络 (2015) </font></font></b> <a href="http://vis-www.cs.umass.edu/mvcnn/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/f4a0426fea65303b105354ee98b2aa152db065e03ca2d9ba0d44ee8608c56f9f/687474703a2f2f7669732d7777772e63732e756d6173732e6564752f6d76636e6e2f696d616765732f6d76636e6e2e706e67"><img width="50%" src="https://camo.githubusercontent.com/f4a0426fea65303b105354ee98b2aa152db065e03ca2d9ba0d44ee8608c56f9f/687474703a2f2f7669732d7777772e63732e756d6173732e6564752f6d76636e6e2f696d616765732f6d76636e6e2e706e67" data-canonical-src="http://vis-www.cs.umass.edu/mvcnn/images/mvcnn.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">📷 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">DeepPano：3D 形状识别的深度全景表示（2015）</font></font></b> <a href="http://mclab.eic.hust.edu.cn/UpLoadFiles/Papers/DeepPano_SPL2015.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/bc6850325b640ec169477fae20b4769d689c90fdee86a9ebdb3d62d224409847/68747470733a2f2f6169322d73322d7075626c69632e73332e616d617a6f6e6177732e636f6d2f666967757265732f323031362d31312d30382f356131623564333139303564386365636537623738353130663531663364386262623036333036332f312d466967757265332d312e706e67"><img width="30%" src="https://camo.githubusercontent.com/bc6850325b640ec169477fae20b4769d689c90fdee86a9ebdb3d62d224409847/68747470733a2f2f6169322d73322d7075626c69632e73332e616d617a6f6e6177732e636f6d2f666967757265732f323031362d31312d30382f356131623564333139303564386365636537623738353130663531663364386262623036333036332f312d466967757265332d312e706e67" data-canonical-src="https://ai2-s2-public.s3.amazonaws.com/figures/2016-11-08/5a1b5d31905d8cece7b78510f51f3d8bbb063063/1-Figure3-1.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">👾📷 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">FusionNet：使用多种数据表示的 3D 对象分类（2016）</font></font></b> <a href="https://stanford.edu/~rezab/papers/fusionnet.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/877035a9cf0f5ce1e63ce2d744d475bc692c8122aeb11f80a0c1ea48a6fb29dc/68747470733a2f2f6169322d73322d7075626c69632e73332e616d617a6f6e6177732e636f6d2f666967757265732f323031362d31312d30382f306161623866626365663166306131346635363533643137306361333666346535616165383031302f362d466967757265352d312e706e67"><img width="30%" src="https://camo.githubusercontent.com/877035a9cf0f5ce1e63ce2d744d475bc692c8122aeb11f80a0c1ea48a6fb29dc/68747470733a2f2f6169322d73322d7075626c69632e73332e616d617a6f6e6177732e636f6d2f666967757265732f323031362d31312d30382f306161623866626365663166306131346635363533643137306361333666346535616165383031302f362d466967757265352d312e706e67" data-canonical-src="https://ai2-s2-public.s3.amazonaws.com/figures/2016-11-08/0aab8fbcef1f0a14f5653d170ca36f4e5aae8010/6-Figure5-1.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">👾📷</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">用于 3D 数据对象分类的体积和多视图 CNN (2016) </font></font></b> <a href="https://arxiv.org/pdf/1604.03265.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/charlesq34/3dcnn.torch"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/6e79e302e73444cd0d0bce651ae4569207e0734fd0675943247461b5568855cd/687474703a2f2f67726170686963732e7374616e666f72642e6564752f70726f6a656374732f3364636e6e2f7465617365722e6a7067"><img width="40%" src="https://camo.githubusercontent.com/6e79e302e73444cd0d0bce651ae4569207e0734fd0675943247461b5568855cd/687474703a2f2f67726170686963732e7374616e666f72642e6564752f70726f6a656374732f3364636e6e2f7465617365722e6a7067" data-canonical-src="http://graphics.stanford.edu/projects/3dcnn/teaser.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">👾</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">使用卷积神经网络进行生成和判别体素建模 (2016) </font></font></b> <a href="https://arxiv.org/pdf/1608.04236.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/ajbrock/Generative-and-Discriminative-Voxel-Modeling"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/b622fc556df61f67e6847567a6162dc824490af25261b0ef1fb20bfd797427c8/687474703a2f2f6461766964737475747a2e64652f776f726470726573732f77702d636f6e74656e742f75706c6f6164732f323031372f30322f62726f636b5f7661652e706e67"><img width="50%" src="https://camo.githubusercontent.com/b622fc556df61f67e6847567a6162dc824490af25261b0ef1fb20bfd797427c8/687474703a2f2f6461766964737475747a2e64652f776f726470726573732f77702d636f6e74656e742f75706c6f6164732f323031372f30322f62726f636b5f7661652e706e67" data-canonical-src="http://davidstutz.de/wordpress/wp-content/uploads/2017/02/brock_vae.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">💎</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">使用混合模型 CNN 对图和流形进行几何深度学习 (2016) </font></font></b> <a href="https://arxiv.org/pdf/1611.08402.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[链接]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/1973b46ac7fa8efce553fa3a8d60c3817789177ea62b79a5144c81a5b546a4cd/68747470733a2f2f69322e77702e636f6d2f70726566657272656472657365617263682e6a702f77702d636f6e74656e742f75706c6f6164732f323031372f30382f6d6f6e65742e706e673f726573697a653d3538312532433135352673736c3d31"><img width="50%" src="https://camo.githubusercontent.com/1973b46ac7fa8efce553fa3a8d60c3817789177ea62b79a5144c81a5b546a4cd/68747470733a2f2f69322e77702e636f6d2f70726566657272656472657365617263682e6a702f77702d636f6e74656e742f75706c6f6164732f323031372f30382f6d6f6e65742e706e673f726573697a653d3538312532433135352673736c3d31" data-canonical-src="https://i2.wp.com/preferredresearch.jp/wp-content/uploads/2017/08/monet.png?resize=581%2C155&amp;ssl=1" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">👾 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3D GAN：通过 3D 生成对抗建模学习对象形状的概率潜在空间（2016）</font></font></b> <a href="https://arxiv.org/pdf/1610.07584.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/zck119/3dgan-release"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/3d4a942fc3d9c13e017bc208ce048de928317c972fa7c0d6c15c85dffbde739b/687474703a2f2f336467616e2e637361696c2e6d69742e6564752f696d616765732f6d6f64656c2e6a7067"><img width="50%" src="https://camo.githubusercontent.com/3d4a942fc3d9c13e017bc208ce048de928317c972fa7c0d6c15c85dffbde739b/687474703a2f2f336467616e2e637361696c2e6d69742e6564752f696d616765732f6d6f64656c2e6a7067" data-canonical-src="http://3dgan.csail.mit.edu/images/model.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">👾</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">使用卷积神经网络进行生成和判别体素建模（2017）</font></font></b> <a href="https://github.com/ajbrock/Generative-and-Discriminative-Voxel-Modeling"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/ajbrock/Generative-and-Discriminative-Voxel-Modeling/blob/master/doc/GUI3.png"><img width="50%" src="https://github.com/ajbrock/Generative-and-Discriminative-Voxel-Modeling/raw/master/doc/GUI3.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">👾 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">FPNN：3D 数据的现场探测神经网络 (2016) </font></font></b> <a href="http://yangyanli.github.io/FPNN/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/yangyanli/FPNN"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/94a3c0a757f7c80d8def0c22b0beb9ac6f1c6b437c4439579aa8de78e4e1d2c1/68747470733a2f2f6169322d73322d7075626c69632e73332e616d617a6f6e6177732e636f6d2f666967757265732f323031362d31312d30382f313563613761646363663563643464633330396364636161363332386634633432396561643333372f312d466967757265322d312e706e67"><img width="30%" src="https://camo.githubusercontent.com/94a3c0a757f7c80d8def0c22b0beb9ac6f1c6b437c4439579aa8de78e4e1d2c1/68747470733a2f2f6169322d73322d7075626c69632e73332e616d617a6f6e6177732e636f6d2f666967757265732f323031362d31312d30382f313563613761646363663563643464633330396364636161363332386634633432396561643333372f312d466967757265322d312e706e67" data-canonical-src="https://ai2-s2-public.s3.amazonaws.com/figures/2016-11-08/15ca7adccf5cd4dc309cdcaa6328f4c429ead337/1-Figure2-1.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">👾 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">OctNet：学习高分辨率的深度 3D 表示（2017）</font></font></b> <a href="https://arxiv.org/pdf/1611.05009.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/griegler/octnet"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/d77fde296f1c7f13e1935f4661bc30cb1024570ce689c333caebf4bc257887e4/68747470733a2f2f69732e74756562696e67656e2e6d70672e64652f75706c6f6164732f7075626c69636174696f6e2f696d6167652f31383932312f696d6730332e706e67"><img width="30%" src="https://camo.githubusercontent.com/d77fde296f1c7f13e1935f4661bc30cb1024570ce689c333caebf4bc257887e4/68747470733a2f2f69732e74756562696e67656e2e6d70672e64652f75706c6f6164732f7075626c69636174696f6e2f696d6167652f31383932312f696d6730332e706e67" data-canonical-src="https://is.tuebingen.mpg.de/uploads/publication/image/18921/img03.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">👾 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O-CNN：用于 3D 形状分析的基于八叉树的卷积神经网络 (2017) </font></font></b> <a href="http://wang-ps.github.io/O-CNN" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/Microsoft/O-CNN"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/28f048d642f933ee2b2f49362860fa03b05e29e2e0892a7bc5c8f4755dabc971/687474703a2f2f77616e672d70732e6769746875622e696f2f4f2d434e4e5f66696c65732f7465617365722e706e67"><img width="50%" src="https://camo.githubusercontent.com/28f048d642f933ee2b2f49362860fa03b05e29e2e0892a7bc5c8f4755dabc971/687474703a2f2f77616e672d70732e6769746875622e696f2f4f2d434e4e5f66696c65732f7465617365722e706e67" data-canonical-src="http://wang-ps.github.io/O-CNN_files/teaser.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">👾</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">用于 3D 对象识别的方向增强体素网络 (2017) </font></font></b> <a href="https://lmb.informatik.uni-freiburg.de/Publications/2017/SZB17a/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/lmb-freiburg/orion"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/eb1b948e65fc71adf4824a235635008cf473b00c1dabc063ccae1dec3342b882/68747470733a2f2f6c6d622e696e666f726d6174696b2e756e692d66726569627572672e64652f5075626c69636174696f6e732f323031372f535a423137612f7465617365725f772e706e67"><img width="50%" src="https://camo.githubusercontent.com/eb1b948e65fc71adf4824a235635008cf473b00c1dabc063ccae1dec3342b882/68747470733a2f2f6c6d622e696e666f726d6174696b2e756e692d66726569627572672e64652f5075626c69636174696f6e732f323031372f535a423137612f7465617365725f772e706e67" data-canonical-src="https://lmb.informatik.uni-freiburg.de/Publications/2017/SZB17a/teaser_w.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">🎲 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">PointNet：用于 3D 分类和分割的点集深度学习 (2017) </font></font></b> <a href="http://stanford.edu/~rqi/pointnet/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/charlesq34/pointnet"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/a05a099e9614ae8d47902075ebb1a2b8be8b58bab56493238defd7d936f9197f/68747470733a2f2f7765622e7374616e666f72642e6564752f7e7271692f7061706572732f706f696e746e65742e706e67"><img width="40%" src="https://camo.githubusercontent.com/a05a099e9614ae8d47902075ebb1a2b8be8b58bab56493238defd7d936f9197f/68747470733a2f2f7765622e7374616e666f72642e6564752f7e7271692f7061706572732f706f696e746e65742e706e67" data-canonical-src="https://web.stanford.edu/~rqi/papers/pointnet.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">🎲 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">PointNet++：度量空间中点集的深度层次特征学习（2017）</font></font></b> <a href="https://arxiv.org/pdf/1706.02413.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/charlesq34/pointnet2"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/3D-Machine-Learning/blob/master/imgs/PointNet%2B%2B-%20Deep%20Hierarchical%20Feature%20Learning%20on%20Point%20Sets%20in%20a%20Metric%20Space.png"><img width="40%" src="https://github.com/timzhang642/3D-Machine-Learning/raw/master/imgs/PointNet%2B%2B-%20Deep%20Hierarchical%20Feature%20Learning%20on%20Point%20Sets%20in%20a%20Metric%20Space.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">📷</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">反馈网络 (2017) </font></font></b> <a href="http://feedbacknet.stanford.edu/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/amir32002/feedback-networks"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/3D-Machine-Learning/blob/master/imgs/Feedback%20Networks.png"><img width="50%" src="https://github.com/timzhang642/3D-Machine-Learning/raw/master/imgs/Feedback%20Networks.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">🎲</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">逃离细胞：用于识别 3D 点云模型的深度 Kd 网络（2017）</font></font></b> <a href="http://www.arxiv.org/pdf/1704.01222.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/3D-Machine-Learning/blob/master/imgs/Escape From Cells.png"><img width="50%" src="https://github.com/timzhang642/3D-Machine-Learning/raw/master/imgs/Escape From Cells.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">🎲</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">用&ZeroWidthSpace;&ZeroWidthSpace;于点云学习的动态图 CNN（2018）</font></font></b> <a href="https://arxiv.org/pdf/1801.07829.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/c5ae8c4f32a922013a9d382bf4f2cadd2f17bd49f2a6207fac0aad8a6b2a2f1c/68747470733a2f2f6c69757a69776569372e6769746875622e696f2f686f6d65706167655f66696c65732f64796e616d696367636e6e5f6c6f676f2e706e67"><img width="50%" src="https://camo.githubusercontent.com/c5ae8c4f32a922013a9d382bf4f2cadd2f17bd49f2a6207fac0aad8a6b2a2f1c/68747470733a2f2f6c69757a69776569372e6769746875622e696f2f686f6d65706167655f66696c65732f64796e616d696367636e6e5f6c6f676f2e706e67" data-canonical-src="https://liuziwei7.github.io/homepage_files/dynamicgcnn_logo.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">🎲 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">PointCNN (2018) </font></font></b> <a href="https://yangyanli.github.io/PointCNN/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/c5f93015d4948dd7cb9c1307f7c4488a1dd657c81c6af6f550ce6aa3c87d458e/687474703a2f2f79616e6779616e2e6c692f696d616765732f70617065722f706f696e74636e6e2e706e67"><img width="50%" src="https://camo.githubusercontent.com/c5f93015d4948dd7cb9c1307f7c4488a1dd657c81c6af6f550ce6aa3c87d458e/687474703a2f2f79616e6779616e2e6c692f696d616765732f70617065722f706f696e74636e6e2e706e67" data-canonical-src="http://yangyan.li/images/paper/pointcnn.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">🎲📷</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">通过自动深度图像生成进行点云分类的网络架构（2018 CVPR）</font></font></b> <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Roveri_A_Network_Architecture_CVPR_2018_paper.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/f868e89f9e2050766ae09544b1ecdecca5614b7c75d25af2af834dd43283ca84/68747470733a2f2f73332d75732d776573742d312e616d617a6f6e6177732e636f6d2f6469736e657972657365617263682f77702d636f6e74656e742f75706c6f6164732f32303138303631393131343733322f412d4e6574776f726b2d4172636869746563747572652d666f722d506f696e742d436c6f75642d436c617373696669636174696f6e2d7669612d4175746f6d617469632d44657074682d496d616765732d47656e65726174696f6e2d496d6167652d363030783331372e6a7067"><img width="50%" src="https://camo.githubusercontent.com/f868e89f9e2050766ae09544b1ecdecca5614b7c75d25af2af834dd43283ca84/68747470733a2f2f73332d75732d776573742d312e616d617a6f6e6177732e636f6d2f6469736e657972657365617263682f77702d636f6e74656e742f75706c6f6164732f32303138303631393131343733322f412d4e6574776f726b2d4172636869746563747572652d666f722d506f696e742d436c6f75642d436c617373696669636174696f6e2d7669612d4175746f6d617469632d44657074682d496d616765732d47656e65726174696f6e2d496d6167652d363030783331372e6a7067" data-canonical-src="https://s3-us-west-1.amazonaws.com/disneyresearch/wp-content/uploads/20180619114732/A-Network-Architecture-for-Point-Cloud-Classification-via-Automatic-Depth-Images-Generation-Image-600x317.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">🎲👾 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">PointGrid：用于 3D 形状理解的深度网络 (CVPR 2018) </font></font></b> <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Le_PointGrid_A_Deep_CVPR_2018_paper.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/trucleduc/PointGrid"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/3D-Machine-Learning/blob/master/imgs/PointGrid-%20A%20Deep%20Network%20for%203D%20Shape%20Understanding%20(2018).jpeg"><img width="50%" src="https://github.com/timzhang642/3D-Machine-Learning/raw/master/imgs/PointGrid-%20A%20Deep%20Network%20for%203D%20Shape%20Understanding%20(2018).jpeg" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">💎 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">MeshNet：用于 3D 形状表示的网状神经网络（AAAI 2019）</font></font></b> <a href="https://arxiv.org/pdf/1811.11424.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/Yue-Group/MeshNet"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/608bc1356aa65267abf327e55c658b709217a7c7036b29a0eed6bbdb0e544ec4/687474703a2f2f7777772e67616f7975652e6f72672f656e5f7473696e676875612f72657372632f6d6573686e65742e6a7067"><img width="50%" src="https://camo.githubusercontent.com/608bc1356aa65267abf327e55c658b709217a7c7036b29a0eed6bbdb0e544ec4/687474703a2f2f7777772e67616f7975652e6f72672f656e5f7473696e676875612f72657372632f6d6573686e65742e6a7067" data-canonical-src="http://www.gaoyue.org/en_tsinghua/resrc/meshnet.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">🎲 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SpiderCNN (2018) </font></font></b> <a href="https://github.com/xyf513/SpiderCNN"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a><a href="https://github.com/xyf513/SpiderCNN"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/886491f5a844811e56271c5005ab88b204b5a52abe6f91707dd1235239187a1e/687474703a2f2f356230393838653539353232352e63646e2e736f687563732e636f6d2f696d616765732f32303138313130392f34356333623637306536376634336232383837393163363530666237666230622e6a706567"><img width="50%" src="https://camo.githubusercontent.com/886491f5a844811e56271c5005ab88b204b5a52abe6f91707dd1235239187a1e/687474703a2f2f356230393838653539353232352e63646e2e736f687563732e636f6d2f696d616765732f32303138313130392f34356333623637306536376634336232383837393163363530666237666230622e6a706567" data-canonical-src="http://5b0988e595225.cdn.sohucs.com/images/20181109/45c3b670e67f43b288791c650fb7fb0b.jpeg" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">🎲 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">PointConv (2018) </font></font></b> <a href="https://github.com/DylanWusee/pointconv/tree/master/imgs"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a><a href="https://github.com/DylanWusee/pointconv/tree/master/imgs"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/d8c542d42d3d33d2900f6defc28a30e23acf2ac63c9dd1562f23f9c7be0de7c2/68747470733a2f2f70696373342e62616964752e636f6d2f666565642f386238326239303134613930663630333237326665323966383865663036316662323531656434392e6a7065673f746f6b656e3d623233653164626261656166313266666533643136386264393937613864363626733d3031333037443332384645303743303130433639433143453030303044304233"><img width="50%" src="https://camo.githubusercontent.com/d8c542d42d3d33d2900f6defc28a30e23acf2ac63c9dd1562f23f9c7be0de7c2/68747470733a2f2f70696373342e62616964752e636f6d2f666565642f386238326239303134613930663630333237326665323966383865663036316662323531656434392e6a7065673f746f6b656e3d623233653164626261656166313266666533643136386264393937613864363626733d3031333037443332384645303743303130433639433143453030303044304233" data-canonical-src="https://pics4.baidu.com/feed/8b82b9014a90f603272fe29f88ef061fb251ed49.jpeg?token=b23e1dbbaeaf12ffe3d168bd997a8d66&amp;s=01307D328FE07C010C69C1CE0000D0B3" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">💎 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">MeshCNN (SIGGRAPH 2019) </font></font></b> <a href="https://bit.ly/meshcnn" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a><a href="https://github.com/ranahanocka/MeshCNN"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><animated-image data-catalyst="" style="width: 50%;"><a target="_blank" rel="noopener noreferrer" href="https://github.com/ranahanocka/MeshCNN/blob/master/docs/imgs/alien.gif?raw=true" data-target="animated-image.originalLink"><img src="https://github.com/ranahanocka/MeshCNN/raw/master/docs/imgs/alien.gif?raw=true" style="max-width: 100%; display: inline-block;" data-target="animated-image.originalImage"></a>
      <span class="AnimatedImagePlayer" data-target="animated-image.player" hidden="">
        <a data-target="animated-image.replacedLink" class="AnimatedImagePlayer-images" href="https://github.com/ranahanocka/MeshCNN/blob/master/docs/imgs/alien.gif?raw=true" target="_blank">
          
        <span data-target="animated-image.imageContainer">
            <img data-target="animated-image.replacedImage" alt="外星人.gif?raw=true" class="AnimatedImagePlayer-animatedImage" src="https://github.com/ranahanocka/MeshCNN/raw/master/docs/imgs/alien.gif?raw=true" style="display: block; opacity: 1;">
          <canvas class="AnimatedImagePlayer-stillImage" aria-hidden="true" width="399" height="224"></canvas></span></a>
        <button data-target="animated-image.imageButton" class="AnimatedImagePlayer-images" tabindex="-1" aria-label="播放 Alien.gif?raw=true" hidden=""></button>
        <span class="AnimatedImagePlayer-controls" data-target="animated-image.controls" hidden="">
          <button data-target="animated-image.playButton" class="AnimatedImagePlayer-button" aria-label="播放 Alien.gif?raw=true">
            <svg aria-hidden="true" focusable="false" class="octicon icon-play" width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg">
              <path d="M4 13.5427V2.45734C4 1.82607 4.69692 1.4435 5.2295 1.78241L13.9394 7.32507C14.4334 7.63943 14.4334 8.36057 13.9394 8.67493L5.2295 14.2176C4.69692 14.5565 4 14.1739 4 13.5427Z">
            </path></svg>
            <svg aria-hidden="true" focusable="false" class="octicon icon-pause" width="16" height="16" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg">
              <rect x="4" y="2" width="3" height="12" rx="1"></rect>
              <rect x="9" y="2" width="3" height="12" rx="1"></rect>
            </svg>
          </button>
          <a data-target="animated-image.openButton" aria-label="在新窗口中打开 Alien.gif?raw=true" class="AnimatedImagePlayer-button" href="https://github.com/ranahanocka/MeshCNN/blob/master/docs/imgs/alien.gif?raw=true" target="_blank">
            <svg aria-hidden="true" class="octicon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" width="16" height="16">
              <path fill-rule="evenodd" d="M10.604 1h4.146a.25.25 0 01.25.25v4.146a.25.25 0 01-.427.177L13.03 4.03 9.28 7.78a.75.75 0 01-1.06-1.06l3.75-3.75-1.543-1.543A.25.25 0 0110.604 1zM3.75 2A1.75 1.75 0 002 3.75v8.5c0 .966.784 1.75 1.75 1.75h8.5A1.75 1.75 0 0014 12.25v-3.5a.75.75 0 00-1.5 0v3.5a.25.25 0 01-.25.25h-8.5a.25.25 0 01-.25-.25v-8.5a.25.25 0 01.25-.25h3.5a.75.75 0 000-1.5h-3.5z"></path>
            </svg>
          </a>
        </span>
      </span></animated-image></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">🎲 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SampleNet：可微点云采样（CVPR 2020）</font></font></b> <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Lang_SampleNet_Differentiable_Point_Cloud_Sampling_CVPR_2020_paper.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/itailang/SampleNet"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/itailang/SampleNet/blob/master/doc/teaser.png"><img width="50%" src="https://github.com/itailang/SampleNet/raw/master/doc/teaser.png" style="max-width: 100%;"></a></p>
<a name="user-content-multiple_detection">
</a><div class="markdown-heading" dir="auto"><a><h2 tabindex="-1" class="heading-element" dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">多个物体检测</font></font></h2></a><a id="user-content-multiple-objects-detection" class="anchor" aria-label="永久链接：多个对象检测" href="#multiple-objects-detection"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">用于深度图像中 3D 物体检测的滑动形状 (2014) </font></font></b> <a href="http://slidingshapes.cs.princeton.edu/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/43d6de21549c2cb5bdfef875971956285d98d2abec83483a39a3c04811ea0df4/687474703a2f2f736c6964696e677368617065732e63732e7072696e6365746f6e2e6564752f7465617365722e6a7067"><img width="50%" src="https://camo.githubusercontent.com/43d6de21549c2cb5bdfef875971956285d98d2abec83483a39a3c04811ea0df4/687474703a2f2f736c6964696e677368617065732e63732e7072696e6365746f6e2e6564752f7465617365722e6a7067" data-canonical-src="http://slidingshapes.cs.princeton.edu/teaser.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">在多视图图像中使用 CNN 进行 3D 场景中的对象检测 (2016) </font></font></b> <a href="https://stanford.edu/class/ee367/Winter2016/Qi_Report.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/3D-Machine-Learning/blob/master/imgs/Object%20Detection%20in%203D%20Scenes%20Using%20CNNs%20in%20Multi-view%20Images.png"><img width="50%" src="https://github.com/timzhang642/3D-Machine-Learning/raw/master/imgs/Object%20Detection%20in%203D%20Scenes%20Using%20CNNs%20in%20Multi-view%20Images.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">RGB-D 图像中用于 Amodal 3D 物体检测的深度滑动形状 (2016) </font></font></b> <a href="http://dss.cs.princeton.edu/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/shurans/DeepSlidingShape"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/16382da3dcad7d501aa1ed54eb35860f2196e80f3dcf2d47c52f7ed90161a83d/687474703a2f2f3364766973696f6e2e7072696e6365746f6e2e6564752f736c6964652f4453532e6a7067"><img width="50%" src="https://camo.githubusercontent.com/16382da3dcad7d501aa1ed54eb35860f2196e80f3dcf2d47c52f7ed90161a83d/687474703a2f2f3364766973696f6e2e7072696e6365746f6e2e6564752f736c6964652f4453532e6a7067" data-canonical-src="http://3dvision.princeton.edu/slide/DSS.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">使用定向梯度云进行三维对象检测和布局预测 (2016) </font></font></b> <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Ren_Three-Dimensional_Object_Detection_CVPR_2016_paper.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[CVPR '16 论文] </font></font></a> <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Ren_3D_Object_Detection_CVPR_2018_paper.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[CVPR '18 论文] </font></font></a> <a href="https://arxiv.org/pdf/1906.04725" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[T-PAMI '19 论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/luvegood/3D-Machine-Learning/blob/master/imgs/Three-Dimensional%20Object%20Detection%20and%20Layout%20Prediction%20using%20Clouds%20of%20Oriented%20Gradients.png"><img width="50%" src="https://github.com/luvegood/3D-Machine-Learning/raw/master/imgs/Three-Dimensional%20Object%20Detection%20and%20Layout%20Prediction%20using%20Clouds%20of%20Oriented%20Gradients.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">DeepContext：用于 3D 整体场景理解的上下文编码神经通路（2016）</font></font></b> <a href="http://deepcontext.cs.princeton.edu/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/f7da98d35ce134e4d41f88e262eacc5b5aa4c0b45590e715b460c90ce531313d/687474703a2f2f64656570636f6e746578742e63732e7072696e6365746f6e2e6564752f7465617365722e706e67"><img width="50%" src="https://camo.githubusercontent.com/f7da98d35ce134e4d41f88e262eacc5b5aa4c0b45590e715b460c90ce531313d/687474703a2f2f64656570636f6e746578742e63732e7072696e6365746f6e2e6564752f7465617365722e706e67" data-canonical-src="http://deepcontext.cs.princeton.edu/teaser.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SUN RGB-D：RGB-D 场景理解基准套件（2017）</font></font></b> <a href="http://rgbd.cs.princeton.edu/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/f122922f0dd2d5019a89bf04c9689c28e41399e77543d982968951e0bad5a4d1/687474703a2f2f726762642e63732e7072696e6365746f6e2e6564752f7465617365722e6a7067"><img width="50%" src="https://camo.githubusercontent.com/f122922f0dd2d5019a89bf04c9689c28e41399e77543d982968951e0bad5a4d1/687474703a2f2f726762642e63732e7072696e6365746f6e2e6564752f7465617365722e6a7067" data-canonical-src="http://rgbd.cs.princeton.edu/teaser.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">VoxelNet：基于点云的 3D 对象检测的端到端学习（2017）</font></font></b> <a href="https://arxiv.org/pdf/1711.06396.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/06ca5a757c817eaab364df9ce89946df8d187a5f815c45c35417843b6d5487e2/68747470733a2f2f7062732e7477696d672e636f6d2f6d656469612f44504d744c68485855416351556a322e6a7067"><img width="50%" src="https://camo.githubusercontent.com/06ca5a757c817eaab364df9ce89946df8d187a5f815c45c35417843b6d5487e2/68747470733a2f2f7062732e7477696d672e636f6d2f6d656469612f44504d744c68485855416351556a322e6a7067" data-canonical-src="https://pbs.twimg.com/media/DPMtLhHXUAcQUj2.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">用于从 RGB-D 数据检测 3D 对象的 Frustum PointNets (CVPR2018) </font></font></b> <a href="https://arxiv.org/pdf/1711.08488.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/87487e5ca1e37fe90aa96423ef03d280ef06edbe3369e084af5e5a5db58bf153/687474703a2f2f7374616e666f72642e6564752f7e7271692f6672757374756d2d706f696e746e6574732f696d616765732f7465617365722e6a7067"><img width="50%" src="https://camo.githubusercontent.com/87487e5ca1e37fe90aa96423ef03d280ef06edbe3369e084af5e5a5db58bf153/687474703a2f2f7374616e666f72642e6564752f7e7271692f6672757374756d2d706f696e746e6574732f696d616765732f7465617365722e6a7067" data-canonical-src="http://stanford.edu/~rqi/frustum-pointnets/images/teaser.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A^2-Net：根据冷冻电镜密度体积估计分子结构（AAAI2019）</font></font></b> <a href="https://arxiv.org/abs/1901.00785" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="/timzhang642/3D-Machine-Learning/blob/master/imgs/a-square-net-min.jpg"><img width="50%" src="/timzhang642/3D-Machine-Learning/raw/master/imgs/a-square-net-min.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">基于 Stereo R-CNN 的自动驾驶 3D 物体检测 (CVPR2019) </font></font></b> <a href="https://arxiv.org/abs/1902.09738v1" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/eb0c7cab1e75c4dcf08daf7b8a17e16329e2be9e4f5388a147e89b97d54e7241/68747470733a2f2f7777772e67726f756e6461692e636f6d2f6d656469612f61727869765f70726f6a656374732f3531353333382f73797374656d5f6e65776e65772e706e67"><img width="50%" src="https://camo.githubusercontent.com/eb0c7cab1e75c4dcf08daf7b8a17e16329e2be9e4f5388a147e89b97d54e7241/68747470733a2f2f7777772e67726f756e6461692e636f6d2f6d656469612f61727869765f70726f6a656374732f3531353333382f73797374656d5f6e65776e65772e706e67" data-canonical-src="https://www.groundai.com/media/arxiv_projects/515338/system_newnew.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">用于点云中 3D 对象检测的深度霍夫投票 (ICCV2019) </font></font></b> <a href="https://arxiv.org/pdf/1904.09664.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/facebookresearch/votenet"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/facebookresearch/votenet/blob/master/doc/teaser.jpg"><img width="50%" src="https://github.com/facebookresearch/votenet/raw/master/doc/teaser.jpg" style="max-width: 100%;"></a></p>
<a name="user-content-segmentation">
</a><div class="markdown-heading" dir="auto"><a><h2 tabindex="-1" class="heading-element" dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">场景/对象语义分割</font></font></h2></a><a id="user-content-sceneobject-semantic-segmentation" class="anchor" aria-label="永久链接：场景/对象语义分割" href="#sceneobject-semantic-segmentation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">学习 3D 网格分割和标记 (2010) </font></font></b> <a href="https://people.cs.umass.edu/~kalo/papers/LabelMeshes/LabelMeshes.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/ca4b48f0f4276569e5b0cd9c5398d4861c9086c4756f88ba5b4181428f6250d6/68747470733a2f2f6169322d73322d7075626c69632e73332e616d617a6f6e6177732e636f6d2f666967757265732f323031362d31312d30382f306266333930653261313466373462636338383338643566623163306334636336306539326562372f372d466967757265372d312e706e67"><img width="50%" src="https://camo.githubusercontent.com/ca4b48f0f4276569e5b0cd9c5398d4861c9086c4756f88ba5b4181428f6250d6/68747470733a2f2f6169322d73322d7075626c69632e73332e616d617a6f6e6177732e636f6d2f666967757265732f323031362d31312d30382f306266333930653261313466373462636338383338643566623163306334636336306539326562372f372d466967757265372d312e706e67" data-canonical-src="https://ai2-s2-public.s3.amazonaws.com/figures/2016-11-08/0bf390e2a14f74bcc8838d5fb1c0c4cc60e92eb7/7-Figure7-1.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">通过描述符空间谱聚类对一组形状进行无监督联合分割（2011）</font></font></b> <a href="https://www.cs.sfu.ca/~haoz/pubs/sidi_siga11_coseg.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/4ebea4934560f1cd487dfecc65b9a536e739617fb1e4103607600398606a55b3/687474703a2f2f70656f706c652e7363732e6361726c65746f6e2e63612f7e6f6c6976657276616e6b6169636b2f636f7365676d656e746174696f6e2f726573756c7473362e706e67"><img width="30%" src="https://camo.githubusercontent.com/4ebea4934560f1cd487dfecc65b9a536e739617fb1e4103607600398606a55b3/687474703a2f2f70656f706c652e7363732e6361726c65746f6e2e63612f7e6f6c6976657276616e6b6169636b2f636f7365676d656e746174696f6e2f726573756c7473362e706e67" data-canonical-src="http://people.scs.carleton.ca/~olivervankaick/cosegmentation/results6.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">通过图像和形状集合的联合分析进行单视图重建 (2015) </font></font></b> <a href="https://www.cs.utexas.edu/~huangqx/modeling_sig15.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/huangqx/image_shape_align"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/2ba8030b349f65aaabc4d08e4c228cd290b341f342ece51f0baa0035bf453f56/687474703a2f2f766c61646c656e2e696e666f2f77702d636f6e74656e742f75706c6f6164732f323031352f30352f73696e676c652d766965772e706e67"><img width="50%" src="https://camo.githubusercontent.com/2ba8030b349f65aaabc4d08e4c228cd290b341f342ece51f0baa0035bf453f56/687474703a2f2f766c61646c656e2e696e666f2f77702d636f6e74656e742f75706c6f6164732f323031352f30352f73696e676c652d766965772e706e67" data-canonical-src="http://vladlen.info/wp-content/uploads/2015/05/single-view.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">使用投影卷积网络进行 3D 形状分割 (2017) </font></font></b> <a href="http://people.cs.umass.edu/~kalo/papers/shapepfcn/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/kalov/ShapePFCN"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/7c52098c20a89209d8e61fcde13eadcb7d62b82ee3b7c7932863807271c76f1a/687474703a2f2f70656f706c652e63732e756d6173732e6564752f7e6b616c6f2f7061706572732f73686170657066636e2f7465617365722e6a7067"><img width="50%" src="https://camo.githubusercontent.com/7c52098c20a89209d8e61fcde13eadcb7d62b82ee3b7c7932863807271c76f1a/687474703a2f2f70656f706c652e63732e756d6173732e6564752f7e6b616c6f2f7061706572732f73686170657066636e2f7465617365722e6a7067" data-canonical-src="http://people.cs.umass.edu/~kalo/papers/shapepfcn/teaser.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">从在线存储库学习分层形状分割和标签（2017）</font></font></b> <a href="http://cs.stanford.edu/~ericyi/project_page/hier_seg/index.html" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/65f85102d2dd68801a7e24d57f91cccb12eb949d8bec619f5ab787adaf0767e4/687474703a2f2f63732e7374616e666f72642e6564752f7e6572696379692f70726f6a6563745f706167652f686965725f7365672f666967757265732f7465617365722e6a7067"><img width="50%" src="https://camo.githubusercontent.com/65f85102d2dd68801a7e24d57f91cccb12eb949d8bec619f5ab787adaf0767e4/687474703a2f2f63732e7374616e666f72642e6564752f7e6572696379692f70726f6a6563745f706167652f686965725f7365672f666967757265732f7465617365722e6a7067" data-canonical-src="http://cs.stanford.edu/~ericyi/project_page/hier_seg/figures/teaser.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">👾 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ScanNet (2017) </font></font></b> <a href="https://arxiv.org/pdf/1702.04405.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/scannet/scannet"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/5ceac8f7f34847b2ddd4d470190035db5f7db99e4b56cd6930bcef6c7b965832/687474703a2f2f7777772e7363616e2d6e65742e6f72672f696d672f766f78656c2d70726564696374696f6e732e6a7067"><img width="50%" src="https://camo.githubusercontent.com/5ceac8f7f34847b2ddd4d470190035db5f7db99e4b56cd6930bcef6c7b965832/687474703a2f2f7777772e7363616e2d6e65742e6f72672f696d672f766f78656c2d70726564696374696f6e732e6a7067" data-canonical-src="http://www.scan-net.org/img/voxel-predictions.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">🎲 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">PointNet：用于 3D 分类和分割的点集深度学习 (2017) </font></font></b> <a href="http://stanford.edu/~rqi/pointnet/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/charlesq34/pointnet"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/a05a099e9614ae8d47902075ebb1a2b8be8b58bab56493238defd7d936f9197f/68747470733a2f2f7765622e7374616e666f72642e6564752f7e7271692f7061706572732f706f696e746e65742e706e67"><img width="40%" src="https://camo.githubusercontent.com/a05a099e9614ae8d47902075ebb1a2b8be8b58bab56493238defd7d936f9197f/68747470733a2f2f7765622e7374616e666f72642e6564752f7e7271692f7061706572732f706f696e746e65742e706e67" data-canonical-src="https://web.stanford.edu/~rqi/papers/pointnet.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">🎲 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">PointNet++：度量空间中点集的深度层次特征学习（2017）</font></font></b> <a href="https://arxiv.org/pdf/1706.02413.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/charlesq34/pointnet2"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/3D-Machine-Learning/blob/master/imgs/PointNet%2B%2B-%20Deep%20Hierarchical%20Feature%20Learning%20on%20Point%20Sets%20in%20a%20Metric%20Space.png"><img width="40%" src="https://github.com/timzhang642/3D-Machine-Learning/raw/master/imgs/PointNet%2B%2B-%20Deep%20Hierarchical%20Feature%20Learning%20on%20Point%20Sets%20in%20a%20Metric%20Space.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">🎲</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">用&ZeroWidthSpace;&ZeroWidthSpace;于 RGBD 语义分割的 3D 图神经网络 (2017) </font></font></b> <a href="http://www.cs.toronto.edu/~rjliao/papers/iccv_2017_3DGNN.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/ceee3ef79cf3415e5de7355e3d3815ff9d26d49bcc90467fe5c1dbf9ece34895/687474703a2f2f7777772e666f6e6f772e636f6d2f496d616765732f323031372d31302d31382f36363337322d32303137313031383131353830393734302d323132353232373235302e6a7067"><img width="40%" src="https://camo.githubusercontent.com/ceee3ef79cf3415e5de7355e3d3815ff9d26d49bcc90467fe5c1dbf9ece34895/687474703a2f2f7777772e666f6e6f772e636f6d2f496d616765732f323031372d31302d31382f36363337322d32303137313031383131353830393734302d323132353232373235302e6a7067" data-canonical-src="http://www.fonow.com/Images/2017-10-18/66372-20171018115809740-2125227250.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">🎲 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3DCNN-DQN-RNN：用于大规模 3D 点云语义解析的深度强化学习框架（2017）</font></font></b> <a href="https://arxiv.org/pdf/1707.06783.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/3D-Machine-Learning/blob/master/imgs/3DCNN-DQN-RNN.png"><img width="40%" src="https://github.com/timzhang642/3D-Machine-Learning/raw/master/imgs/3DCNN-DQN-RNN.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">🎲👾</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">使用卷积神经网络对室内点云进行语义分割（2017）</font></font></b> <a href="https://www.isprs-ann-photogramm-remote-sens-spatial-inf-sci.net/IV-4-W4/101/2017/isprs-annals-IV-4-W4-101-2017.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/3D-Machine-Learning/blob/master/imgs/Semantic Segmentation of Indoor Point Clouds using Convolutional Neural Networks.png"><img width="55%" src="https://github.com/timzhang642/3D-Machine-Learning/raw/master/imgs/Semantic Segmentation of Indoor Point Clouds using Convolutional Neural Networks.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">🎲👾 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SEGCloud：3D 点云的语义分割（2017）</font></font></b> <a href="https://arxiv.org/pdf/1710.07563.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/3D-Machine-Learning/blob/master/imgs/SEGCloud.png"><img width="55%" src="https://github.com/timzhang642/3D-Machine-Learning/raw/master/imgs/SEGCloud.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">🎲👾 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ShapeNet Core55 的大规模 3D 形状重建和分割（2017）</font></font></b> <a href="https://arxiv.org/pdf/1710.06104.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/3D-Machine-Learning/blob/master/imgs/Core55.png"><img width="40%" src="https://github.com/timzhang642/3D-Machine-Learning/raw/master/imgs/Core55.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">🎲</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">点状卷积神经网络（CVPR 2018）</font></font></b> <a href="http://pointwise.scenenn.net/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[链接]</font></font></a>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
我们提出了点状卷积，它可以执行动态体素化来学习点云的局部特征。</font></font></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/20e40f981fdf6a0b564afbea176e749053569744db032a45ebacdef67c6b8dd0/687474703a2f2f706f696e74776973652e7363656e656e6e2e6e65742f696d616765732f7465617365722e706e67"><img width="50%" src="https://camo.githubusercontent.com/20e40f981fdf6a0b564afbea176e749053569744db032a45ebacdef67c6b8dd0/687474703a2f2f706f696e74776973652e7363656e656e6e2e6e65742f696d616765732f7465617365722e706e67" data-canonical-src="http://pointwise.scenenn.net/images/teaser.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">🎲</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">用&ZeroWidthSpace;&ZeroWidthSpace;于点云学习的动态图 CNN（2018）</font></font></b> <a href="https://arxiv.org/pdf/1801.07829.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/c5ae8c4f32a922013a9d382bf4f2cadd2f17bd49f2a6207fac0aad8a6b2a2f1c/68747470733a2f2f6c69757a69776569372e6769746875622e696f2f686f6d65706167655f66696c65732f64796e616d696367636e6e5f6c6f676f2e706e67"><img width="50%" src="https://camo.githubusercontent.com/c5ae8c4f32a922013a9d382bf4f2cadd2f17bd49f2a6207fac0aad8a6b2a2f1c/68747470733a2f2f6c69757a69776569372e6769746875622e696f2f686f6d65706167655f66696c65732f64796e616d696367636e6e5f6c6f676f2e706e67" data-canonical-src="https://liuziwei7.github.io/homepage_files/dynamicgcnn_logo.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">🎲 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">PointCNN (2018) </font></font></b> <a href="https://yangyanli.github.io/PointCNN/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/c5f93015d4948dd7cb9c1307f7c4488a1dd657c81c6af6f550ce6aa3c87d458e/687474703a2f2f79616e6779616e2e6c692f696d616765732f70617065722f706f696e74636e6e2e706e67"><img width="50%" src="https://camo.githubusercontent.com/c5f93015d4948dd7cb9c1307f7c4488a1dd657c81c6af6f550ce6aa3c87d458e/687474703a2f2f79616e6779616e2e6c692f696d616765732f70617065722f706f696e74636e6e2e706e67" data-canonical-src="http://yangyan.li/images/paper/pointcnn.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">📷👾 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3DMV：3D 语义场景分割的联合 3D 多视图预测（2018）</font></font></b> <a href="https://arxiv.org/pdf/1803.10409.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/angeladai/3DMV/blob/master/images/teaser.jpg"><img width="50%" src="https://github.com/angeladai/3DMV/raw/master/images/teaser.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">👾 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ScanComplete：3D 扫描的大规模场景补全和语义分割（2018）</font></font></b> <a href="https://arxiv.org/pdf/1712.10215.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/angeladai/ScanComplete/blob/master/images/teaser_mesh.jpg"><img width="50%" src="https://github.com/angeladai/ScanComplete/raw/master/images/teaser_mesh.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">🎲📷 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SPLATNet：用于点云处理的稀疏格子网络（2018）</font></font></b> <a href="https://arxiv.org/pdf/1802.08275.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/3D-Machine-Learning/blob/master/imgs/SPLATNet-%20Sparse%20Lattice%20Networks%20for%20Point%20Cloud%20Processing.jpeg"><img width="50%" src="https://github.com/timzhang642/3D-Machine-Learning/raw/master/imgs/SPLATNet-%20Sparse%20Lattice%20Networks%20for%20Point%20Cloud%20Processing.jpeg" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">🎲👾 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">PointGrid：用于 3D 形状理解的深度网络 (CVPR 2018) </font></font></b> <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Le_PointGrid_A_Deep_CVPR_2018_paper.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/trucleduc/PointGrid"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/3D-Machine-Learning/blob/master/imgs/PointGrid-%20A%20Deep%20Network%20for%203D%20Shape%20Understanding%20(2018).jpeg"><img width="50%" src="https://github.com/timzhang642/3D-Machine-Learning/raw/master/imgs/PointGrid-%20A%20Deep%20Network%20for%203D%20Shape%20Understanding%20(2018).jpeg" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">🎲 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">PointConv (2018) </font></font></b> <a href="https://github.com/DylanWusee/pointconv/tree/master/imgs"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a><a href="https://github.com/DylanWusee/pointconv/tree/master/imgs"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/d8c542d42d3d33d2900f6defc28a30e23acf2ac63c9dd1562f23f9c7be0de7c2/68747470733a2f2f70696373342e62616964752e636f6d2f666565642f386238326239303134613930663630333237326665323966383865663036316662323531656434392e6a7065673f746f6b656e3d623233653164626261656166313266666533643136386264393937613864363626733d3031333037443332384645303743303130433639433143453030303044304233"><img width="50%" src="https://camo.githubusercontent.com/d8c542d42d3d33d2900f6defc28a30e23acf2ac63c9dd1562f23f9c7be0de7c2/68747470733a2f2f70696373342e62616964752e636f6d2f666565642f386238326239303134613930663630333237326665323966383865663036316662323531656434392e6a7065673f746f6b656e3d623233653164626261656166313266666533643136386264393937613864363626733d3031333037443332384645303743303130433639433143453030303044304233" data-canonical-src="https://pics4.baidu.com/feed/8b82b9014a90f603272fe29f88ef061fb251ed49.jpeg?token=b23e1dbbaeaf12ffe3d168bd997a8d66&amp;s=01307D328FE07C010C69C1CE0000D0B3" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">🎲 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SpiderCNN (2018) </font></font></b> <a href="https://github.com/xyf513/SpiderCNN"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a><a href="https://github.com/xyf513/SpiderCNN"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/886491f5a844811e56271c5005ab88b204b5a52abe6f91707dd1235239187a1e/687474703a2f2f356230393838653539353232352e63646e2e736f687563732e636f6d2f696d616765732f32303138313130392f34356333623637306536376634336232383837393163363530666237666230622e6a706567"><img width="50%" src="https://camo.githubusercontent.com/886491f5a844811e56271c5005ab88b204b5a52abe6f91707dd1235239187a1e/687474703a2f2f356230393838653539353232352e63646e2e736f687563732e636f6d2f696d616765732f32303138313130392f34356333623637306536376634336232383837393163363530666237666230622e6a706567" data-canonical-src="http://5b0988e595225.cdn.sohucs.com/images/20181109/45c3b670e67f43b288791c650fb7fb0b.jpeg" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">👾 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3D-SIS：RGB-D 扫描的 3D 语义实例分割（CVPR 2019）</font></font></b> <a href="https://arxiv.org/pdf/1812.07003.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a><a href="https://github.com/Sekunde/3D-SIS"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/ec0b50d3a7ba08067a3a27d01f7c0e72176af55a292aa5407850424a80d9af3f/687474703a2f2f7777772e6e696573736e65726c61622e6f72672f7061706572732f323031392f367369732f7465617365722e6a7067"><img width="50%" src="https://camo.githubusercontent.com/ec0b50d3a7ba08067a3a27d01f7c0e72176af55a292aa5407850424a80d9af3f/687474703a2f2f7777772e6e696573736e65726c61622e6f72672f7061706572732f323031392f367369732f7465617365722e6a7067" data-canonical-src="http://www.niessnerlab.org/papers/2019/6sis/teaser.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">🎲</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">室内场景实时渐进 3D 语义分割 (WACV 2019) </font></font></b> <a href="https://pqhieu.github.io/research/proseg/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[链接]</font></font></a>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
我们提出了一种高效而稳健的技术，用于 3D 室内场景的动态密集重建和语义分割。</font><font style="vertical-align: inherit;">我们的方法建立在高效的超级体素聚类方法和条件随机场之上，具有来自结构和对象线索的高阶约束，无需任何预计算即可实现渐进式密集语义分割。</font></font></p>
<p align="center" dir="auto"><animated-image data-catalyst="" style="width: 50%;"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/4e1821c5802f56ae1d82704ed261b1690036b8f15f9d09ff87885c35b2847213/68747470733a2f2f7071686965752e6769746875622e696f2f6d656469612f696d616765732f7761637631392f7468756d626e61696c2e676966" data-target="animated-image.originalLink" hidden=""><img src="https://camo.githubusercontent.com/4e1821c5802f56ae1d82704ed261b1690036b8f15f9d09ff87885c35b2847213/68747470733a2f2f7071686965752e6769746875622e696f2f6d656469612f696d616765732f7761637631392f7468756d626e61696c2e676966" data-canonical-src="https://pqhieu.github.io/media/images/wacv19/thumbnail.gif" style="max-width: 100%;" data-target="animated-image.originalImage" hidden=""></a>
      <span class="AnimatedImagePlayer" data-target="animated-image.player">
        <a data-target="animated-image.replacedLink" class="AnimatedImagePlayer-images" href="https://camo.githubusercontent.com/4e1821c5802f56ae1d82704ed261b1690036b8f15f9d09ff87885c35b2847213/68747470733a2f2f7071686965752e6769746875622e696f2f6d656469612f696d616765732f7761637631392f7468756d626e61696c2e676966" target="_blank">
          <span data-target="animated-image.imageContainer">
            <img data-target="animated-image.replacedImage" alt="" class="AnimatedImagePlayer-animatedImage" src="https://camo.githubusercontent.com/4e1821c5802f56ae1d82704ed261b1690036b8f15f9d09ff87885c35b2847213/68747470733a2f2f7071686965752e6769746875622e696f2f6d656469612f696d616765732f7761637631392f7468756d626e61696c2e676966">
          </span>
        </a>
        <button data-target="animated-image.imageButton" class="AnimatedImagePlayer-images" tabindex="-1"></button>
        <span class="AnimatedImagePlayer-controls" data-target="animated-image.controls">
          <button data-target="animated-image.playButton" class="AnimatedImagePlayer-button">
            <svg aria-hidden="true" focusable="false" class="octicon icon-play" width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg">
              <path d="M4 13.5427V2.45734C4 1.82607 4.69692 1.4435 5.2295 1.78241L13.9394 7.32507C14.4334 7.63943 14.4334 8.36057 13.9394 8.67493L5.2295 14.2176C4.69692 14.5565 4 14.1739 4 13.5427Z">
            </path></svg>
            <svg aria-hidden="true" focusable="false" class="octicon icon-pause" width="16" height="16" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg">
              <rect x="4" y="2" width="3" height="12" rx="1"></rect>
              <rect x="9" y="2" width="3" height="12" rx="1"></rect>
            </svg>
          </button>
          <a data-target="animated-image.openButton" aria-label="在新窗口中打开" class="AnimatedImagePlayer-button" href="https://camo.githubusercontent.com/4e1821c5802f56ae1d82704ed261b1690036b8f15f9d09ff87885c35b2847213/68747470733a2f2f7071686965752e6769746875622e696f2f6d656469612f696d616765732f7761637631392f7468756d626e61696c2e676966" target="_blank">
            <svg aria-hidden="true" class="octicon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" width="16" height="16">
              <path fill-rule="evenodd" d="M10.604 1h4.146a.25.25 0 01.25.25v4.146a.25.25 0 01-.427.177L13.03 4.03 9.28 7.78a.75.75 0 01-1.06-1.06l3.75-3.75-1.543-1.543A.25.25 0 0110.604 1zM3.75 2A1.75 1.75 0 002 3.75v8.5c0 .966.784 1.75 1.75 1.75h8.5A1.75 1.75 0 0014 12.25v-3.5a.75.75 0 00-1.5 0v3.5a.25.25 0 01-.25.25h-8.5a.25.25 0 01-.25-.25v-8.5a.25.25 0 01.25-.25h3.5a.75.75 0 000-1.5h-3.5z"></path>
            </svg>
          </a>
        </span>
      </span></animated-image></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">🎲 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">JSIS3D：3D 点云的联合语义实例分割（CVPR 2019）</font></font></b> <a href="https://pqhieu.github.io/research/jsis3d/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[链接]</font></font></a>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
我们使用多任务逐点网络联合解决 3D 点云的语义和实例分割问题，该网络同时执行两个任务：预测语义类3D 点并将这些点嵌入到高维向量中，以便同一对象实例的点由相似的嵌入表示。</font><font style="vertical-align: inherit;">然后，我们提出了一种多值条件随机场模型来合并语义和实例标签，并将语义和实例分割问题表述为联合优化场模型中的标签。</font></font></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="/timzhang642/3D-Machine-Learning/blob/master/imgs/jsis3d.png"><img width="50%" src="/timzhang642/3D-Machine-Learning/raw/master/imgs/jsis3d.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">🎲 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ShellNet：使用同心壳统计的高效点云卷积神经网络（ICCV 2019）</font></font></b> <a href="https://hkust-vgd.github.io/shellnet/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[链接]</font></font></a>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
我们提出了一种用于点云深度学习的高效端到端排列不变卷积。</font><font style="vertical-align: inherit;">我们使用同心球壳的统计数据来定义代表性特征并解决点顺序模糊性，从而允许传统卷积在此类特征上高效执行。</font></font></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/4c30da36cc17f79ff4feefa81f491825fc143b041583870fb3270b9508025d94/68747470733a2f2f686b7573742d7667642e6769746875622e696f2f7368656c6c6e65742f696d616765732f7368656c6c636f6e765f6e65772e706e67"><img width="50%" src="https://camo.githubusercontent.com/4c30da36cc17f79ff4feefa81f491825fc143b041583870fb3270b9508025d94/68747470733a2f2f686b7573742d7667642e6769746875622e696f2f7368656c6c6e65742f696d616765732f7368656c6c636f6e765f6e65772e706e67" data-canonical-src="https://hkust-vgd.github.io/shellnet/images/shellconv_new.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">🎲 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3D 点云深度学习的旋转不变卷积（3DV 2019）</font></font></b> <a href="https://hkust-vgd.github.io/riconv/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[链接]</font></font></a>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
我们引入了一种新颖的点云卷积算子，可以实现旋转不变性。</font><font style="vertical-align: inherit;">我们的核心思想是利用距离和角度等低级旋转不变几何特征来设计用于点云学习的卷积算子。</font></font></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/cc90d678f35f8ea19ff10a01c16a957a2e2ed7a8c6907b73a3bccd2494c86b1a/68747470733a2f2f686b7573742d7667642e6769746875622e696f2f7269636f6e762f696d616765732f52494f5f63616d2e706e67"><img width="50%" src="https://camo.githubusercontent.com/cc90d678f35f8ea19ff10a01c16a957a2e2ed7a8c6907b73a3bccd2494c86b1a/68747470733a2f2f686b7573742d7667642e6769746875622e696f2f7269636f6e762f696d616765732f52494f5f63616d2e706e67" data-canonical-src="https://hkust-vgd.github.io/riconv/images/RIO_cam.png" style="max-width: 100%;"></a></p>
<a name="user-content-3d_synthesis">
</a><div class="markdown-heading" dir="auto"><a><h2 tabindex="-1" class="heading-element" dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3D模型合成/重建</font></font></h2></a><a id="user-content-3d-model-synthesisreconstruction" class="anchor" aria-label="永久链接：3D 模型合成/重建" href="#3d-model-synthesisreconstruction"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<a name="user-content-3d_synthesis_model_based">
</a><div class="markdown-heading" dir="auto"><a><h3 tabindex="-1" class="heading-element" dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">基于参数可变形模型的方法</font></font></h3></a><a id="user-content-parametric-morphable-model-based-methods" class="anchor" aria-label="永久链接：基于参数可变形模型的方法" href="#parametric-morphable-model-based-methods"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">用于合成 3D 人脸的可变形模型 (1999) </font></font></b> <a href="http://gravis.dmi.unibas.ch/publications/Sigg99/morphmod2.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a><a href="https://github.com/MichaelMure/3DMM"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/124a1d4dd5fbdf9907a0fd78ff33fe4cb92233ed0eba7c60d22e67e67a11f23e/687474703a2f2f6d626c6f677468756d62332e7068696e662e6e617665722e6e65742f4d6a41784e7a417a4d5464664d6a637a2f4d4441784e4467354e7a45334d7a55304f4449332e396c51696f4c78776f476d746f49565858397362564f7a68657a6f71674b4d4b69546f76426e6255464e30672e73584e357447344b6f68676b374f4a4574506e75782d6d76374f416f5856787843796f3353475a4d633659672e504e472e6174656c6965726a70726f2f3033313731375f303232325f4461746144726976656e53342e706e673f747970653d77343230"><img width="40%" src="https://camo.githubusercontent.com/124a1d4dd5fbdf9907a0fd78ff33fe4cb92233ed0eba7c60d22e67e67a11f23e/687474703a2f2f6d626c6f677468756d62332e7068696e662e6e617665722e6e65742f4d6a41784e7a417a4d5464664d6a637a2f4d4441784e4467354e7a45334d7a55304f4449332e396c51696f4c78776f476d746f49565858397362564f7a68657a6f71674b4d4b69546f76426e6255464e30672e73584e357447344b6f68676b374f4a4574506e75782d6d76374f416f5856787843796f3353475a4d633659672e504e472e6174656c6965726a70726f2f3033313731375f303232325f4461746144726976656e53342e706e673f747970653d77343230" data-canonical-src="http://mblogthumb3.phinf.naver.net/MjAxNzAzMTdfMjcz/MDAxNDg5NzE3MzU0ODI3.9lQioLxwoGmtoIVXX9sbVOzhezoqgKMKiTovBnbUFN0g.sXN5tG4Kohgk7OJEtPnux-mv7OAoXVxxCyo3SGZMc6Yg.PNG.atelierjpro/031717_0222_DataDrivenS4.png?type=w420" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">FLAME：通过铰接模型和表达式学习面部 (2017) </font></font></b> <a href="https://ps.is.tuebingen.mpg.de/uploads_file/attachment/attachment/400/paper.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a><a href="https://github.com/Rubikplayer/flame-fitting"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码 (Chumpy)] </font></font></a><a href="https://github.com/TimoBolkart/TF_FLAME"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码 (TF)] </font></font></a> <a href="https://github.com/HavenFeng/photometric_optimization"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码 (PyTorch)] </font></font></a>
<br><a href="http://flame.is.tue.mpg.de/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">FLAME</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">是一种轻量级且富有表现力的通用头部模型，从超过 33,000 个精确对齐的头部模型中学习3D 扫描。</font><font style="vertical-align: inherit;">该模型将线性身份形状空间（通过 3800 个人类头部扫描进行训练）与铰接式颈部、下巴和眼球、依赖于姿势的校正混合形状以及其他全局表达混合形状相结合。</font><font style="vertical-align: inherit;">该代码演示了如何 1) 从图像重建纹理 3D 面部，2) 将模型拟合到 3D 地标或注册的 3D 网格，或 3) 为</font></font><a href="https://github.com/TimoBolkart/voca"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">语音驱动的面部动画</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">生成 3D 面部模板。</font></font></p>
<p align="center" dir="auto"> <animated-image data-catalyst="" style="width: 50%;"><a target="_blank" rel="noopener noreferrer" href="https://github.com/TimoBolkart/TF_FLAME/blob/master/gifs/model_variations.gif" data-target="animated-image.originalLink"><img src="https://github.com/TimoBolkart/TF_FLAME/raw/master/gifs/model_variations.gif" style="max-width: 100%; display: inline-block;" data-target="animated-image.originalImage"></a>
      <span class="AnimatedImagePlayer" data-target="animated-image.player" hidden="">
        <a data-target="animated-image.replacedLink" class="AnimatedImagePlayer-images" href="https://github.com/TimoBolkart/TF_FLAME/blob/master/gifs/model_variations.gif" target="_blank">
          
        <span data-target="animated-image.imageContainer">
            <img data-target="animated-image.replacedImage" alt="model_variations.gif" class="AnimatedImagePlayer-animatedImage" src="https://github.com/TimoBolkart/TF_FLAME/raw/master/gifs/model_variations.gif" style="display: block; opacity: 1;">
          <canvas class="AnimatedImagePlayer-stillImage" aria-hidden="true" width="399" height="164"></canvas></span></a>
        <button data-target="animated-image.imageButton" class="AnimatedImagePlayer-images" tabindex="-1" aria-label="播放 model_variations.gif" hidden=""></button>
        <span class="AnimatedImagePlayer-controls" data-target="animated-image.controls" hidden="">
          <button data-target="animated-image.playButton" class="AnimatedImagePlayer-button" aria-label="播放 model_variations.gif">
            <svg aria-hidden="true" focusable="false" class="octicon icon-play" width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg">
              <path d="M4 13.5427V2.45734C4 1.82607 4.69692 1.4435 5.2295 1.78241L13.9394 7.32507C14.4334 7.63943 14.4334 8.36057 13.9394 8.67493L5.2295 14.2176C4.69692 14.5565 4 14.1739 4 13.5427Z">
            </path></svg>
            <svg aria-hidden="true" focusable="false" class="octicon icon-pause" width="16" height="16" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg">
              <rect x="4" y="2" width="3" height="12" rx="1"></rect>
              <rect x="9" y="2" width="3" height="12" rx="1"></rect>
            </svg>
          </button>
          <a data-target="animated-image.openButton" aria-label="在新窗口中打开 model_variations.gif" class="AnimatedImagePlayer-button" href="https://github.com/TimoBolkart/TF_FLAME/blob/master/gifs/model_variations.gif" target="_blank">
            <svg aria-hidden="true" class="octicon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" width="16" height="16">
              <path fill-rule="evenodd" d="M10.604 1h4.146a.25.25 0 01.25.25v4.146a.25.25 0 01-.427.177L13.03 4.03 9.28 7.78a.75.75 0 01-1.06-1.06l3.75-3.75-1.543-1.543A.25.25 0 0110.604 1zM3.75 2A1.75 1.75 0 002 3.75v8.5c0 .966.784 1.75 1.75 1.75h8.5A1.75 1.75 0 0014 12.25v-3.5a.75.75 0 00-1.5 0v3.5a.25.25 0 01-.25.25h-8.5a.25.25 0 01-.25-.25v-8.5a.25.25 0 01.25-.25h3.5a.75.75 0 000-1.5h-3.5z"></path>
            </svg>
          </a>
        </span>
      </span></animated-image></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">人体形状的空间：范围扫描的重建和参数化（2003）</font></font></b> <a href="http://grail.cs.washington.edu/projects/digital-human/pub/allen03space-submit.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/61c4c228dd9ccbfa3aaa9dac01d1e0bc60e5b549caf87ab3fe53f8ffc559827d/68747470733a2f2f6169322d73322d7075626c69632e73332e616d617a6f6e6177732e636f6d2f666967757265732f323031362d31312d30382f343664333962306532316165393536653462636237613738396639326265343830643435656531322f372d46696775726531302d312e706e67"><img width="50%" src="https://camo.githubusercontent.com/61c4c228dd9ccbfa3aaa9dac01d1e0bc60e5b549caf87ab3fe53f8ffc559827d/68747470733a2f2f6169322d73322d7075626c69632e73332e616d617a6f6e6177732e636f6d2f666967757265732f323031362d31312d30382f343664333962306532316165393536653462636237613738396639326265343830643435656531322f372d46696775726531302d312e706e67" data-canonical-src="https://ai2-s2-public.s3.amazonaws.com/figures/2016-11-08/46d39b0e21ae956e4bcb7a789f92be480d45ee12/7-Figure10-1.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SMPL-X：富有表现力的身体捕捉：来自单个图像的 3D 手、脸部和身体 (2019) </font></font></b> <a href="https://ps.is.tuebingen.mpg.de/uploads_file/attachment/attachment/497/SMPL-X.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a><a href="https://youtu.be/XyXIEmapWkw" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[视频] </font></font></a><a href="https://github.com/vchoutas/smplify-x"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"> <a target="_blank" rel="noopener noreferrer" href="https://github.com/vchoutas/smplify-x/blob/master/images/teaser_fig.png"><img width="50%" src="https://github.com/vchoutas/smplify-x/raw/master/images/teaser_fig.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">PIFuHD：用于高分辨率 3D 人体数字化的多级像素对齐隐式函数 (CVPR 2020) </font></font></b> <a href="https://arxiv.org/pdf/2004.00452.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a><a href="https://www.youtube.com/watch?v=uEDqCxvF5yc&amp;feature=youtu.be" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[视频] </font></font></a><a href="https://github.com/facebookresearch/pifuhd"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"> <a target="_blank" rel="noopener noreferrer" href="/timzhang642/3D-Machine-Learning/blob/master"><img width="50%" src="/timzhang642/3D-Machine-Learning/raw/master" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ExPose：通过身体驱动注意力的单目表达身体回归（2020）</font></font></b> <a href="https://ps.is.tuebingen.mpg.de/uploads_file/attachment/attachment/620/0983.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a><a href="https://youtu.be/lNTmHLYTiB8" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[视频] </font></font></a><a href="https://github.com/vchoutas/expose"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"> <a target="_blank" rel="noopener noreferrer" href="https://github.com/vchoutas/expose/blob/master/images/expose.png"><img width="50%" src="https://github.com/vchoutas/expose/raw/master/images/expose.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">从单个图像重建特定类别的对象 (2014) </font></font></b> <a href="https://people.eecs.berkeley.edu/~akar/categoryshapes.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/c4a3660b53f2e32e5041bccba7c6a16571e19be8eb53a0f6b123e7ea1a167edb/687474703a2f2f70656f706c652e656563732e6265726b656c65792e6564752f7e616b61722f63617465676f72795368617065732f696d616765732f7465617365722e706e67"><img width="50%" src="https://camo.githubusercontent.com/c4a3660b53f2e32e5041bccba7c6a16571e19be8eb53a0f6b123e7ea1a167edb/687474703a2f2f70656f706c652e656563732e6265726b656c65792e6564752f7e616b61722f63617465676f72795368617065732f696d616765732f7465617365722e706e67" data-canonical-src="http://people.eecs.berkeley.edu/~akar/categoryShapes/images/teaser.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">🎲 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">DeformNet：用于从单个图像重建 3D 形状的自由形式变形网络（2017）</font></font></b> <a href="http://ai.stanford.edu/~haosu/papers/SI2PC_arxiv_submit.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/2abf8218d24a8057403422bb3984dc26d4261e7dfcd9d07cb05450b5addf9428/68747470733a2f2f636872697363686f792e6769746875622e696f2f696d616765732f7075626c69636174696f6e2f6465666f726d6e65742f6d6f64656c2e706e67"><img width="50%" src="https://camo.githubusercontent.com/2abf8218d24a8057403422bb3984dc26d4261e7dfcd9d07cb05450b5addf9428/68747470733a2f2f636872697363686f792e6769746875622e696f2f696d616765732f7075626c69636174696f6e2f6465666f726d6e65742f6d6f64656c2e706e67" data-canonical-src="https://chrischoy.github.io/images/publication/deformnet/model.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">💎</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">用于局部变形分量分析的基于网格的自动编码器（2017）</font></font></b> <a href="https://arxiv.org/pdf/1709.04304.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/ae46f183ad0bb37def89c6dc6c13e679851a10664b6da96a80b0fdf99526ec1a/687474703a2f2f717974616e2e636f6d2f696d672f706f696e745f636f6e762e6a7067"><img width="50%" src="https://camo.githubusercontent.com/ae46f183ad0bb37def89c6dc6c13e679851a10664b6da96a80b0fdf99526ec1a/687474703a2f2f717974616e2e636f6d2f696d672f706f696e745f636f6e762e6a7067" data-canonical-src="http://qytan.com/img/point_conv.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">💎</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">使用自动编码器网络探索生成 3D 形状 (Autodesk 2017) </font></font></b> <a href="https://www.autodeskresearch.com/publications/exploring_generative_3d_shapes" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/3D-Machine-Learning/blob/master/imgs/Exploring%20Generative%203D%20Shapes%20Using%20Autoencoder%20Networks.jpeg"><img width="50%" src="https://github.com/timzhang642/3D-Machine-Learning/raw/master/imgs/Exploring%20Generative%203D%20Shapes%20Using%20Autoencoder%20Networks.jpeg" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">💎</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">使用本地对应的 CAD 模型从单个图像进行密集 3D 重建 (2017) </font></font></b> <a href="http://ci2cv.net/media/papers/chenkong_cvpr_2017.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/be9882d45624bfebe77529ac4547d183ccc0cb565c5680ecce6e28cf9a632ab4/68747470733a2f2f6368656e687375616e6c696e2e6269746275636b65742e696f2f696d616765732f72702f7230322e706e67"><img width="50%" src="https://camo.githubusercontent.com/be9882d45624bfebe77529ac4547d183ccc0cb565c5680ecce6e28cf9a632ab4/68747470733a2f2f6368656e687375616e6c696e2e6269746275636b65742e696f2f696d616765732f72702f7230322e706e67" data-canonical-src="https://chenhsuanlin.bitbucket.io/images/rp/r02.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">💎 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3D 重建的紧凑模型表示（2017）</font></font></b> <a href="https://jhonykaesemodel.com/publication/3dv2017/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/00e8ee350026bed87fc54cde4ac3bde8ee48cb6b0a1481799d206d478cfa0d11/68747470733a2f2f6a686f6e796b616573656d6f64656c2e636f6d2f696d672f686561646572732f6f766572766965772e706e67"><img width="50%" src="https://camo.githubusercontent.com/00e8ee350026bed87fc54cde4ac3bde8ee48cb6b0a1481799d206d478cfa0d11/68747470733a2f2f6a686f6e796b616573656d6f64656c2e636f6d2f696d672f686561646572732f6f766572766965772e706e67" data-canonical-src="https://jhonykaesemodel.com/img/headers/overview.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">💎 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Image2Mesh：单图像 3D 重建的学习框架（2017）</font></font></b> <a href="https://arxiv.org/pdf/1711.10669.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/b18c1dfcc69842a196dc2a2f1332f32bf744df3b7aa2033124857425f71e8b48/68747470733a2f2f7062732e7477696d672e636f6d2f6d656469612f44573556686a70573441414553484f2e6a7067"><img width="50%" src="https://camo.githubusercontent.com/b18c1dfcc69842a196dc2a2f1332f32bf744df3b7aa2033124857425f71e8b48/68747470733a2f2f7062732e7477696d672e636f6d2f6d656469612f44573556686a70573441414553484f2e6a7067" data-canonical-src="https://pbs.twimg.com/media/DW5VhjpW4AAESHO.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">💎</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">学习 3D 对象重建的自由形式变形（2018）</font></font></b> <a href="https://jhonykaesemodel.com/publication/learning_ffd/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/3df2c549c4c7620176f12a6b2f88c26236d8e9839376eaa452feca6e78b8f040/68747470733a2f2f6a686f6e796b616573656d6f64656c2e636f6d2f6c6561726e696e675f6666645f6f766572766965772e706e67"><img width="50%" src="https://camo.githubusercontent.com/3df2c549c4c7620176f12a6b2f88c26236d8e9839376eaa452feca6e78b8f040/68747470733a2f2f6a686f6e796b616573656d6f64656c2e636f6d2f6c6561726e696e675f6666645f6f766572766965772e706e67" data-canonical-src="https://jhonykaesemodel.com/learning_ffd_overview.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">💎</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">用于变形 3D 网格模型的变分自动编码器（2018 CVPR）</font></font></b> <a href="http://qytan.com/publication/vae/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/e347c5fa527c657006960dc3f28cbb26a88db40bc3b21f0b9de29a5481d533d3/687474703a2f2f68756d616e6d6f74696f6e2e6963742e61632e636e2f7061706572732f3230313850355f566172696174696f6e616c4175746f656e636f646572732f546561736572496d6167652e6a7067"><img width="50%" src="https://camo.githubusercontent.com/e347c5fa527c657006960dc3f28cbb26a88db40bc3b21f0b9de29a5481d533d3/687474703a2f2f68756d616e6d6f74696f6e2e6963742e61632e636e2f7061706572732f3230313850355f566172696174696f6e616c4175746f656e636f646572732f546561736572496d6167652e6a7067" data-canonical-src="http://humanmotion.ict.ac.cn/papers/2018P5_VariationalAutoencoders/TeaserImage.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">💎</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">狮子、老虎和熊：从图像中捕获非刚性、3D、铰接形状（2018 CVPR）</font></font></b> <a href="http://files.is.tue.mpg.de/black/papers/zuffiCVPR2018.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/a35ba07557fe1ac9e0a79e1db19f0cf9ac096620ed6217ac336694bcf484154e/68747470733a2f2f336331373033666538642e736974652e696e7465726e617063646e2e6e65742f6e65776d616e2f6766782f6e6577732f68697265732f323031382f7265616c69737469636176612e6a7067"><img width="50%" src="https://camo.githubusercontent.com/a35ba07557fe1ac9e0a79e1db19f0cf9ac096620ed6217ac336694bcf484154e/68747470733a2f2f336331373033666538642e736974652e696e7465726e617063646e2e6e65742f6e65776d616e2f6766782f6e6577732f68697265732f323031382f7265616c69737469636176612e6a7067" data-canonical-src="https://3c1703fe8d.site.internapcdn.net/newman/gfx/news/hires/2018/realisticava.jpg" style="max-width: 100%;"></a></p>
<a name="user-content-3d_synthesis_template_based">
</a><div class="markdown-heading" dir="auto"><a><h3 tabindex="-1" class="heading-element" dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">基于部分的模板学习方法</font></font></h3></a><a id="user-content-part-based-template-learning-methods" class="anchor" aria-label="永久链接：基于部分的模板学习方法" href="#part-based-template-learning-methods"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">实例建模 (2004) </font></font></b> <a href="http://www.cs.princeton.edu/~funk/sig04a.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/19c510fc19bbce4859d7a3e19dfb649faced5de638788d760d37d9a5e691d6b9/687474703a2f2f6766782e63732e7072696e6365746f6e2e6564752f707562732f46756e6b686f757365725f323030345f4d42452f63686169722e6a7067"><img width="20%" src="https://camo.githubusercontent.com/19c510fc19bbce4859d7a3e19dfb649faced5de638788d760d37d9a5e691d6b9/687474703a2f2f6766782e63732e7072696e6365746f6e2e6564752f707562732f46756e6b686f757365725f323030345f4d42452f63686169722e6a7067" data-canonical-src="http://gfx.cs.princeton.edu/pubs/Funkhouser_2004_MBE/chair.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">可互换组件的模型组合 (2007) </font></font></b> <a href="http://www.cs.princeton.edu/courses/archive/spring11/cos598A/pdfs/Kraevoy07.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/84418a628023d231f9b83f8a6160a08550acd3d755ba8a3722659da7201e83f4/687474703a2f2f7777772e63732e7562632e63612f6c6162732f696d616765722f74722f323030372f566c61645f53687566666c65722f7465617365722e6a7067"><img width="40%" src="https://camo.githubusercontent.com/84418a628023d231f9b83f8a6160a08550acd3d755ba8a3722659da7201e83f4/687474703a2f2f7777772e63732e7562632e63612f6c6162732f696d616765722f74722f323030372f566c61645f53687566666c65722f7465617365722e6a7067" data-canonical-src="http://www.cs.ubc.ca/labs/imager/tr/2007/Vlad_Shuffler/teaser.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3D 建模中创意支持的数据驱动建议 (2010) </font></font></b> <a href="http://vladlen.info/publications/data-driven-suggestions-for-creativity-support-in-3d-modeling/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/d1ee26b4158dd09ef5a009fcb3c100850eae7ad8311096a2928a25eb055dddc6/687474703a2f2f766c61646c656e2e696e666f2f77702d636f6e74656e742f75706c6f6164732f323031312f31322f637265617469766974792e706e67"><img width="50%" src="https://camo.githubusercontent.com/d1ee26b4158dd09ef5a009fcb3c100850eae7ad8311096a2928a25eb055dddc6/687474703a2f2f766c61646c656e2e696e666f2f77702d636f6e74656e742f75706c6f6164732f323031312f31322f637265617469766974792e706e67" data-canonical-src="http://vladlen.info/wp-content/uploads/2011/12/creativity.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">照片启发模型驱动的 3D 对象建模 (2011) </font></font></b> <a href="http://kevinkaixu.net/projects/photo-inspired.html" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/361b254365fe062803e9d4fc15e0ac83dbf2a2c2e573b5ef9fc99068dbf92f43/687474703a2f2f6b6576696e6b616978752e6e65742f70726f6a656374732f70686f746f2d696e7370697265642f6f766572766965772e504e47"><img width="50%" src="https://camo.githubusercontent.com/361b254365fe062803e9d4fc15e0ac83dbf2a2c2e573b5ef9fc99068dbf92f43/687474703a2f2f6b6576696e6b616978752e6e65742f70726f6a656374732f70686f746f2d696e7370697265642f6f766572766965772e504e47" data-canonical-src="http://kevinkaixu.net/projects/photo-inspired/overview.PNG" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">基于装配的 3D 建模的概率推理 (2011) </font></font></b> <a href="https://people.cs.umass.edu/~kalo/papers/assembly/ProbReasoningShapeModeling.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/7a4ebd1d9eadadf1d9019dc4521bc31646f10c194a25ad4f304a59f62634a2c2/687474703a2f2f766c61646c656e2e696e666f2f77702d636f6e74656e742f75706c6f6164732f323031312f31322f686967686c69676874392e706e67"><img width="50%" src="https://camo.githubusercontent.com/7a4ebd1d9eadadf1d9019dc4521bc31646f10c194a25ad4f304a59f62634a2c2/687474703a2f2f766c61646c656e2e696e666f2f77702d636f6e74656e742f75706c6f6164732f323031312f31322f686967686c69676874392e706e67" data-canonical-src="http://vladlen.info/wp-content/uploads/2011/12/highlight9.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">基于组件的形状合成的概率模型 (2012) </font></font></b> <a href="https://people.cs.umass.edu/~kalo/papers/ShapeSynthesis/ShapeSynthesis.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/test1/blob/master/imgs/A%20Probabilistic%20Model%20for%20Component-Based%20Shape%20Synthesis.png"><img width="50%" src="https://github.com/timzhang642/test1/raw/master/imgs/A%20Probabilistic%20Model%20for%20Component-Based%20Shape%20Synthesis.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">通过零件组装恢复结构（2012）</font></font></b> <a href="http://cg.cs.tsinghua.edu.cn/StructureRecovery/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/test1/blob/master/imgs/Structure%20Recovery%20by%20Part%20Assembly.png"><img width="50%" src="https://github.com/timzhang642/test1/raw/master/imgs/Structure%20Recovery%20by%20Part%20Assembly.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">适合且多样化：激发灵感的 3D 形状画廊的设置演变 (2012) </font></font></b> <a href="http://kevinkaixu.net/projects/civil.html" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/2417f0ab7542b7960c3473027d5539ae7d7d9f33428e6280d81a0c1c78ec7a27/687474703a2f2f6b6576696e6b616978752e6e65742f70726f6a656374732f636976696c2f7465617365722e706e67"><img width="50%" src="https://camo.githubusercontent.com/2417f0ab7542b7960c3473027d5539ae7d7d9f33428e6280d81a0c1c78ec7a27/687474703a2f2f6b6576696e6b616978752e6e65742f70726f6a656374732f636976696c2f7465617365722e706e67" data-canonical-src="http://kevinkaixu.net/projects/civil/teaser.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">AttribIt：具有语义属性的内容创建（2013）</font></font></b> <a href="https://people.cs.umass.edu/~kalo/papers/attribit/AttribIt.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/d9e9ae6e20f593e2784ffb0cc7cdb7320c52e1e48aaa03b4b4607aca19d99d46/687474703a2f2f6766782e63732e7072696e6365746f6e2e6564752f6766782f707562732f4368617564687572695f323031335f4143432f7465617365722e6a7067"><img width="30%" src="https://camo.githubusercontent.com/d9e9ae6e20f593e2784ffb0cc7cdb7320c52e1e48aaa03b4b4607aca19d99d46/687474703a2f2f6766782e63732e7072696e6365746f6e2e6564752f6766782f707562732f4368617564687572695f323031335f4143432f7465617365722e6a7067" data-canonical-src="http://gfx.cs.princeton.edu/gfx/pubs/Chaudhuri_2013_ACC/teaser.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">从大量 3D 形状中学习基于零件的模板（2013）</font></font></b> <a href="http://shape.cs.princeton.edu/vkcorrs/papers/13_SIGGRAPH_CorrsTmplt.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/test1/blob/master/imgs/Learning%20Part-based%20Templates%20from%20Large%20Collections%20of%203D%20Shapes.png"><img width="50%" src="https://github.com/timzhang642/test1/raw/master/imgs/Learning%20Part-based%20Templates%20from%20Large%20Collections%20of%203D%20Shapes.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">通过结构混合创建拓扑变化的 3D 形状 (2014) </font></font></b> <a href="http://gruvi.cs.sfu.ca/project/topo/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/772936cbc8826568020d0231822bac635e89a7cb324c2e4e3a081bb3c45e95f2/68747470733a2f2f692e7974696d672e636f6d2f76692f5863347166377636612d772f6d617872657364656661756c742e6a7067"><img width="50%" src="https://camo.githubusercontent.com/772936cbc8826568020d0231822bac635e89a7cb324c2e4e3a081bb3c45e95f2/68747470733a2f2f692e7974696d672e636f6d2f76692f5863347166377636612d772f6d617872657364656661756c742e6a7067" data-canonical-src="https://i.ytimg.com/vi/Xc4qf7v6a-w/maxresdefault.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">使用形状集合估计图像深度 (2014) </font></font></b> <a href="http://vecg.cs.ucl.ac.uk/Projects/SmartGeometry/image_shape_net/imageShapeNet_sigg14.html" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/fe4d69fc367cb85ec1a9640797788ad371c4be745726f0f0827a648119b61062/687474703a2f2f766563672e63732e75636c2e61632e756b2f50726f6a656374732f536d61727447656f6d657472792f696d6167655f73686170655f6e65742f70617065725f646f63732f706970656c696e652e6a7067"><img width="50%" src="https://camo.githubusercontent.com/fe4d69fc367cb85ec1a9640797788ad371c4be745726f0f0827a648119b61062/687474703a2f2f766563672e63732e75636c2e61632e756b2f50726f6a656374732f536d61727447656f6d657472792f696d6167655f73686170655f6e65742f70617065725f646f63732f706970656c696e652e6a7067" data-canonical-src="http://vecg.cs.ucl.ac.uk/Projects/SmartGeometry/image_shape_net/paper_docs/pipeline.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">通过图像和形状集合的联合分析进行单视图重建（2015）</font></font></b> <a href="https://www.cs.utexas.edu/~huangqx/modeling_sig15.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/2ba8030b349f65aaabc4d08e4c228cd290b341f342ece51f0baa0035bf453f56/687474703a2f2f766c61646c656e2e696e666f2f77702d636f6e74656e742f75706c6f6164732f323031352f30352f73696e676c652d766965772e706e67"><img width="50%" src="https://camo.githubusercontent.com/2ba8030b349f65aaabc4d08e4c228cd290b341f342ece51f0baa0035bf453f56/687474703a2f2f766c61646c656e2e696e666f2f77702d636f6e74656e742f75706c6f6164732f323031352f30352f73696e676c652d766965772e706e67" data-canonical-src="http://vladlen.info/wp-content/uploads/2015/05/single-view.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">基于动手装配建模的可互换组件（2016）</font></font></b> <a href="http://www.cs.umb.edu/~craigyu/papers/handson_low_res.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/test1/blob/master/imgs/Interchangeable%20Components%20for%20Hands-On%20Assembly%20Based%20Modeling.png"><img width="30%" src="https://github.com/timzhang642/test1/raw/master/imgs/Interchangeable%20Components%20for%20Hands-On%20Assembly%20Based%20Modeling.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">从单个 RGBD 图像完成形状 (2016) </font></font></b> <a href="http://www.kunzhou.net/2016/shapecompletion-tvcg16.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/2972e905615b78d44d9820347707c743c941a32131b8c2697fc1a86356d61fe5/687474703a2f2f7469616e6a69617368616f2e636f6d2f496d616765732f323031352f636f6d706c6574696f6e2e6a7067"><img width="40%" src="https://camo.githubusercontent.com/2972e905615b78d44d9820347707c743c941a32131b8c2697fc1a86356d61fe5/687474703a2f2f7469616e6a69617368616f2e636f6d2f496d616765732f323031352f636f6d706c6574696f6e2e6a7067" data-canonical-src="http://tianjiashao.com/Images/2015/completion.jpg" style="max-width: 100%;"></a></p>
<a name="user-content-3d_synthesis_dl_based">
</a><div class="markdown-heading" dir="auto"><a><h3 tabindex="-1" class="heading-element" dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">深度学习方法</font></font></h3></a><a id="user-content-deep-learning-methods" class="anchor" aria-label="永久链接：深度学习方法" href="#deep-learning-methods"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">📷</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">学习用卷积网络生成椅子、桌子和汽车（2014）</font></font></b> <a href="https://arxiv.org/pdf/1411.5928.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/f3a1e5bcc0290c29a70112aba46cbabe0c4e2d11b0ac9203b5515e418c02f5d9/68747470733a2f2f7a6f372e6769746875622e696f2f696d672f323031362d30392d32352d67656e65726174696e672d66616365732f6368616972732d6d6f64656c2e706e67"><img width="50%" src="https://camo.githubusercontent.com/f3a1e5bcc0290c29a70112aba46cbabe0c4e2d11b0ac9203b5515e418c02f5d9/68747470733a2f2f7a6f372e6769746875622e696f2f696d672f323031362d30392d32352d67656e65726174696e672d66616365732f6368616972732d6d6f64656c2e706e67" data-canonical-src="https://zo7.github.io/img/2016-09-25-generating-faces/chairs-model.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">📷</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">用于 3D 视图合成的弱监督解缠与循环变换（2015，NIPS）</font></font></b> <a href="https://papers.nips.cc/paper/5639-weakly-supervised-disentangling-with-recurrent-transformations-for-3d-view-synthesis.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/jimeiyang/deepRotator/blob/master/demo_img.png"><img width="50%" src="https://github.com/jimeiyang/deepRotator/raw/master/demo_img.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">🎲</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">通过深度学习的表面生成模型分析和合成 3D 形状族（2015）</font></font></b> <a href="https://people.cs.umass.edu/~hbhuang/publications/bsm/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/fd8cc5bc30bce8ea49c81256246793759a6a549fe35afc763655c6da16e342d2/68747470733a2f2f70656f706c652e63732e756d6173732e6564752f7e68626875616e672f7075626c69636174696f6e732f62736d2f62736d5f7465617365722e6a7067"><img width="50%" src="https://camo.githubusercontent.com/fd8cc5bc30bce8ea49c81256246793759a6a549fe35afc763655c6da16e342d2/68747470733a2f2f70656f706c652e63732e756d6173732e6564752f7e68626875616e672f7075626c69636174696f6e732f62736d2f62736d5f7465617365722e6a7067" data-canonical-src="https://people.cs.umass.edu/~hbhuang/publications/bsm/bsm_teaser.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">📷</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">用于 3D 视图合成的弱监督解缠与循环变换 (2015) </font></font></b> <a href="https://papers.nips.cc/paper/5639-weakly-supervised-disentangling-with-recurrent-transformations-for-3d-view-synthesis.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/jimeiyang/deepRotator"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/dd552d13ae91f9d8c8ad819d69ad5fbe06b829724806a88f4c9bfa9155302248/68747470733a2f2f6169322d73322d7075626c69632e73332e616d617a6f6e6177732e636f6d2f666967757265732f323031362d31312d30382f303432393933633436323934613534323934366339633137303662376232326465623164376334332f322d466967757265312d312e706e67"><img width="50%" src="https://camo.githubusercontent.com/dd552d13ae91f9d8c8ad819d69ad5fbe06b829724806a88f4c9bfa9155302248/68747470733a2f2f6169322d73322d7075626c69632e73332e616d617a6f6e6177732e636f6d2f666967757265732f323031362d31312d30382f303432393933633436323934613534323934366339633137303662376232326465623164376334332f322d466967757265312d312e706e67" data-canonical-src="https://ai2-s2-public.s3.amazonaws.com/figures/2016-11-08/042993c46294a542946c9c1706b7b22deb1d7c43/2-Figure1-1.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">📷</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">使用卷积网络从单图像建立多视图 3D 模型 (2016) </font></font></b> <a href="https://arxiv.org/pdf/1511.06702.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/lmb-freiburg/mv3d"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/4bd532be171e2effa64635ea0561c87228140aeb1105464f5d9b4210fd3f8e9c/68747470733a2f2f6169322d73322d7075626c69632e73332e616d617a6f6e6177732e636f6d2f666967757265732f323031362d31312d30382f336437636135616433346632336135666162313665373365323837643161303539646337656639612f342d466967757265322d312e706e67"><img width="50%" src="https://camo.githubusercontent.com/4bd532be171e2effa64635ea0561c87228140aeb1105464f5d9b4210fd3f8e9c/68747470733a2f2f6169322d73322d7075626c69632e73332e616d617a6f6e6177732e636f6d2f666967757265732f323031362d31312d30382f336437636135616433346632336135666162313665373365323837643161303539646337656639612f342d466967757265322d312e706e67" data-canonical-src="https://ai2-s2-public.s3.amazonaws.com/figures/2016-11-08/3d7ca5ad34f23a5fab16e73e287d1a059dc7ef9a/4-Figure2-1.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">📷</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">按外观流程查看合成 (2016) </font></font></b> <a href="https://people.eecs.berkeley.edu/~tinghuiz/papers/eccv16_appflow.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/tinghuiz/appearance-flow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/b12a92e5b967abb3401431708ece79ae300abbbb02c24111f28f092cac619893/68747470733a2f2f6169322d73322d7075626c69632e73332e616d617a6f6e6177732e636f6d2f666967757265732f323031362d31312d30382f313232383035303664633862356333636132646232396663336265363934643961386265663438632f362d466967757265322d312e706e67"><img width="50%" src="https://camo.githubusercontent.com/b12a92e5b967abb3401431708ece79ae300abbbb02c24111f28f092cac619893/68747470733a2f2f6169322d73322d7075626c69632e73332e616d617a6f6e6177732e636f6d2f666967757265732f323031362d31312d30382f313232383035303664633862356333636132646232396663336265363934643961386265663438632f362d466967757265322d312e706e67" data-canonical-src="https://ai2-s2-public.s3.amazonaws.com/figures/2016-11-08/12280506dc8b5c3ca2db29fc3be694d9a8bef48c/6-Figure2-1.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">👾 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Voxlets：从单个深度图像中对未观察到的体素进行结构化预测（2016）</font></font></b> <a href="http://visual.cs.ucl.ac.uk/pubs/depthPrediction/http://visual.cs.ucl.ac.uk/pubs/depthPrediction/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/mdfirman/voxlets"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/6c8166313e11911c42853e960c128910c832163c1f329b0e2cd1ebf747a2842a/68747470733a2f2f692e7974696d672e636f6d2f76692f317779347932475744356f2f6d617872657364656661756c742e6a7067"><img width="30%" src="https://camo.githubusercontent.com/6c8166313e11911c42853e960c128910c832163c1f329b0e2cd1ebf747a2842a/68747470733a2f2f692e7974696d672e636f6d2f76692f317779347932475744356f2f6d617872657364656661756c742e6a7067" data-canonical-src="https://i.ytimg.com/vi/1wy4y2GWD5o/maxresdefault.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">👾 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3D-R2N2：3D 循环重建神经网络 (2016) </font></font></b> <a href="http://cvgl.stanford.edu/3d-r2n2/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/chrischoy/3D-R2N2"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/415e984e65cd227330668ee7ac53324a875ee4675f3c44f883db56f0b1a75c64/687474703a2f2f33642d72326e322e7374616e666f72642e6564752f696d67732f6f766572766965772e706e67"><img width="50%" src="https://camo.githubusercontent.com/415e984e65cd227330668ee7ac53324a875ee4675f3c44f883db56f0b1a75c64/687474703a2f2f33642d72326e322e7374616e666f72642e6564752f696d67732f6f766572766965772e706e67" data-canonical-src="http://3d-r2n2.stanford.edu/imgs/overview.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">👾 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Perspective Transformer Nets：在没有 3D 监督的情况下学习单视图 3D 对象重建（2016）</font></font></b> <a href="https://eng.ucmerced.edu/people/jyang44/papers/nips16_ptn.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/8064c981a9404cd1603c6a870e7668636fae9bee433066c6785068090d6a56db/68747470733a2f2f73697465732e676f6f676c652e636f6d2f736974652f736b7977616c6b65727978632f5f2f727372632f313438313130343539363233382f70657273706563746976655f7472616e73666f726d65725f6e6574732f6e6574776f726b5f617263682e706e67"><img width="70%" src="https://camo.githubusercontent.com/8064c981a9404cd1603c6a870e7668636fae9bee433066c6785068090d6a56db/68747470733a2f2f73697465732e676f6f676c652e636f6d2f736974652f736b7977616c6b65727978632f5f2f727372632f313438313130343539363233382f70657273706563746976655f7472616e73666f726d65725f6e6574732f6e6574776f726b5f617263682e706e67" data-canonical-src="https://sites.google.com/site/skywalkeryxc/_/rsrc/1481104596238/perspective_transformer_nets/network_arch.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">👾 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">TL-Embedding Network：学习对象的可预测和生成向量表示（2016）</font></font></b> <a href="https://arxiv.org/pdf/1603.08637.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/1740a2aeba2503b32ea0bc8438deeb253fefeacd8ac9937aec1d3c7f7102a932/68747470733a2f2f726f686974676972646861722e6769746875622e696f2f47656e657261746976655072656469637461626c65566f78656c732f6173736574732f7765627465617365722e6a7067"><img width="50%" src="https://camo.githubusercontent.com/1740a2aeba2503b32ea0bc8438deeb253fefeacd8ac9937aec1d3c7f7102a932/68747470733a2f2f726f686974676972646861722e6769746875622e696f2f47656e657261746976655072656469637461626c65566f78656c732f6173736574732f7765627465617365722e6a7067" data-canonical-src="https://rohitgirdhar.github.io/GenerativePredictableVoxels/assets/webteaser.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">👾 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3D GAN：通过 3D 生成对抗建模学习对象形状的概率潜在空间（2016）</font></font></b> <a href="https://arxiv.org/pdf/1610.07584.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/3d4a942fc3d9c13e017bc208ce048de928317c972fa7c0d6c15c85dffbde739b/687474703a2f2f336467616e2e637361696c2e6d69742e6564752f696d616765732f6d6f64656c2e6a7067"><img width="50%" src="https://camo.githubusercontent.com/3d4a942fc3d9c13e017bc208ce048de928317c972fa7c0d6c15c85dffbde739b/687474703a2f2f336467616e2e637361696c2e6d69742e6564752f696d616765732f6d6f64656c2e6a7067" data-canonical-src="http://3dgan.csail.mit.edu/images/model.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">👾</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">从多个对象的 2D 视图进行 3D 形状归纳 (2016) </font></font></b> <a href="https://arxiv.org/pdf/1612.05872.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/3311354c34a820298f81b1206299553a4c0a6ff78543cfd2f6ffb848bdb33844/68747470733a2f2f6169322d73322d7075626c69632e73332e616d617a6f6e6177732e636f6d2f666967757265732f323031362d31312d30382f653738353732656565663862393637646563343230303133633635613636383434383763313362322f322d466967757265322d312e706e67"><img width="50%" src="https://camo.githubusercontent.com/3311354c34a820298f81b1206299553a4c0a6ff78543cfd2f6ffb848bdb33844/68747470733a2f2f6169322d73322d7075626c69632e73332e616d617a6f6e6177732e636f6d2f666967757265732f323031362d31312d30382f653738353732656565663862393637646563343230303133633635613636383434383763313362322f322d466967757265322d312e706e67" data-canonical-src="https://ai2-s2-public.s3.amazonaws.com/figures/2016-11-08/e78572eeef8b967dec420013c65a6684487c13b2/2-Figure2-1.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">📷</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">图像 3D 结构的无监督学习 (2016) </font></font></b> <a href="https://arxiv.org/pdf/1607.00662.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/8bd2167be801183afc3366bb92d3ac58af486b33bf00fa544761d3d343884793/68747470733a2f2f61647269616e636f6c7965722e66696c65732e776f726470726573732e636f6d2f323031362f31322f756e737570657276697365642d33642d6669672d31302e6a7065673f773d363030"><img width="50%" src="https://camo.githubusercontent.com/8bd2167be801183afc3366bb92d3ac58af486b33bf00fa544761d3d343884793/68747470733a2f2f61647269616e636f6c7965722e66696c65732e776f726470726573732e636f6d2f323031362f31322f756e737570657276697365642d33642d6669672d31302e6a7065673f773d363030" data-canonical-src="https://adriancolyer.files.wordpress.com/2016/12/unsupervised-3d-fig-10.jpeg?w=600" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">👾</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">使用卷积神经网络进行生成和判别体素建模 (2016) </font></font></b> <a href="https://arxiv.org/pdf/1608.04236.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/ajbrock/Generative-and-Discriminative-Voxel-Modeling"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/b622fc556df61f67e6847567a6162dc824490af25261b0ef1fb20bfd797427c8/687474703a2f2f6461766964737475747a2e64652f776f726470726573732f77702d636f6e74656e742f75706c6f6164732f323031372f30322f62726f636b5f7661652e706e67"><img width="50%" src="https://camo.githubusercontent.com/b622fc556df61f67e6847567a6162dc824490af25261b0ef1fb20bfd797427c8/687474703a2f2f6461766964737475747a2e64652f776f726470726573732f77702d636f6e74656e742f75706c6f6164732f323031372f30322f62726f636b5f7661652e706e67" data-canonical-src="http://davidstutz.de/wordpress/wp-content/uploads/2017/02/brock_vae.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">📷</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">通过可微分射线一致性进行单视图重建的多视图监督（2017）</font></font></b> <a href="https://shubhtuls.github.io/drc/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/a5247ed5d647f628e8efd656b118526de093cb2bf6485400c186e431db5a8292/68747470733a2f2f736875626874756c732e6769746875622e696f2f6472632f7265736f75726365732f696d616765732f74656173657243686169722e706e67"><img width="50%" src="https://camo.githubusercontent.com/a5247ed5d647f628e8efd656b118526de093cb2bf6485400c186e431db5a8292/68747470733a2f2f736875626874756c732e6769746875622e696f2f6472632f7265736f75726365732f696d616765732f74656173657243686169722e706e67" data-canonical-src="https://shubhtuls.github.io/drc/resources/images/teaserChair.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">📷</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">通过使用深度生成网络建模多视图深度图和轮廓来合成 3D 形状（2017）</font></font></b> <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Soltani_Synthesizing_3D_Shapes_CVPR_2017_paper.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a>  <a href="https://github.com/Amir-Arsalan/Synthesize3DviaDepthOrSil"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/3a85fd1cf4c495895548acb393c675a0e380c0edc276623ee056c54eae22f7c5/68747470733a2f2f6a69616a756e77752e636f6d2f696d616765732f73706f746c696768745f33647661652e6a7067"><img width="50%" src="https://camo.githubusercontent.com/3a85fd1cf4c495895548acb393c675a0e380c0edc276623ee056c54eae22f7c5/68747470733a2f2f6a69616a756e77752e636f6d2f696d616765732f73706f746c696768745f33647661652e6a7067" data-canonical-src="https://jiajunwu.com/images/spotlight_3dvae.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">👾</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">使用 3D 编码器预测器 CNN 和形状合成进行形状补全 (2017) </font></font></b> <a href="https://arxiv.org/pdf/1612.00101.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/angeladai/cnncomplete"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/9e9efb6892529acdfba777d23d428663f8879b712f5df64c32b4936b0378d70c/687474703a2f2f67726170686963732e7374616e666f72642e6564752f70726f6a656374732f636e6e636f6d706c6574652f7465617365722e6a7067"><img width="50%" src="https://camo.githubusercontent.com/9e9efb6892529acdfba777d23d428663f8879b712f5df64c32b4936b0378d70c/687474703a2f2f67726170686963732e7374616e666f72642e6564752f70726f6a656374732f636e6e636f6d706c6574652f7465617365722e6a7067" data-canonical-src="http://graphics.stanford.edu/projects/cnncomplete/teaser.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">👾</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">八叉树生成网络：用于高分辨率 3D 输出的高效卷积架构 (2017) </font></font></b> <a href="https://arxiv.org/pdf/1703.09438.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/lmb-freiburg/ogn"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/6c9dfac788d2cc0abe65bfec1f9bad42e9422204dcb8a3358dd65844f7088d0b/68747470733a2f2f6169322d73322d7075626c69632e73332e616d617a6f6e6177732e636f6d2f666967757265732f323031362d31312d30382f366332613239326262303138613837343263626230626263356532336464306134353466666533612f322d466967757265322d312e706e67"><img width="50%" src="https://camo.githubusercontent.com/6c9dfac788d2cc0abe65bfec1f9bad42e9422204dcb8a3358dd65844f7088d0b/68747470733a2f2f6169322d73322d7075626c69632e73332e616d617a6f6e6177732e636f6d2f666967757265732f323031362d31312d30382f366332613239326262303138613837343263626230626263356532336464306134353466666533612f322d466967757265322d312e706e67" data-canonical-src="https://ai2-s2-public.s3.amazonaws.com/figures/2016-11-08/6c2a292bb018a8742cbb0bbc5e23dd0a454ffe3a/2-Figure2-1.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">👾 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3D 对象重建的分层表面预测（2017）</font></font></b> <a href="https://arxiv.org/pdf/1704.00710.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/aaebcb82bc3adb8febb489b75f6183f7510fe2648d55a23b1e1c3a13de82c817/687474703a2f2f626169722e6265726b656c65792e6564752f626c6f672f6173736574732f6873702f696d6167655f322e706e67"><img width="50%" src="https://camo.githubusercontent.com/aaebcb82bc3adb8febb489b75f6183f7510fe2648d55a23b1e1c3a13de82c817/687474703a2f2f626169722e6265726b656c65792e6564752f626c6f672f6173736574732f6873702f696d6167655f322e706e67" data-canonical-src="http://bair.berkeley.edu/blog/assets/hsp/image_2.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">👾 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">OctNetFusion：从数据中学习深度融合（2017）</font></font></b> <a href="https://arxiv.org/pdf/1704.01047.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/griegler/octnetfusion"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/3D-Machine-Learning/blob/master/imgs/OctNetFusion-%20Learning%20Depth%20Fusion%20from%20Data.jpeg"><img width="50%" src="https://github.com/timzhang642/3D-Machine-Learning/raw/master/imgs/OctNetFusion-%20Learning%20Depth%20Fusion%20from%20Data.jpeg" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">🎲</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">用&ZeroWidthSpace;&ZeroWidthSpace;于从单个图像重建 3D 对象的点集生成网络 (2017) </font></font></b> <a href="http://ai.stanford.edu/~haosu/papers/SI2PC_arxiv_submit.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/fanhqme/PointSetGeneration"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/3D-Machine-Learning/blob/master/imgs/A%20Point%20Set%20Generation%20Network%20for%203D%20Object%20Reconstruction%20from%20a%20Single%20Image%20(2017).jpeg"><img width="50%" src="https://github.com/timzhang642/3D-Machine-Learning/raw/master/imgs/A%20Point%20Set%20Generation%20Network%20for%203D%20Object%20Reconstruction%20from%20a%20Single%20Image%20(2017).jpeg" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">🎲</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">学习 3D 点云的表示和生成模型 (2017) </font></font></b> <a href="https://arxiv.org/pdf/1707.02392.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/optas/latent_3d_points"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/optas/latent_3d_points/blob/master/doc/images/teaser.jpg"><img width="50%" src="https://github.com/optas/latent_3d_points/raw/master/doc/images/teaser.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">🎲</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">使用空间分区点云生成形状（2017）</font></font></b> <a href="https://arxiv.org/pdf/1707.06267.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/ad126c0bbdb30d72fd53b9cf5f1895fbc95a0693117353b3a8e0040fba02807c/687474703a2f2f6d676164656c68612e6d652f737070632f6669672f61627374726163742e706e67"><img width="50%" src="https://camo.githubusercontent.com/ad126c0bbdb30d72fd53b9cf5f1895fbc95a0693117353b3a8e0040fba02807c/687474703a2f2f6d676164656c68612e6d652f737070632f6669672f61627374726163742e706e67" data-canonical-src="http://mgadelha.me/sppc/fig/abstract.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">🎲 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">PCPNET 从原始点云学习局部形状属性（2017）</font></font></b> <a href="https://arxiv.org/pdf/1710.04954.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/3D-Machine-Learning/blob/master/imgs/PCPNET%20Learning%20Local%20Shape%20Properties%20from%20Raw%20Point%20Clouds%20(2017).jpeg"><img width="50%" src="https://github.com/timzhang642/3D-Machine-Learning/raw/master/imgs/PCPNET%20Learning%20Local%20Shape%20Properties%20from%20Raw%20Point%20Clouds%20(2017).jpeg" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">📷</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">用于新颖 3D 视图合成的基于变换的图像生成网络 (2017) </font></font></b> <a href="http://www.cs.unc.edu/~eunbyung/tvsn/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/silverbottlep/tvsn"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><animated-image data-catalyst="" style="width: 50%;"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/4ee2ca23ad7c68caaf227ff64e50d6f0a653cf96acaefd6bae92c2a036ee0893/68747470733a2f2f656e672e75636d65726365642e6564752f70656f706c652f6a79616e6734342f706963732f766965775f73796e7468657369732e676966" data-target="animated-image.originalLink" hidden=""><img src="https://camo.githubusercontent.com/4ee2ca23ad7c68caaf227ff64e50d6f0a653cf96acaefd6bae92c2a036ee0893/68747470733a2f2f656e672e75636d65726365642e6564752f70656f706c652f6a79616e6734342f706963732f766965775f73796e7468657369732e676966" data-canonical-src="https://eng.ucmerced.edu/people/jyang44/pics/view_synthesis.gif" style="max-width: 100%;" data-target="animated-image.originalImage" hidden=""></a>
      <span class="AnimatedImagePlayer" data-target="animated-image.player">
        <a data-target="animated-image.replacedLink" class="AnimatedImagePlayer-images" href="https://camo.githubusercontent.com/4ee2ca23ad7c68caaf227ff64e50d6f0a653cf96acaefd6bae92c2a036ee0893/68747470733a2f2f656e672e75636d65726365642e6564752f70656f706c652f6a79616e6734342f706963732f766965775f73796e7468657369732e676966" target="_blank">
          <span data-target="animated-image.imageContainer">
            <img data-target="animated-image.replacedImage" alt="" class="AnimatedImagePlayer-animatedImage" src="https://camo.githubusercontent.com/4ee2ca23ad7c68caaf227ff64e50d6f0a653cf96acaefd6bae92c2a036ee0893/68747470733a2f2f656e672e75636d65726365642e6564752f70656f706c652f6a79616e6734342f706963732f766965775f73796e7468657369732e676966">
          </span>
        </a>
        <button data-target="animated-image.imageButton" class="AnimatedImagePlayer-images" tabindex="-1"></button>
        <span class="AnimatedImagePlayer-controls" data-target="animated-image.controls">
          <button data-target="animated-image.playButton" class="AnimatedImagePlayer-button">
            <svg aria-hidden="true" focusable="false" class="octicon icon-play" width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg">
              <path d="M4 13.5427V2.45734C4 1.82607 4.69692 1.4435 5.2295 1.78241L13.9394 7.32507C14.4334 7.63943 14.4334 8.36057 13.9394 8.67493L5.2295 14.2176C4.69692 14.5565 4 14.1739 4 13.5427Z">
            </path></svg>
            <svg aria-hidden="true" focusable="false" class="octicon icon-pause" width="16" height="16" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg">
              <rect x="4" y="2" width="3" height="12" rx="1"></rect>
              <rect x="9" y="2" width="3" height="12" rx="1"></rect>
            </svg>
          </button>
          <a data-target="animated-image.openButton" aria-label="在新窗口中打开" class="AnimatedImagePlayer-button" href="https://camo.githubusercontent.com/4ee2ca23ad7c68caaf227ff64e50d6f0a653cf96acaefd6bae92c2a036ee0893/68747470733a2f2f656e672e75636d65726365642e6564752f70656f706c652f6a79616e6734342f706963732f766965775f73796e7468657369732e676966" target="_blank">
            <svg aria-hidden="true" class="octicon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" width="16" height="16">
              <path fill-rule="evenodd" d="M10.604 1h4.146a.25.25 0 01.25.25v4.146a.25.25 0 01-.427.177L13.03 4.03 9.28 7.78a.75.75 0 01-1.06-1.06l3.75-3.75-1.543-1.543A.25.25 0 0110.604 1zM3.75 2A1.75 1.75 0 002 3.75v8.5c0 .966.784 1.75 1.75 1.75h8.5A1.75 1.75 0 0014 12.25v-3.5a.75.75 0 00-1.5 0v3.5a.25.25 0 01-.25.25h-8.5a.25.25 0 01-.25-.25v-8.5a.25.25 0 01.25-.25h3.5a.75.75 0 000-1.5h-3.5z"></path>
            </svg>
          </a>
        </span>
      </span></animated-image></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">📷</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">用于对象图像重新渲染的标签解缠结生成对抗网络（2017）</font></font></b> <a href="http://static.ijcai.org/proceedings-2017/0404.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/3D-Machine-Learning/blob/master/imgs/Tag%20Disentangled%20Generative%20Adversarial%20Networks%20for%20Object%20Image%20Re-rendering.jpeg"><img width="50%" src="https://github.com/timzhang642/3D-Machine-Learning/raw/master/imgs/Tag%20Disentangled%20Generative%20Adversarial%20Networks%20for%20Object%20Image%20Re-rendering.jpeg" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">📷</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">通过多视图卷积网络从草图重建 3D 形状 (2017) </font></font></b> <a href="http://people.cs.umass.edu/~zlun/papers/SketchModeling/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/happylun/SketchModeling"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/e222b72415ce9112f5ca3f86fc66b36ecbee9341405aa874f207f6e6a6becf70/68747470733a2f2f70656f706c652e63732e756d6173732e6564752f7e7a6c756e2f7061706572732f536b657463684d6f64656c696e672f536b657463684d6f64656c696e675f7465617365722e706e67"><img width="50%" src="https://camo.githubusercontent.com/e222b72415ce9112f5ca3f86fc66b36ecbee9341405aa874f207f6e6a6becf70/68747470733a2f2f70656f706c652e63732e756d6173732e6564752f7e7a6c756e2f7061706572732f536b657463684d6f64656c696e672f536b657463684d6f64656c696e675f7465617365722e706e67" data-canonical-src="https://people.cs.umass.edu/~zlun/papers/SketchModeling/SketchModeling_teaser.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">👾</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">使用生成对抗网络进行交互式 3D 建模（2017）</font></font></b> <a href="https://arxiv.org/pdf/1706.05170.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/59665d2fc54e8be647bb2e339d97d698f8771bebac2fcbf43b7aafecd76359cd/68747470733a2f2f7062732e7477696d672e636f6d2f6d656469612f444373504b4c71586f414542642d562e6a7067"><img width="50%" src="https://camo.githubusercontent.com/59665d2fc54e8be647bb2e339d97d698f8771bebac2fcbf43b7aafecd76359cd/68747470733a2f2f7062732e7477696d672e636f6d2f6d656469612f444373504b4c71586f414542642d562e6a7067" data-canonical-src="https://pbs.twimg.com/media/DCsPKLqXoAEBd-V.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">📷👾</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">具有对抗性约束的弱监督 3D 重建 (2017) </font></font></b> <a href="https://arxiv.org/pdf/1705.10904.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/jgwak/McRecon"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/3D-Machine-Learning/blob/master/imgs/Weakly%20supervised%203D%20Reconstruction%20with%20Adversarial%20Constraint%20(2017).jpeg"><img width="50%" src="https://github.com/timzhang642/3D-Machine-Learning/raw/master/imgs/Weakly%20supervised%203D%20Reconstruction%20with%20Adversarial%20Constraint%20(2017).jpeg" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">📷 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SurfNet：使用深度残差网络生成 3D 形状表面（2017）</font></font></b> <a href="https://arxiv.org/pdf/1703.04079.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/9cda53cea80c565a0ffb0ce18f55d8fab3033c0a638f420d9f0da1a3ef447171/68747470733a2f2f336461646570742e636f6d2f77702d636f6e74656e742f75706c6f6164732f323031372f30372f53637265656e73686f742d66726f6d2d323031372d30372d32362d3134353532312d65313530313037373533393732332e706e67"><img width="50%" src="https://camo.githubusercontent.com/9cda53cea80c565a0ffb0ce18f55d8fab3033c0a638f420d9f0da1a3ef447171/68747470733a2f2f336461646570742e636f6d2f77702d636f6e74656e742f75706c6f6164732f323031372f30372f53637265656e73686f742d66726f6d2d323031372d30372d32362d3134353532312d65313530313037373533393732332e706e67" data-canonical-src="https://3dadept.com/wp-content/uploads/2017/07/Screenshot-from-2017-07-26-145521-e1501077539723.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">📷</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">学习使用 3D 表面的平面参数化重建对称形状（2019）</font></font></b> <a href="https://openaccess.thecvf.com/content_ICCVW_2019/papers/GMDL/Jain_Learning_to_Reconstruct_Symmetric_Shapes_using_Planar_Parameterization_of_3D_ICCVW_2019_paper.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/hrdkjain/LearningSymmetricShapes"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/hrdkjain/LearningSymmetricShapes/blob/master/Images/teaser.png"><img width="50%" src="https://github.com/hrdkjain/LearningSymmetricShapes/raw/master/Images/teaser.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">💊 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">GRASS：形状结构的生成递归自动编码器 (SIGGRAPH 2017) </font></font></b> <a href="http://kevinkaixu.net/projects/grass.html" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/junli-lj/grass"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码] </font></font></a> <a href="https://github.com/kevin-kaixu/grass_pytorch"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/7987f92ae81eaaa4afe7c74d1ff6c9a8a8befd2cc88d516c5f064a9da1cf9506/687474703a2f2f6b6576696e6b616978752e6e65742f70726f6a656374732f67726173732f7465617365722e6a7067"><img width="50%" src="https://camo.githubusercontent.com/7987f92ae81eaaa4afe7c74d1ff6c9a8a8befd2cc88d516c5f064a9da1cf9506/687474703a2f2f6b6576696e6b616978752e6e65742f70726f6a656374732f67726173732f7465617365722e6a7067" data-canonical-src="http://kevinkaixu.net/projects/grass/teaser.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">💊 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3D-PRNN：使用循环神经网络生成形状基元 (2017) </font></font></b> <a href="https://arxiv.org/pdf/1708.01648.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a><a href="https://github.com/zouchuhang/3D-PRNN"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/zouchuhang/3D-PRNN/blob/master/figs/teasor.jpg"><img width="50%" src="https://github.com/zouchuhang/3D-PRNN/raw/master/figs/teasor.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">💎</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">神经 3D 网格渲染器 (2017) </font></font></b> <a href="http://hiroharu-kato.com/projects_en/neural_renderer.html" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/hiroharu-kato/neural_renderer.git"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/bbdebc8e16e4bc216858b2858815b7303aa593f0def9083151f016b158cd08f9/68747470733a2f2f7062732e7477696d672e636f6d2f6d656469612f4450536d2d3448576b414170455a642e6a7067"><img width="50%" src="https://camo.githubusercontent.com/bbdebc8e16e4bc216858b2858815b7303aa593f0def9083151f016b158cd08f9/68747470733a2f2f7062732e7477696d672e636f6d2f6d656469612f4450536d2d3448576b414170455a642e6a7067" data-canonical-src="https://pbs.twimg.com/media/DPSm-4HWkAApEZd.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">🎲👾 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ShapeNet Core55 的大规模 3D 形状重建和分割（2017）</font></font></b> <a href="https://arxiv.org/pdf/1710.06104.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/3D-Machine-Learning/blob/master/imgs/Core55.png"><img width="40%" src="https://github.com/timzhang642/3D-Machine-Learning/raw/master/imgs/Core55.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">👾 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pix2vox：使用堆叠生成对抗网络进行基于草图的 3D 探索（2017）</font></font></b> <a href="https://github.com/maxorange/pix2vox"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><animated-image data-catalyst="" style="width: 50%;"><a target="_blank" rel="noopener noreferrer" href="https://github.com/maxorange/pix2vox/blob/master/img/sample.gif" data-target="animated-image.originalLink"><img src="https://github.com/maxorange/pix2vox/raw/master/img/sample.gif" style="max-width: 100%; display: inline-block;" data-target="animated-image.originalImage"></a>
      <span class="AnimatedImagePlayer" data-target="animated-image.player" hidden="">
        <a data-target="animated-image.replacedLink" class="AnimatedImagePlayer-images" href="https://github.com/maxorange/pix2vox/blob/master/img/sample.gif" target="_blank">
          
        <span data-target="animated-image.imageContainer">
            <img data-target="animated-image.replacedImage" alt="sample.gif" class="AnimatedImagePlayer-animatedImage" src="https://github.com/maxorange/pix2vox/raw/master/img/sample.gif" style="display: block; opacity: 1;">
          <canvas class="AnimatedImagePlayer-stillImage" aria-hidden="true" width="399" height="328"></canvas></span></a>
        <button data-target="animated-image.imageButton" class="AnimatedImagePlayer-images" tabindex="-1" aria-label="Play sample.gif" hidden=""></button>
        <span class="AnimatedImagePlayer-controls" data-target="animated-image.controls" hidden="">
          <button data-target="animated-image.playButton" class="AnimatedImagePlayer-button" aria-label="Play sample.gif">
            <svg aria-hidden="true" focusable="false" class="octicon icon-play" width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg">
              <path d="M4 13.5427V2.45734C4 1.82607 4.69692 1.4435 5.2295 1.78241L13.9394 7.32507C14.4334 7.63943 14.4334 8.36057 13.9394 8.67493L5.2295 14.2176C4.69692 14.5565 4 14.1739 4 13.5427Z">
            </path></svg>
            <svg aria-hidden="true" focusable="false" class="octicon icon-pause" width="16" height="16" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg">
              <rect x="4" y="2" width="3" height="12" rx="1"></rect>
              <rect x="9" y="2" width="3" height="12" rx="1"></rect>
            </svg>
          </button>
          <a data-target="animated-image.openButton" aria-label="在新窗口中打开" class="AnimatedImagePlayer-button" href="https://github.com/maxorange/pix2vox/blob/master/img/sample.gif" target="_blank">
            <svg aria-hidden="true" class="octicon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" width="16" height="16">
              <path fill-rule="evenodd" d="M10.604 1h4.146a.25.25 0 01.25.25v4.146a.25.25 0 01-.427.177L13.03 4.03 9.28 7.78a.75.75 0 01-1.06-1.06l3.75-3.75-1.543-1.543A.25.25 0 0110.604 1zM3.75 2A1.75 1.75 0 002 3.75v8.5c0 .966.784 1.75 1.75 1.75h8.5A1.75 1.75 0 0014 12.25v-3.5a.75.75 0 00-1.5 0v3.5a.25.25 0 01-.25.25h-8.5a.25.25 0 01-.25-.25v-8.5a.25.25 0 01.25-.25h3.5a.75.75 0 000-1.5h-3.5z"></path>
            </svg>
          </a>
        </span>
      </span></animated-image></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">📷👾</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">所画即所得：使用多视图深度体积预测进行 3D 草图绘制（2017）</font></font></b> <a href="https://arxiv.org/pdf/1707.08390.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/761bb3132ba3fa7ffff1135a9e4bda8ba4cce5226e2ed3d0ceb329b36367f877/68747470733a2f2f61727869762d73616e6974792d73616e6974792d70726f64756374696f6e2e73332e616d617a6f6e6177732e636f6d2f72656e6465722d6f75747075742f33313633312f78312e706e67"><img width="50%" src="https://camo.githubusercontent.com/761bb3132ba3fa7ffff1135a9e4bda8ba4cce5226e2ed3d0ceb329b36367f877/68747470733a2f2f61727869762d73616e6974792d73616e6974792d70726f64756374696f6e2e73332e616d617a6f6e6177732e636f6d2f72656e6465722d6f75747075742f33313633312f78312e706e67" data-canonical-src="https://arxiv-sanity-sanity-production.s3.amazonaws.com/render-output/31631/x1.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">📷👾 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">MarrNet：通过 2.5D 草图进行 3D 形状重建（2017）</font></font></b> <a href="http://marrnet.csail.mit.edu/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/056648111d57935bc551879943c486d07a0c81cc2b404cbe858c6f1d000da038/687474703a2f2f6d6172726e65742e637361696c2e6d69742e6564752f696d616765732f6d6f64656c2e6a7067"><img width="50%" src="https://camo.githubusercontent.com/056648111d57935bc551879943c486d07a0c81cc2b404cbe858c6f1d000da038/687474703a2f2f6d6172726e65742e637361696c2e6d69742e6564752f696d616765732f6d6f64656c2e6a7067" data-canonical-src="http://marrnet.csail.mit.edu/images/model.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">📷👾🎲</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">学习多视图立体机器（2017 NIPS）</font></font></b> <a href="http://bair.berkeley.edu/blog/2017/09/05/unified-3d/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/e11a91e5800b9d64184d744f66e2601bbccaaed9a769e46602e70790e2940b20/687474703a2f2f626169722e6265726b656c65792e6564752f7374617469632f626c6f672f756e69666965642d33642f4e6574776f726b2e706e67"><img width="50%" src="https://camo.githubusercontent.com/e11a91e5800b9d64184d744f66e2601bbccaaed9a769e46602e70790e2940b20/687474703a2f2f626169722e6265726b656c65792e6564752f7374617469632f626c6f672f756e69666965642d33642f4e6574776f726b2e706e67" data-canonical-src="http://bair.berkeley.edu/static/blog/unified-3d/Network.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">👾 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3DMatch：从 RGB-D 重建中学习局部几何描述符（2017）</font></font></b> <a href="http://3dmatch.cs.princeton.edu/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/6eaadbace0a8920da7ec2a33a16fcf42c678077db8e594e981b3ec7c7139c1b4/687474703a2f2f33646d617463682e63732e7072696e6365746f6e2e6564752f696d672f6f766572766965772e6a7067"><img width="50%" src="https://camo.githubusercontent.com/6eaadbace0a8920da7ec2a33a16fcf42c678077db8e594e981b3ec7c7139c1b4/687474703a2f2f33646d617463682e63732e7072696e6365746f6e2e6564752f696d672f6f766572766965772e6a7067" data-canonical-src="http://3dmatch.cs.princeton.edu/img/overview.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">👾</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">缩放 CNN 以从单个图像进行高分辨率体积重建 (2017) </font></font></b> <a href="https://ieeexplore.ieee.org/document/8265323/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/frankhjwx/3D-Machine-Learning/blob/master/imgs/Scaling%20CNN%20Reconstruction.png"><img width="50%" src="https://github.com/frankhjwx/3D-Machine-Learning/raw/master/imgs/Scaling%20CNN%20Reconstruction.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">💊 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ComplementMe：3D 建模的弱监督组件建议（2017）</font></font></b> <a href="https://arxiv.org/pdf/1708.01841.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/130e9c0bd9b56df029295faf56e16675480ec8ef08c2ab0cfa1617c86e11bc5a/68747470733a2f2f6d6873756e672e6769746875622e696f2f6173736574732f696d616765732f636f6d706c656d656e742d6d652f6669677572655f322e706e67"><img width="50%" src="https://camo.githubusercontent.com/130e9c0bd9b56df029295faf56e16675480ec8ef08c2ab0cfa1617c86e11bc5a/68747470733a2f2f6d6873756e672e6769746875622e696f2f6173736574732f696d616765732f636f6d706c656d656e742d6d652f6669677572655f322e706e67" data-canonical-src="https://mhsung.github.io/assets/images/complement-me/figure_2.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">👾</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">用于 3D 形状合成和分析的学习描述符网络 (2018 CVPR) </font></font></b>    <a href="http://www.stat.ucla.edu/~jxie/3DEBM/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[项目] </font></font></a> <a href="http://www.stat.ucla.edu/~jxie/3DDescriptorNet/3DDescriptorNet_file/doc/3DDescriptorNet.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> [</font></font><a href="https://github.com/jianwen-xie/3DDescriptorNet"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">代码</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">]</font></font></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">基于能量的 3D 形状描述符网络是一种基于深度能量的体积形状图案模型。</font><font style="vertical-align: inherit;">模型的最大似然训练遵循“综合分析”方案，可以解释为模式搜索和模式转换过程。</font><font style="vertical-align: inherit;">该模型可以通过 MCMC（例如 Langevin 动力学）从概率分布中采样来合成 3D 形状图案。</font><font style="vertical-align: inherit;">实验表明，所提出的模型可以生成逼真的 3D 形状图案，并且可用于 3D 形状分析。</font></font></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/9daf473a7f4a18317383697083511d787f98c4a2f77323bf1069ac30501671f3/687474703a2f2f7777772e737461742e75636c612e6564752f7e6a7869652f334445424d2f66696c65732f33445f73796e2e706e67"><img width="60%" src="https://camo.githubusercontent.com/9daf473a7f4a18317383697083511d787f98c4a2f77323bf1069ac30501671f3/687474703a2f2f7777772e737461742e75636c612e6564752f7e6a7869652f334445424d2f66696c65732f33445f73796e2e706e67" data-canonical-src="http://www.stat.ucla.edu/~jxie/3DEBM/files/3D_syn.png" style="max-width: 100%;"></a></p> 
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">🎲 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">PU-Net：点云上采样网络（2018）</font></font></b> <a href="https://arxiv.org/pdf/1801.06761.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/yulequan/PU-Net"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/51e38a2c778e6f01cb62b413d351262cf13379942b392484c05fdf235dec39b6/687474703a2f2f6170707372762e6373652e6375686b2e6564752e686b2f7e6c7179752f696e646578706963732f50752d4e65742e706e67"><img width="50%" src="https://camo.githubusercontent.com/51e38a2c778e6f01cb62b413d351262cf13379942b392484c05fdf235dec39b6/687474703a2f2f6170707372762e6373652e6375686b2e6564752e686b2f7e6c7179752f696e646578706963732f50752d4e65742e706e67" data-canonical-src="http://appsrv.cse.cuhk.edu.hk/~lqyu/indexpics/Pu-Net.png" style="max-width: 100%;"></a></p> 
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">📷👾</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">多视图一致性作为学习形状和姿势预测的监督信号（2018 CVPR）</font></font></b> <a href="https://shubhtuls.github.io/mvcSnP/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/a5da4622e393300cd5d23bd7c5dbfcaaeca895457545c55671767315b5fe6c5b/68747470733a2f2f736875626874756c732e6769746875622e696f2f6d7663536e502f7265736f75726365732f696d616765732f7465617365722e706e67"><img width="50%" src="https://camo.githubusercontent.com/a5da4622e393300cd5d23bd7c5dbfcaaeca895457545c55671767315b5fe6c5b/68747470733a2f2f736875626874756c732e6769746875622e696f2f6d7663536e502f7265736f75726365732f696d616765732f7465617365722e706e67" data-canonical-src="https://shubhtuls.github.io/mvcSnP/resources/images/teaser.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">📷🎲</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">具有 Deep Shape Prior 的以对象为中心的光度束调整 (2018) </font></font></b> <a href="http://ci2cv.net/media/papers/WACV18.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/916882152f4f972b18a30e38f21dc109b3428a823e4d41b97fc175564ea1b8c9/68747470733a2f2f6368656e687375616e6c696e2e6269746275636b65742e696f2f696d616765732f72702f7230362e706e67"><img width="50%" src="https://camo.githubusercontent.com/916882152f4f972b18a30e38f21dc109b3428a823e4d41b97fc175564ea1b8c9/68747470733a2f2f6368656e687375616e6c696e2e6269746275636b65742e696f2f696d616765732f72702f7230362e706e67" data-canonical-src="https://chenhsuanlin.bitbucket.io/images/rp/r06.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">📷🎲</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">学习高效点云生成以进行密集 3D 对象重建（2018 AAAI）</font></font></b> <a href="https://chenhsuanlin.bitbucket.io/3D-point-cloud-generation/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/97f2c278902b9c48e5c96e2fb7bb288c462f762dce0c63d268f6885906801c80/68747470733a2f2f6368656e687375616e6c696e2e6269746275636b65742e696f2f696d616765732f72702f7230352e706e67"><img width="50%" src="https://camo.githubusercontent.com/97f2c278902b9c48e5c96e2fb7bb288c462f762dce0c63d268f6885906801c80/68747470733a2f2f6368656e687375616e6c696e2e6269746275636b65742e696f2f696d616765732f72702f7230352e706e67" data-canonical-src="https://chenhsuanlin.bitbucket.io/images/rp/r05.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">💎 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pixel2Mesh：从单个 RGB 图像生成 3D 网格模型 (2018) </font></font></b> <a href="https://github.com/nywang16/Pixel2Mesh"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/af13cf6ea0243ded397f5ecfbc5041fa71c0063f3ecc292683bbee1d4bb0d623/68747470733a2f2f7777772e67726f756e6461692e636f6d2f6d656469612f61727869765f70726f6a656374732f3138383931312f78322e706e672e37353078305f7137355f63726f702e706e67"><img width="50%" src="https://camo.githubusercontent.com/af13cf6ea0243ded397f5ecfbc5041fa71c0063f3ecc292683bbee1d4bb0d623/68747470733a2f2f7777772e67726f756e6461692e636f6d2f6d656469612f61727869765f70726f6a656374732f3138383931312f78322e706e672e37353078305f7137355f63726f702e706e67" data-canonical-src="https://www.groundai.com/media/arxiv_projects/188911/x2.png.750x0_q75_crop.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">💎 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">AtlasNet：学习 3D 表面生成的纸质方法（2018 CVPR）</font></font></b> <a href="http://imagine.enpc.fr/~groueixt/atlasnet/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/ThibaultGROUEIX/AtlasNet"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/5b93ed1cd2d76bb82d13167c4abbb66de3f73e164e5f0464f666d65dd4bf1e52/687474703a2f2f696d6167696e652e656e70632e66722f7e67726f75656978742f61746c61736e65742f696d67732f7465617365722e736d616c6c2e706e67"><img width="50%" src="https://camo.githubusercontent.com/5b93ed1cd2d76bb82d13167c4abbb66de3f73e164e5f0464f666d65dd4bf1e52/687474703a2f2f696d6167696e652e656e70632e66722f7e67726f75656978742f61746c61736e65742f696d67732f7465617365722e736d616c6c2e706e67" data-canonical-src="http://imagine.enpc.fr/~groueixt/atlasnet/imgs/teaser.small.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">👾💎 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Deep Marching Cubes：学习显式表面表示（2018 CVPR）</font></font></b> <a href="http://www.cvlibs.net/publications/Liao2018CVPR.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/frankhjwx/3D-Machine-Learning/blob/master/imgs/Deep%20Marching%20Cubes.png"><img width="50%" src="https://github.com/frankhjwx/3D-Machine-Learning/raw/master/imgs/Deep%20Marching%20Cubes.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">👾 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Im2Avatar：从单个图像重建彩色 3D (2018) </font></font></b> <a href="https://arxiv.org/pdf/1804.06375v1.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/syb7573330/im2avatar/blob/master/misc/demo_teaser.png"><img width="50%" src="https://github.com/syb7573330/im2avatar/raw/master/misc/demo_teaser.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">💎</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">从图像集合中学习特定类别的网格重建（2018）</font></font></b> <a href="https://akanazawa.github.io/cmr/#" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/7865e23d083be182ef2e9b5b694b2ebe7a32e9dfd666c9ccfe193ec3a379d2c0/68747470733a2f2f616b616e617a6177612e6769746875622e696f2f636d722f7265736f75726365732f696d616765732f7465617365722e706e67"><img width="50%" src="https://camo.githubusercontent.com/7865e23d083be182ef2e9b5b694b2ebe7a32e9dfd666c9ccfe193ec3a379d2c0/68747470733a2f2f616b616e617a6177612e6769746875622e696f2f636d722f7265736f75726365732f696d616765732f7465617365722e706e67" data-canonical-src="https://akanazawa.github.io/cmr/resources/images/teaser.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">💊 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">CSGNet：构造实体几何的神经形状解析器（2018）</font></font></b> <a href="https://arxiv.org/pdf/1712.08290.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/f734bf64b00c6b066a8db32689e8b8d677184078941631668882b9f16f2a3e88/68747470733a2f2f7062732e7477696d672e636f6d2f6d656469612f44522d5267626155384145796a65572e6a7067"><img width="50%" src="https://camo.githubusercontent.com/f734bf64b00c6b066a8db32689e8b8d677184078941631668882b9f16f2a3e88/68747470733a2f2f7062732e7477696d672e636f6d2f6d656469612f44522d5267626155384145796a65572e6a7067" data-canonical-src="https://pbs.twimg.com/media/DR-RgbaU8AEyjeW.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">👾 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Text2Shape：通过学习联合嵌入从自然语言生成形状（2018）</font></font></b> <a href="http://text2shape.stanford.edu/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/901930ab3f1067d56d27c2a3443c4e94dd5626009010b68e11c3b8ca9f114b3c/687474703a2f2f746578743273686170652e7374616e666f72642e6564752f666967757265732f70756c6c2e706e67"><img width="50%" src="https://camo.githubusercontent.com/901930ab3f1067d56d27c2a3443c4e94dd5626009010b68e11c3b8ca9f114b3c/687474703a2f2f746578743273686170652e7374616e666f72642e6564752f666967757265732f70756c6c2e706e67" data-canonical-src="http://text2shape.stanford.edu/figures/pull.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">👾💎📷</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">用于高分辨率 3D 对象表示的多视图轮廓和深度分解 (2018) </font></font></b>  <a href="https://arxiv.org/abs/1802.09987" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/EdwardSmith1884/Multi-View-Silhouette-and-Depth-Decomposition-for-High-Resolution-3D-Object-Representation"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="/timzhang642/3D-Machine-Learning/blob/master/imgs/decomposition_new.png"><img width="60%" src="/timzhang642/3D-Machine-Learning/raw/master/imgs/decomposition_new.png" style="max-width: 100%;"></a> <a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/3D-Machine-Learning/blob/master/imgs/Multi-View%20Silhouette%20and%20Depth%20Decomposition%20for%20High%20Resolution%203D%20Object%20Representation.png"><img width="60%" src="https://github.com/timzhang642/3D-Machine-Learning/raw/master/imgs/Multi-View%20Silhouette%20and%20Depth%20Decomposition%20for%20High%20Resolution%203D%20Object%20Representation.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">👾💎📷</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">像素、体素和视图：单视图 3D 对象形状预测的形状表示研究（2018 CVPR）</font></font></b>  <a href="https://arxiv.org/abs/1804.06032" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="/timzhang642/3D-Machine-Learning/blob/master/imgs/pixels-voxels-views-rgb2mesh.png"><img width="60%" src="/timzhang642/3D-Machine-Learning/raw/master/imgs/pixels-voxels-views-rgb2mesh.png" style="max-width: 100%;"></a> </p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">📷🎲</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">神经场景表示和渲染（2018）</font></font></b> <a href="https://deepmind.com/blog/neural-scene-representation-and-rendering/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/e855c0d01b6249aac6ed909c8739e46ba02e997c2b9ef0c3d7fe039dac161091/687474703a2f2f7777772e6172696d6f72636f732e636f6d2f7374617469632f696d616765732f7075626c69636174696f6e5f696d616765732f67716e5f696d6167652e706e67"><img width="50%" src="https://camo.githubusercontent.com/e855c0d01b6249aac6ed909c8739e46ba02e997c2b9ef0c3d7fe039dac161091/687474703a2f2f7777772e6172696d6f72636f732e636f6d2f7374617469632f696d616765732f7075626c69636174696f6e5f696d616765732f67716e5f696d6167652e706e67" data-canonical-src="http://www.arimorcos.com/static/images/publication_images/gqn_image.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">💊 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Im2Struct：从单个 RGB 图像恢复 3D 形状结构（2018 CVPR）</font></font></b> <a href="https://arxiv.org/pdf/1804.05469.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/a0b99b2bdaba077b7aac0de1a059d87a758af01267c955a45185cb4b42c8dd0e/68747470733a2f2f6b6576696e6b616978752e6e65742f696d616765732f7075626c69636174696f6e732f6e69755f6376707231382e6a7067"><img width="50%" src="https://camo.githubusercontent.com/a0b99b2bdaba077b7aac0de1a059d87a758af01267c955a45185cb4b42c8dd0e/68747470733a2f2f6b6576696e6b616978752e6e65742f696d616765732f7075626c69636174696f6e732f6e69755f6376707231382e6a7067" data-canonical-src="https://kevinkaixu.net/images/publications/niu_cvpr18.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">🎲 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">FoldingNet：通过深度网格变形的点云自动编码器（2018 CVPR）</font></font></b> <a href="https://arxiv.org/pdf/1712.07262.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/f5ab04daecd42eef238527a18cb83abb7fe1e9e9caea8700c822eda132c23b79/687474703a2f2f73696d6261666f72726573742e6769746875622e696f2f6669672f466f6c64696e674e65742e6a7067"><img width="50%" src="https://camo.githubusercontent.com/f5ab04daecd42eef238527a18cb83abb7fe1e9e9caea8700c822eda132c23b79/687474703a2f2f73696d6261666f72726573742e6769746875622e696f2f6669672f466f6c64696e674e65742e6a7067" data-canonical-src="http://simbaforrest.github.io/fig/FoldingNet.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">📷👾 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pix3D：单图像 3D 形状建模的数据集和方法（2018 CVPR）</font></font></b> <a href="http://pix3d.csail.mit.edu/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/3D-Machine-Learning/blob/master/imgs/Pix3D%20-%20Dataset%20and%20Methods%20for%20Single-Image%203D%20Shape%20Modeling%20(2018%20CVPR).png"><img width="50%" src="https://github.com/timzhang642/3D-Machine-Learning/raw/master/imgs/Pix3D%20-%20Dataset%20and%20Methods%20for%20Single-Image%203D%20Shape%20Modeling%20(2018%20CVPR).png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">💎 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3D-RCNN：通过渲染和比较进行实例级 3D 对象重建（2018 CVPR）</font></font></b> <a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/1128.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/3D-Machine-Learning/blob/master/imgs/3D-RCNN-%20Instance-level%203D%20Object%20Reconstruction%20via%20Render-and-Compare%20(2018%20CVPR).jpeg"><img width="50%" src="https://github.com/timzhang642/3D-Machine-Learning/raw/master/imgs/3D-RCNN-%20Instance-level%203D%20Object%20Reconstruction%20via%20Render-and-Compare%20(2018%20CVPR).jpeg" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">👾 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Matryoshka Networks：通过嵌套形状层预测 3D 几何（2018 CVPR）</font></font></b> <a href="https://arxiv.org/pdf/1804.10975.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/3D-Machine-Learning/blob/master/imgs/Matryoshka%20Networks-%20Predicting%203D%20Geometry%20via%20Nested%20Shape%20Layers%20(2018%20CVPR).jpeg"><img width="50%" src="https://github.com/timzhang642/3D-Machine-Learning/raw/master/imgs/Matryoshka%20Networks-%20Predicting%203D%20Geometry%20via%20Nested%20Shape%20Layers%20(2018%20CVPR).jpeg" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">💎</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
使用图卷积自动编码器完成可变形形状（2018 CVPR）</font></font></b> <a href="https://arxiv.org/pdf/1712.00268v1.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/cf2cc3791a74d3565485cdf6239e59aeec06d925c746858fe2ef793278ddbd8b/68747470733a2f2f6f726c6974616e792e6769746875622e696f2f4f4c5f66696c65732f7368617065436f6d702e706e67"><img width="50%" src="https://camo.githubusercontent.com/cf2cc3791a74d3565485cdf6239e59aeec06d925c746858fe2ef793278ddbd8b/68747470733a2f2f6f726c6974616e792e6769746875622e696f2f4f4c5f66696c65732f7368617065436f6d702e706e67" data-canonical-src="https://orlitany.github.io/OL_files/shapeComp.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">👾 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3D 形状的全局到局部生成模型（SIGGRAPH Asia 2018）</font></font></b> <a href="http://vcc.szu.edu.cn/research/2018/G2L" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a><a href="https://github.com/Hao-HUST/G2LGAN"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/3D-Machine-Learning/blob/master/imgs/Global-to-Local%20Generative%20Model%20for%203D%20Shapes.jpg"><img width="50%" src="https://github.com/timzhang642/3D-Machine-Learning/raw/master/imgs/Global-to-Local%20Generative%20Model%20for%203D%20Shapes.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">💎🎲👾 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ALIGNet：通过无监督学习实现部分形状不可知的对齐（TOG 2018）</font></font></b> <a href="https://bit.ly/alignet" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/ranahanocka/ALIGNet/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/ranahanocka/ALIGNet/blob/master/docs/rep.png"><img width="50%" src="https://github.com/ranahanocka/ALIGNet/raw/master/docs/rep.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">🎲👾 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">PointGrid：用于 3D 形状理解的深度网络 (CVPR 2018) </font></font></b> <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Le_PointGrid_A_Deep_CVPR_2018_paper.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/trucleduc/PointGrid"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/3D-Machine-Learning/blob/master/imgs/PointGrid-%20A%20Deep%20Network%20for%203D%20Shape%20Understanding%20(2018).jpeg"><img width="50%" src="https://github.com/timzhang642/3D-Machine-Learning/raw/master/imgs/PointGrid-%20A%20Deep%20Network%20for%203D%20Shape%20Understanding%20(2018).jpeg" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">🎲 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">GAL：单视图 3D 对象重建的几何对抗损失（2018）</font></font></b> <a href="https://xjqi.github.io/GAL.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><animated-image data-catalyst="" style="width: 50%;"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/23f45240e455f27af8d051c8ac60b599962776f3a81e97e2886545f7d9ef4270/68747470733a2f2f6d656469612e737072696e6765726e61747572652e636f6d2f6f726967696e616c2f737072696e6765722d7374617469632f696d6167652f63687025334131302e313030372532463937382d332d3033302d30313233372d335f34392f4d656469614f626a656374732f3437343231335f315f456e5f34395f466967325f48544d4c2e676966" data-target="animated-image.originalLink"><img src="https://camo.githubusercontent.com/23f45240e455f27af8d051c8ac60b599962776f3a81e97e2886545f7d9ef4270/68747470733a2f2f6d656469612e737072696e6765726e61747572652e636f6d2f6f726967696e616c2f737072696e6765722d7374617469632f696d6167652f63687025334131302e313030372532463937382d332d3033302d30313233372d335f34392f4d656469614f626a656374732f3437343231335f315f456e5f34395f466967325f48544d4c2e676966" data-canonical-src="https://media.springernature.com/original/springer-static/image/chp%3A10.1007%2F978-3-030-01237-3_49/MediaObjects/474213_1_En_49_Fig2_HTML.gif" style="max-width: 100%; display: inline-block;" data-target="animated-image.originalImage"></a>
      <span class="AnimatedImagePlayer" data-target="animated-image.player" hidden="">
        <a data-target="animated-image.replacedLink" class="AnimatedImagePlayer-images" href="https://camo.githubusercontent.com/23f45240e455f27af8d051c8ac60b599962776f3a81e97e2886545f7d9ef4270/68747470733a2f2f6d656469612e737072696e6765726e61747572652e636f6d2f6f726967696e616c2f737072696e6765722d7374617469632f696d6167652f63687025334131302e313030372532463937382d332d3033302d30313233372d335f34392f4d656469614f626a656374732f3437343231335f315f456e5f34395f466967325f48544d4c2e676966" target="_blank">
          
        <span data-target="animated-image.imageContainer">
            <img data-target="animated-image.replacedImage" alt="68747470733a2f2f6d656469612e737072696e6765726e61747572652e636f6d2f6f726967696e616c2f737072696e6765722d7374617469632f696d616 7652f63687025334131302e313030372532463937382d332d3033302d30313233372d335f34392f4d656469614f626a656374732f3437343231335f315f45 6e5f34395f466967325f48544d4c2e676966" class="AnimatedImagePlayer-animatedImage" src="https://camo.githubusercontent.com/23f45240e455f27af8d051c8ac60b599962776f3a81e97e2886545f7d9ef4270/68747470733a2f2f6d656469612e737072696e6765726e61747572652e636f6d2f6f726967696e616c2f737072696e6765722d7374617469632f696d6167652f63687025334131302e313030372532463937382d332d3033302d30313233372d335f34392f4d656469614f626a656374732f3437343231335f315f456e5f34395f466967325f48544d4c2e676966" style="display: block; opacity: 1;">
          <canvas class="AnimatedImagePlayer-stillImage" aria-hidden="true" width="399" height="272"></canvas></span></a>
        <button data-target="animated-image.imageButton" class="AnimatedImagePlayer-images" tabindex="-1" aria-label="播放 68747470733a2f2f6d656469612e737072696e6765726e61747572652e636f6d2f6f726967696e616c2f737072696e6765722d7374617469632f696d61 67652f63687025334131302e313030372532463937382d332d3033302d30313233372d335f34392f4d656469614f626a656374732f3437343231335f315f4 56e5f34395f466967325f48544d4c2e676966" hidden=""></button>
        <span class="AnimatedImagePlayer-controls" data-target="animated-image.controls" hidden="">
          <button data-target="animated-image.playButton" class="AnimatedImagePlayer-button" aria-label="播放 68747470733a2f2f6d656469612e737072696e6765726e61747572652e636f6d2f6f726967696e616c2f737072696e6765722d7374617469632f696d61 67652f63687025334131302e313030372532463937382d332d3033302d30313233372d335f34392f4d656469614f626a656374732f3437343231335f315f4 56e5f34395f466967325f48544d4c2e676966">
            <svg aria-hidden="true" focusable="false" class="octicon icon-play" width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg">
              <path d="M4 13.5427V2.45734C4 1.82607 4.69692 1.4435 5.2295 1.78241L13.9394 7.32507C14.4334 7.63943 14.4334 8.36057 13.9394 8.67493L5.2295 14.2176C4.69692 14.5565 4 14.1739 4 13.5427Z">
            </path></svg>
            <svg aria-hidden="true" focusable="false" class="octicon icon-pause" width="16" height="16" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg">
              <rect x="4" y="2" width="3" height="12" rx="1"></rect>
              <rect x="9" y="2" width="3" height="12" rx="1"></rect>
            </svg>
          </button>
          <a data-target="animated-image.openButton" aria-label="打开68747470733a2f2f6d656469612e737072696e6765726e61747572652e636f6d2f6f726967696e616c2f737072696e6765722d7374617469632f696d61 67652f63687025334131302e313030372532463937382d332d3033302d30313233372d335f34392f4d656469614f626a656374732f3437343231335f315f4 56e5f34395f466967325f48544d4c2e676966 在新窗口中" class="AnimatedImagePlayer-button" href="https://camo.githubusercontent.com/23f45240e455f27af8d051c8ac60b599962776f3a81e97e2886545f7d9ef4270/68747470733a2f2f6d656469612e737072696e6765726e61747572652e636f6d2f6f726967696e616c2f737072696e6765722d7374617469632f696d6167652f63687025334131302e313030372532463937382d332d3033302d30313233372d335f34392f4d656469614f626a656374732f3437343231335f315f456e5f34395f466967325f48544d4c2e676966" target="_blank">
            <svg aria-hidden="true" class="octicon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" width="16" height="16">
              <path fill-rule="evenodd" d="M10.604 1h4.146a.25.25 0 01.25.25v4.146a.25.25 0 01-.427.177L13.03 4.03 9.28 7.78a.75.75 0 01-1.06-1.06l3.75-3.75-1.543-1.543A.25.25 0 0110.604 1zM3.75 2A1.75 1.75 0 002 3.75v8.5c0 .966.784 1.75 1.75 1.75h8.5A1.75 1.75 0 0014 12.25v-3.5a.75.75 0 00-1.5 0v3.5a.25.25 0 01-.25.25h-8.5a.25.25 0 01-.25-.25v-8.5a.25.25 0 01.25-.25h3.5a.75.75 0 000-1.5h-3.5z"></path>
            </svg>
          </a>
        </span>
      </span></animated-image></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">🎲</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">视觉对象网络：具有解缠结 3D 表示的图像生成（2018）</font></font></b> <a href="https://papers.nips.cc/paper/7297-visual-object-networks-image-generation-with-disentangled-3d-representations.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/3D-Machine-Learning/blob/master/imgs/Visual%20Object%20Networks-%20Image%20Generation%20with%20Disentangled%203D%20Representation%20(2018).jpeg"><img width="50%" src="https://github.com/timzhang642/3D-Machine-Learning/raw/master/imgs/Visual%20Object%20Networks-%20Image%20Generation%20with%20Disentangled%203D%20Representation%20(2018).jpeg" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">👾</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">学习推断和执行 3D 形状程序 (2019)) </font></font></b> <a href="http://shape2prog.csail.mit.edu/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/ad7e7b8bac2a245da71dcbce8f05730ec8c80b110bbdd63d793a75dc0c4f7755/687474703a2f2f73686170653270726f672e637361696c2e6d69742e6564752f73686170655f66696c65732f7465617365722e6a7067"><img width="50%" src="https://camo.githubusercontent.com/ad7e7b8bac2a245da71dcbce8f05730ec8c80b110bbdd63d793a75dc0c4f7755/687474703a2f2f73686170653270726f672e637361696c2e6d69742e6564752f73686170655f66696c65732f7465617365722e6a7067" data-canonical-src="http://shape2prog.csail.mit.edu/shape_files/teaser.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">👾</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">学习推断和执行 3D 形状程序 (2019)) </font></font></b> <a href="https://arxiv.org/pdf/1901.05103.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/3076606db6fc4ddf307973e60c09d9f5d79b326f3e64efe5d7fe70af60b57df0/68747470733a2f2f7062732e7477696d672e636f6d2f6d656469612f44784661572d6d553841456f3977632e6a7067"><img width="50%" src="https://camo.githubusercontent.com/3076606db6fc4ddf307973e60c09d9f5d79b326f3e64efe5d7fe70af60b57df0/68747470733a2f2f7062732e7477696d672e636f6d2f6d656469612f44784661572d6d553841456f3977632e6a7067" data-canonical-src="https://pbs.twimg.com/media/DxFaW-mU8AEo9wc.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">💎</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">学习单视图 3D 重建的视图先验（CVPR 2019）</font></font></b> <a href="http://hiroharu-kato.com/projects_en/view_prior_learning.html" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/3D-Machine-Learning/blob/master/imgs/Learning%20View%20Priors%20for%20Single-view%203D%20Reconstruction.png"><img width="50%" src="https://github.com/timzhang642/3D-Machine-Learning/raw/master/imgs/Learning%20View%20Priors%20for%20Single-view%203D%20Reconstruction.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">💎🎲</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">使用二次损失学习 3D 模型的嵌入 (BMVC 2019) </font></font></b> <a href="https://arxiv.org/abs/1907.10250" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/nitinagarwal/QuadricLoss"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/ee9fa48921f1f934ac03952f50ca1405046d2d634cc5a07dab114e82a57675da/68747470733a2f2f7777772e6963732e7563692e6564752f7e6167617277616c2f626d76635f323031392e706e67"><img width="50%" src="https://camo.githubusercontent.com/ee9fa48921f1f934ac03952f50ca1405046d2d634cc5a07dab114e82a57675da/68747470733a2f2f7777772e6963732e7563692e6564752f7e6167617277616c2f626d76635f323031392e706e67" data-canonical-src="https://www.ics.uci.edu/~agarwal/bmvc_2019.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">🎲 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">CompoNet：学习通过零件合成和组合生成看不见的东西（ICCV 2019）</font></font></b> <a href="https://arxiv.org/abs/1811.07441" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a><a href="https://github.com/nschor/CompoNet"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/nschor/CompoNet/master/images/network_architecture.png"><img width="50%" src="https://raw.githubusercontent.com/nschor/CompoNet/master/images/network_architecture.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">CoMA：卷积网格自动编码器 (2018) </font></font></b> <a href="https://ps.is.tuebingen.mpg.de/uploads_file/attachment/attachment/439/1285.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a><a href="https://github.com/anuragranj/coma"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码 (TF)] </font></font></a><a href="https://github.com/pixelite1201/pytorch_coma/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码 (PyTorch)] </font></font></a><a href="https://github.com/sw-gong/coma"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码 (PyTorch)] </font></font></a>
<br><a href="https://coma.is.tue.mpg.de/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">CoMA</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">是一种多功能模型，它使用网格上的谱卷积来学习面部的非线性表示表面。</font><font style="vertical-align: inherit;">CoMA 引入了网格采样操作，可实现分层网格表示，捕获模型内多个尺度的形状和表达的非线性变化。</font></font></p>
<p align="center" dir="auto"> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/d51a2466493f76a3149798121b2e4b0beec6fac5aa06b020d499019a2a66da8f/68747470733a2f2f636f6d612e69732e7475652e6d70672e64652f75706c6f6164732f636b656469746f722f70696374757265732f39312f636f6e74656e745f636f6d615f66616365732e6a7067"><img width="50%" src="https://camo.githubusercontent.com/d51a2466493f76a3149798121b2e4b0beec6fac5aa06b020d499019a2a66da8f/68747470733a2f2f636f6d612e69732e7475652e6d70672e64652f75706c6f6164732f636b656469746f722f70696374757265732f39312f636f6e74656e745f636f6d615f66616365732e6a7067" data-canonical-src="https://coma.is.tue.mpg.de/uploads/ckeditor/pictures/91/content_coma_faces.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">RingNet：从单张图像重建 3D 人脸 (2019) </font></font></b> <a href="https://ps.is.tuebingen.mpg.de/uploads_file/attachment/attachment/509/paper_camera_ready.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a><a href="https://github.com/soubhiksanyal/RingNet"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"> <animated-image data-catalyst="" style="width: 50%;"><a target="_blank" rel="noopener noreferrer" href="https://github.com/soubhiksanyal/RingNet/blob/master/gif/celeba_reconstruction.gif" data-target="animated-image.originalLink"><img src="https://github.com/soubhiksanyal/RingNet/raw/master/gif/celeba_reconstruction.gif" style="max-width: 100%; display: inline-block;" data-target="animated-image.originalImage"></a>
      <span class="AnimatedImagePlayer" data-target="animated-image.player" hidden="">
        <a data-target="animated-image.replacedLink" class="AnimatedImagePlayer-images" href="https://github.com/soubhiksanyal/RingNet/blob/master/gif/celeba_reconstruction.gif" target="_blank">
          
        <span data-target="animated-image.imageContainer">
            <img data-target="animated-image.replacedImage" alt="celeba_reconstruction.gif" class="AnimatedImagePlayer-animatedImage" src="https://github.com/soubhiksanyal/RingNet/raw/master/gif/celeba_reconstruction.gif" style="display: block; opacity: 1;">
          <canvas class="AnimatedImagePlayer-stillImage" aria-hidden="true" width="399" height="196"></canvas></span></a>
        <button data-target="animated-image.imageButton" class="AnimatedImagePlayer-images" tabindex="-1" aria-label="Play celeba_reconstruction.gif" hidden=""></button>
        <span class="AnimatedImagePlayer-controls" data-target="animated-image.controls" hidden="">
          <button data-target="animated-image.playButton" class="AnimatedImagePlayer-button" aria-label="Play celeba_reconstruction.gif">
            <svg aria-hidden="true" focusable="false" class="octicon icon-play" width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg">
              <path d="M4 13.5427V2.45734C4 1.82607 4.69692 1.4435 5.2295 1.78241L13.9394 7.32507C14.4334 7.63943 14.4334 8.36057 13.9394 8.67493L5.2295 14.2176C4.69692 14.5565 4 14.1739 4 13.5427Z">
            </path></svg>
            <svg aria-hidden="true" focusable="false" class="octicon icon-pause" width="16" height="16" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg">
              <rect x="4" y="2" width="3" height="12" rx="1"></rect>
              <rect x="9" y="2" width="3" height="12" rx="1"></rect>
            </svg>
          </button>
          <a data-target="animated-image.openButton" aria-label="在新窗口中打开" class="AnimatedImagePlayer-button" href="https://github.com/soubhiksanyal/RingNet/blob/master/gif/celeba_reconstruction.gif" target="_blank">
            <svg aria-hidden="true" class="octicon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" width="16" height="16">
              <path fill-rule="evenodd" d="M10.604 1h4.146a.25.25 0 01.25.25v4.146a.25.25 0 01-.427.177L13.03 4.03 9.28 7.78a.75.75 0 01-1.06-1.06l3.75-3.75-1.543-1.543A.25.25 0 0110.604 1zM3.75 2A1.75 1.75 0 002 3.75v8.5c0 .966.784 1.75 1.75 1.75h8.5A1.75 1.75 0 0014 12.25v-3.5a.75.75 0 00-1.5 0v3.5a.25.25 0 01-.25.25h-8.5a.25.25 0 01-.25-.25v-8.5a.25.25 0 01.25-.25h3.5a.75.75 0 000-1.5h-3.5z"></path>
            </svg>
          </a>
        </span>
      </span></animated-image></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">VOCA：语音操作角色动画 (2019) </font></font></b> <a href="https://ps.is.tuebingen.mpg.de/uploads_file/attachment/attachment/510/paper_final.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a><a href="https://youtu.be/XceCxf_GyW4" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[视频] </font></font></a><a href="https://github.com/TimoBolkart/voca"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码] </font></font></a>
<br><a href="https://voca.is.tue.mpg.de/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">VOCA</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">是一种简单且通用的语音驱动的面部动画框架，适用于各种身份。</font><font style="vertical-align: inherit;">该代码库演示了如何在给定任意语音信号和静态角色网格的情况下合成逼真的角色动画。</font></font></p>
<p align="center" dir="auto"> <animated-image data-catalyst="" style="width: 50%;"><a target="_blank" rel="noopener noreferrer" href="https://github.com/TimoBolkart/voca/blob/master/gif/speech_driven_animation.gif" data-target="animated-image.originalLink"><img src="https://github.com/TimoBolkart/voca/raw/master/gif/speech_driven_animation.gif" style="max-width: 100%; display: inline-block;" data-target="animated-image.originalImage"></a>
      <span class="AnimatedImagePlayer" data-target="animated-image.player" hidden="">
        <a data-target="animated-image.replacedLink" class="AnimatedImagePlayer-images" href="https://github.com/TimoBolkart/voca/blob/master/gif/speech_driven_animation.gif" target="_blank">
          
        <span data-target="animated-image.imageContainer">
            <img data-target="animated-image.replacedImage" alt="speech_driven_animation.gif" class="AnimatedImagePlayer-animatedImage" src="https://github.com/TimoBolkart/voca/raw/master/gif/speech_driven_animation.gif" style="display: block; opacity: 1;">
          <canvas class="AnimatedImagePlayer-stillImage" aria-hidden="true" width="399" height="113"></canvas></span></a>
        <button data-target="animated-image.imageButton" class="AnimatedImagePlayer-images" tabindex="-1" aria-label="Play speech_driven_animation.gif" hidden=""></button>
        <span class="AnimatedImagePlayer-controls" data-target="animated-image.controls" hidden="">
          <button data-target="animated-image.playButton" class="AnimatedImagePlayer-button" aria-label="Play speech_driven_animation.gif">
            <svg aria-hidden="true" focusable="false" class="octicon icon-play" width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg">
              <path d="M4 13.5427V2.45734C4 1.82607 4.69692 1.4435 5.2295 1.78241L13.9394 7.32507C14.4334 7.63943 14.4334 8.36057 13.9394 8.67493L5.2295 14.2176C4.69692 14.5565 4 14.1739 4 13.5427Z">
            </path></svg>
            <svg aria-hidden="true" focusable="false" class="octicon icon-pause" width="16" height="16" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg">
              <rect x="4" y="2" width="3" height="12" rx="1"></rect>
              <rect x="9" y="2" width="3" height="12" rx="1"></rect>
            </svg>
          </button>
          <a data-target="animated-image.openButton" aria-label="在新窗口中打开" class="AnimatedImagePlayer-button" href="https://github.com/TimoBolkart/voca/blob/master/gif/speech_driven_animation.gif" target="_blank">
            <svg aria-hidden="true" class="octicon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" width="16" height="16">
              <path fill-rule="evenodd" d="M10.604 1h4.146a.25.25 0 01.25.25v4.146a.25.25 0 01-.427.177L13.03 4.03 9.28 7.78a.75.75 0 01-1.06-1.06l3.75-3.75-1.543-1.543A.25.25 0 0110.604 1zM3.75 2A1.75 1.75 0 002 3.75v8.5c0 .966.784 1.75 1.75 1.75h8.5A1.75 1.75 0 0014 12.25v-3.5a.75.75 0 00-1.5 0v3.5a.25.25 0 01-.25.25h-8.5a.25.25 0 01-.25-.25v-8.5a.25.25 0 01.25-.25h3.5a.75.75 0 000-1.5h-3.5z"></path>
            </svg>
          </a>
        </span>
      </span></animated-image></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">💎</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">学习使用基于插值的可微渲染器预测 3D 对象</font></font></b> <a href="https://arxiv.org/abs/1908.01210" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a><a href="https://nv-tlabs.github.io/DIB-R/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[站点] </font></font></a><a href="https://github.com/nv-tlabs/DIB-R"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/979a6872f05d870e5785242df62ddaad9ab15c5ac32558e49765ec8ff523e299/68747470733a2f2f6e762d746c6162732e6769746875622e696f2f4449422d522f666967757265732f6d6f64656c32612d322e706e67"><img width="50%" src="https://camo.githubusercontent.com/979a6872f05d870e5785242df62ddaad9ab15c5ac32558e49765ec8ff523e299/68747470733a2f2f6e762d746c6162732e6769746875622e696f2f4449422d522f666967757265732f6d6f64656c32612d322e706e67" data-canonical-src="https://nv-tlabs.github.io/DIB-R/figures/model2a-2.png" style="max-width: 100%;"></a> </p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">💎</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">软光栅化器：用于基于图像的 3D 推理的可微分渲染器</font></font></b> <a href="https://arxiv.org/abs/1904.01786" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a><a href="https://github.com/ShichenLiu/SoftRas"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"> <a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/ShichenLiu/SoftRas/master/data/media/teaser/teaser.png"><img width="50%" src="https://raw.githubusercontent.com/ShichenLiu/SoftRas/master/data/media/teaser/teaser.png" style="max-width: 100%;"></a> </p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">NeRF：将场景表示为视图合成的神经辐射场</font></font></b> <a href="http://www.matthewtancik.com/nerf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[项目] </font></font></a><a href="https://arxiv.org/abs/2003.08934" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a><a href="https://github.com/bmild/nerf"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/a3796f06b645f4defa2b02c45aa95ffaff486cebf32718fb7d3489c4241dbe9d/68747470733a2f2f75706c6f6164732d73736c2e776562666c6f772e636f6d2f3531653064373364383364303662616137613030303030662f3565373030656636303637623433383231656435323736385f706970656c696e655f776562736974652d30312d702d3830302e706e67"><img width="50%" src="https://camo.githubusercontent.com/a3796f06b645f4defa2b02c45aa95ffaff486cebf32718fb7d3489c4241dbe9d/68747470733a2f2f75706c6f6164732d73736c2e776562666c6f772e636f6d2f3531653064373364383364303662616137613030303030662f3565373030656636303637623433383231656435323736385f706970656c696e655f776562736974652d30312d702d3830302e706e67" data-canonical-src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5e700ef6067b43821ed52768_pipeline_website-01-p-800.png" style="max-width: 100%;"></a> </p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">💎🎲 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">GAMesh：深点网络的引导和增强网格划分 (3DV 2020) </font></font></b> <a href="https://www.ics.uci.edu/~agarwal/GAMesh/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[项目] </font></font></a> <a href="https://arxiv.org/abs/2010.09774" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/nitinagarwal/GAMesh"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/427a060b5af71a64b6ea342d35caca7a074b4bedc5e16664f0c028b15c548e8a/68747470733a2f2f7777772e6963732e7563692e6564752f7e6167617277616c2f3344565f323032302e706e67"><img width="50%" src="https://camo.githubusercontent.com/427a060b5af71a64b6ea342d35caca7a074b4bedc5e16664f0c028b15c548e8a/68747470733a2f2f7777772e6963732e7563692e6564752f7e6167617277616c2f3344565f323032302e706e67" data-canonical-src="https://www.ics.uci.edu/~agarwal/3DV_2020.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">👾 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Generative VoxelNet：学习基于能量的 3D 形状合成和分析模型（2020 TPAMI）</font></font></b>   <a href="http://www.stat.ucla.edu/~jxie/3DEBM/3DEBM_file/doc/gVoxelNet.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">本文提出了一种基于深度 3D 能量的模型来表示体积形状。</font><font style="vertical-align: inherit;">模型的最大似然训练遵循“综合分析”方案。</font><font style="vertical-align: inherit;">实验表明，所提出的模型可以生成高质量的 3D 形状图案，并且可用于各种 3D 形状分析。</font></font></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="/timzhang642/3D-Machine-Learning/blob/master/imgs/voxelnet.png"><img width="60%" src="/timzhang642/3D-Machine-Learning/raw/master/imgs/voxelnet.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">🎲 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Generative PointNet：基于无序点集的深度能量学习，用于 3D 生成、重建和分类（2021 CVPR）&nbsp; </font></font></b> <a href="http://www.stat.ucla.edu/~jxie/GPointNet/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[项目] </font></font></a> <a href="https://arxiv.org/pdf/2004.01301.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> [</font></font><a href="https://github.com/fei960922/GPointNet"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">代码</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">]</font></font></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Generative PointNet 是一种基于能量的无序点云模型，其中能量函数由输入排列不变的自下而上神经网络参数化。</font><font style="vertical-align: inherit;">该模型可以通过基于 MCMC 的最大似然学习或针对基于能量的模型的短期 MCMC 进行训练，作为点云重建和插值的流式生成器。</font><font style="vertical-align: inherit;">学习到的点云表示可用于点云分类。</font></font></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="/timzhang642/3D-Machine-Learning/blob/master/imgs/gpointnet.png"><img width="60%" src="/timzhang642/3D-Machine-Learning/raw/master/imgs/gpointnet.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">🎲 💎</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">塑造我的脸型：通过表面到表面翻译注册 3D 面部扫描</font></font></b> <a href="https://arxiv.org/abs/2012.09235" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/mbahri/smf"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Shape My Face (SMF) 是一种点云到网格自动编码器，用于注册原始人脸扫描并生成合成人脸。</font><font style="vertical-align: inherit;">SMF 利用改进的 PointNet 编码器和视觉注意模块和可微表面采样，独立于原始表面表示，并减少预处理的需要。</font><font style="vertical-align: inherit;">网格卷积解码器与口腔的专用 PCA 模型相结合，并根据测地距离平滑混合，以创建对噪声具有高度鲁棒性的紧凑模型。</font><font style="vertical-align: inherit;">SMF 用于对使用 iPhone 深度相机在野外捕获的扫描（以网格或点云表示）进行注册和执行表达传输。</font></font></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="/timzhang642/3D-Machine-Learning/blob/master/imgs/ShapeMyFace.png"><img width="60%" src="/timzhang642/3D-Machine-Learning/raw/master/imgs/ShapeMyFace.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">🎲</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">学习生成形状建模的隐式字段（2019）</font></font></b> <a href="https://arxiv.org/abs/1812.02822" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/timzhang642/3D-Machine-Learning"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">我们提倡使用隐式字段来学习形状的生成模型，并引入一种称为 IM-NET 的隐式字段解码器用于形状生成，旨在提高生成形状的视觉质量。</font><font style="vertical-align: inherit;">隐式字段为 3D 空间中的每个点分配一个值，以便可以将形状提取为等值面。</font><font style="vertical-align: inherit;">IM-NET 经过训练，可以通过二元分类器执行此任务。</font><font style="vertical-align: inherit;">具体来说，它采用点坐标以及编码形状的特征向量，并输出指示该点是否在形状之外的值。</font><font style="vertical-align: inherit;">通过用我们的隐式解码器替换传统解码器进行表示学习（通过 IM-AE）和形状生成（通过 IM-GAN），我们在生成形状建模、插值和单视图 3D 重建等任务中展示了卓越的结果，特别是在视觉质量方面。</font></font></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="/timzhang642/3D-Machine-Learning/blob/master/imgs/IM_NET.png"><img width="60%" src="/timzhang642/3D-Machine-Learning/raw/master/imgs/IM_NET.png" style="max-width: 100%;"></a></p>
<a name="user-content-material_synthesis">
</a><div class="markdown-heading" dir="auto"><a><h2 tabindex="-1" class="heading-element" dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">纹理/材料分析与合成</font></font></h2></a><a id="user-content-texturematerial-analysis-and-synthesis" class="anchor" aria-label="永久链接：纹理/材料分析与合成" href="#texturematerial-analysis-and-synthesis"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">使用卷积神经网络进行纹理合成（2015）</font></font></b> <a href="https://arxiv.org/pdf/1505.07376.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/3D-Machine-Learning/blob/master/imgs/Texture%20Synthesis%20Using%20Convolutional%20Neural%20Networks.jpeg"><img width="50%" src="https://github.com/timzhang642/3D-Machine-Learning/raw/master/imgs/Texture%20Synthesis%20Using%20Convolutional%20Neural%20Networks.jpeg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">固定材料的两次 SVBRDF 捕获（SIGGRAPH 2015）</font></font></b> <a href="https://mediatech.aalto.fi/publications/graphics/TwoShotSVBRDF/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/fa311c54cd5d0ecaaec705b6218ae7e7151aa596a5cc84ecc6b46d606dd69e3b/68747470733a2f2f6d65646961746563682e61616c746f2e66692f7075626c69636174696f6e732f67726170686963732f54776f53686f745356425244462f7465617365722e706e67"><img width="50%" src="https://camo.githubusercontent.com/fa311c54cd5d0ecaaec705b6218ae7e7151aa596a5cc84ecc6b46d606dd69e3b/68747470733a2f2f6d65646961746563682e61616c746f2e66692f7075626c69636174696f6e732f67726170686963732f54776f53686f745356425244462f7465617365722e706e67" data-canonical-src="https://mediatech.aalto.fi/publications/graphics/TwoShotSVBRDF/teaser.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">通过神经纹理合成进行反射率建模（2016）</font></font></b> <a href="https://mediatech.aalto.fi/publications/graphics/NeuralSVBRDF/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/a21512d7ba02959f1fa54979607ea47b52944b7d6d85586e9f904704950789e2/68747470733a2f2f6d65646961746563682e61616c746f2e66692f7075626c69636174696f6e732f67726170686963732f4e657572616c5356425244462f7465617365722e706e67"><img width="50%" src="https://camo.githubusercontent.com/a21512d7ba02959f1fa54979607ea47b52944b7d6d85586e9f904704950789e2/68747470733a2f2f6d65646961746563682e61616c746f2e66692f7075626c69636174696f6e732f67726170686963732f4e657572616c5356425244462f7465617365722e706e67" data-canonical-src="https://mediatech.aalto.fi/publications/graphics/NeuralSVBRDF/teaser.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">使用自增强卷积神经网络对单张照片的表面外观进行建模（2017）</font></font></b> <a href="http://msraig.info/~sanet/sanet.htm" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/a650efe0b899233546e5b011ed344a30cacf20f36aedc27f7cc9e078110e1625/687474703a2f2f6d73726169672e696e666f2f7e73616e65742f7465617365722e6a7067"><img width="50%" src="https://camo.githubusercontent.com/a650efe0b899233546e5b011ed344a30cacf20f36aedc27f7cc9e078110e1625/687474703a2f2f6d73726169672e696e666f2f7e73616e65742f7465617365722e6a7067" data-canonical-src="http://msraig.info/~sanet/teaser.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">高分辨率多尺度神经纹理合成（2017）</font></font></b> <a href="https://wxs.ca/research/multiscale-neural-synthesis/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/851b2954071d7086e83a15f76f91aa493446dc3a5c93edb167b6b5c08bca38a7/68747470733a2f2f7778732e63612f72657365617263682f6d756c74697363616c652d6e657572616c2d73796e7468657369732f6d756c74697363616c652d6772616d2d6d6172626c652e6a7067"><img width="50%" src="https://camo.githubusercontent.com/851b2954071d7086e83a15f76f91aa493446dc3a5c93edb167b6b5c08bca38a7/68747470733a2f2f7778732e63612f72657365617263682f6d756c74697363616c652d6e657572616c2d73796e7468657369732f6d756c74697363616c652d6772616d2d6d6172626c652e6a7067" data-canonical-src="https://wxs.ca/research/multiscale-neural-synthesis/multiscale-gram-marble.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">使用深度学习的单一材质镜面物体的反射率和自然照明（2017）</font></font></b> <a href="https://homes.cs.washington.edu/~krematas/Publications/reflectance-natural-illumination.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/b15fe63cf0f700d7fcbebe9b17f67941943dbfc88b6375ac4a7d45ed081ac8b5/687474703a2f2f7777772e766973696f6e2e65652e6574687a2e63682f7e67656f72676f75732f696d616765732f7470616d6931375f746561736572322e706e67"><img width="50%" src="https://camo.githubusercontent.com/b15fe63cf0f700d7fcbebe9b17f67941943dbfc88b6375ac4a7d45ed081ac8b5/687474703a2f2f7777772e766973696f6e2e65652e6574687a2e63682f7e67656f72676f75732f696d616765732f7470616d6931375f746561736572322e706e67" data-canonical-src="http://www.vision.ee.ethz.ch/~georgous/images/tpami17_teaser2.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">野外照片集的联合材质和照明估计（2017）</font></font></b> <a href="https://arxiv.org/pdf/1710.08313.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/3D-Machine-Learning/blob/master/imgs/Joint%20Material%20and%20Illumination%20Estimation%20from%20Photo%20Sets%20in%20the%20Wild.jpeg"><img width="50%" src="https://github.com/timzhang642/3D-Machine-Learning/raw/master/imgs/Joint%20Material%20and%20Illumination%20Estimation%20from%20Photo%20Sets%20in%20the%20Wild.jpeg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">J相机周围有什么？</font><font style="vertical-align: inherit;">(2017) </font></font></b> <a href="https://arxiv.org/pdf/1611.09325v2.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/00b1e2f04ff8a51ee2375adc06b9d98aaef80dc4563f67734161ca7395273ac3/68747470733a2f2f686f6d65732e63732e77617368696e67746f6e2e6564752f7e6b72656d617461732f6d795f696d616765732f61727869763136625f7465617365722e6a7067"><img width="50%" src="https://camo.githubusercontent.com/00b1e2f04ff8a51ee2375adc06b9d98aaef80dc4563f67734161ca7395273ac3/68747470733a2f2f686f6d65732e63732e77617368696e67746f6e2e6564752f7e6b72656d617461732f6d795f696d616765732f61727869763136625f7465617365722e6a7067" data-canonical-src="https://homes.cs.washington.edu/~krematas/my_images/arxiv16b_teaser.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">TextureGAN：用纹理补丁控制深度图像合成（2018 CVPR）</font></font></b> <a href="https://arxiv.org/pdf/1706.02823.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/011734c46bf6e0280a6c0d178b2270275505fe9cbf902f8853aa2d4b7c2596e2/687474703a2f2f7465787475726567616e2e6579652e6761746563682e6564752f696d672f70617065725f6669677572652e706e67"><img width="50%" src="https://camo.githubusercontent.com/011734c46bf6e0280a6c0d178b2270275505fe9cbf902f8853aa2d4b7c2596e2/687474703a2f2f7465787475726567616e2e6579652e6761746563682e6564752f696d672f70617065725f6669677572652e706e67" data-canonical-src="http://texturegan.eye.gatech.edu/img/paper_figure.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">高斯材料合成（2018 SIGGRAPH）</font></font></b> <a href="https://users.cg.tuwien.ac.at/zsolnai/gfx/gaussian-material-synthesis/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/338477f3b173cf99b8c6d344ec2069e9caa3f6f28094d62e6956fda4313e9f5a/68747470733a2f2f692e7974696d672e636f6d2f76692f564d327973436e443947412f6d617872657364656661756c742e6a7067"><img width="50%" src="https://camo.githubusercontent.com/338477f3b173cf99b8c6d344ec2069e9caa3f6f28094d62e6956fda4313e9f5a/68747470733a2f2f692e7974696d672e636f6d2f76692f564d327973436e443947412f6d617872657364656661756c742e6a7067" data-canonical-src="https://i.ytimg.com/vi/VM2ysCnD9GA/maxresdefault.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">通过对抗性扩展进行非平稳纹理合成（2018 SIGGRAPH）</font></font></b> <a href="http://vcc.szu.edu.cn/research/2018/TexSyn" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/jessemelpolio/non-stationary_texture_syn/blob/master/imgs/teaser.png"><img width="50%" src="https://github.com/jessemelpolio/non-stationary_texture_syn/raw/master/imgs/teaser.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">通过图像和梯度幅度系数的多尺度空间和统计纹理属性进行综合纹理质量评估（2018 CVPR）</font></font></b> <a href="https://arxiv.org/pdf/1804.08020.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/12434910/39275366-e18c7c1c-4899-11e8-8e61-05072618bbce.PNG"><img width="50%" src="https://user-images.githubusercontent.com/12434910/39275366-e18c7c1c-4899-11e8-8e61-05072618bbce.PNG" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">LIME：实时内在材料估计（2018 CVPR）</font></font></b> <a href="https://gvv.mpi-inf.mpg.de/projects/LIME/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/2d83ea57db5c4091b97b2a0bf3b9ee787201b3233639b414a15ffd9620960ac2/68747470733a2f2f7765622e7374616e666f72642e6564752f7e7a6f6c6c686f65662f7061706572732f4356505231385f4d6174657269616c2f7465617365722e706e67"><img width="50%" src="https://camo.githubusercontent.com/2d83ea57db5c4091b97b2a0bf3b9ee787201b3233639b414a15ffd9620960ac2/68747470733a2f2f7765622e7374616e666f72642e6564752f7e7a6f6c6c686f65662f7061706572732f4356505231385f4d6174657269616c2f7465617365722e706e67" data-canonical-src="https://web.stanford.edu/~zollhoef/papers/CVPR18_Material/teaser.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">使用渲染感知深度网络捕获单图像 SVBRDF（2018）</font></font></b> <a href="https://team.inria.fr/graphdeco/fr/projects/deep-materials/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/4ee5e504eb495763c769b93e06e7938aa83b95f4d0d2fdeb00527aac2f9732b5/68747470733a2f2f7465616d2e696e7269612e66722f67726170686465636f2f66696c65732f323031382f30382f7465617365725f76302e706e67"><img width="50%" src="https://camo.githubusercontent.com/4ee5e504eb495763c769b93e06e7938aa83b95f4d0d2fdeb00527aac2f9732b5/68747470733a2f2f7465616d2e696e7269612e66722f67726170686465636f2f66696c65732f323031382f30382f7465617365725f76302e706e67" data-canonical-src="https://team.inria.fr/graphdeco/files/2018/08/teaser_v0.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">PhotoShape：用于大规模形状集合的真实感材料（2018）</font></font></b> <a href="https://keunhong.com/publications/photoshape/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/578dfbfaafd57ac94d993371faa29b4544b4e3a7512c78320dcd33f1dedecaf1/68747470733a2f2f6b65756e686f6e672e636f6d2f7075626c69636174696f6e732f70686f746f73686170652f7465617365722e6a7067"><img width="50%" src="https://camo.githubusercontent.com/578dfbfaafd57ac94d993371faa29b4544b4e3a7512c78320dcd33f1dedecaf1/68747470733a2f2f6b65756e686f6e672e636f6d2f7075626c69636174696f6e732f70686f746f73686170652f7465617365722e6a7067" data-canonical-src="https://keunhong.com/publications/photoshape/teaser.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">学习 3D 形状的材质感知局部描述符（2018）</font></font></b> <a href="http://www.vovakim.com/papers/18_3DV_ShapeMatFeat.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/3D-Machine-Learning/blob/master/imgs/Learning%20Material-Aware%20Local%20Descriptors%20for%203D%20Shapes%20(2018).jpeg"><img width="50%" src="https://github.com/timzhang642/3D-Machine-Learning/raw/master/imgs/Learning%20Material-Aware%20Local%20Descriptors%20for%203D%20Shapes%20(2018).jpeg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">FrankenGAN：使用风格同步 GAN 构建大规模模型的引导细节综合（2018 SIGGRAPH Asia）</font></font></b> <a href="http://geometry.cs.ucl.ac.uk/projects/2018/frankengan/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/1a284fd51d00444e7147b56cd694a527ab25e6b8563b68348b2f6256e14c677e/687474703a2f2f67656f6d657472792e63732e75636c2e61632e756b2f70726f6a656374732f323031382f6672616e6b656e67616e2f70617065725f646f63732f7465617365722e6a7067"><img width="50%" src="https://camo.githubusercontent.com/1a284fd51d00444e7147b56cd694a527ab25e6b8563b68348b2f6256e14c677e/687474703a2f2f67656f6d657472792e63732e75636c2e61632e756b2f70726f6a656374732f323031382f6672616e6b656e67616e2f70617065725f646f63732f7465617365722e6a7067" data-canonical-src="http://geometry.cs.ucl.ac.uk/projects/2018/frankengan/paper_docs/teaser.jpg" style="max-width: 100%;"></a></p>
<a name="user-content-style_transfer">
</a><div class="markdown-heading" dir="auto"><a><h2 tabindex="-1" class="heading-element" dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">风格学习和迁移</font></font></h2></a><a id="user-content-style-learning-and-transfer" class="anchor" aria-label="永久链接：风格学习和迁移" href="#style-learning-and-transfer"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">通过各向异性部分尺度进行风格内容分离 (2010) </font></font></b> <a href="https://www.cs.sfu.ca/~haoz/pubs/xu_siga10_style.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/85c5fb24493ff5a73400d8451c9e50878d497c066b4ea06b0b54f069a30c0c10/68747470733a2f2f73697465732e676f6f676c652e636f6d2f736974652f6b6576696e6b616978752f5f2f727372632f313437323835323132333130362f7075626c69636174696f6e732f7374796c655f622e6a70673f6865696768743d3134352677696474683d343030"><img width="50%" src="https://camo.githubusercontent.com/85c5fb24493ff5a73400d8451c9e50878d497c066b4ea06b0b54f069a30c0c10/68747470733a2f2f73697465732e676f6f676c652e636f6d2f736974652f6b6576696e6b616978752f5f2f727372632f313437323835323132333130362f7075626c69636174696f6e732f7374796c655f622e6a70673f6865696768743d3134352677696474683d343030" data-canonical-src="https://sites.google.com/site/kevinkaixu/_/rsrc/1472852123106/publications/style_b.jpg?height=145&amp;width=400" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">设计保留服装转移（2012）</font></font></b> <a href="https://hal.inria.fr/hal-00695903/file/GarmentTransfer.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/d98f4e3fe9e08b33ee55cb94508a68f13f00f84e52bcfda9dae579f3ebc1c639/68747470733a2f2f68616c2e696e7269612e66722f68616c2d303036393539303376322f66696c652f30325f576f6d616e546f416c6c2e6a7067"><img width="30%" src="https://camo.githubusercontent.com/d98f4e3fe9e08b33ee55cb94508a68f13f00f84e52bcfda9dae579f3ebc1c639/68747470733a2f2f68616c2e696e7269612e66722f68616c2d303036393539303376322f66696c652f30325f576f6d616e546f416c6c2e6a7067" data-canonical-src="https://hal.inria.fr/hal-00695903v2/file/02_WomanToAll.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">类比驱动的 3D 风格迁移 (2014) </font></font></b> <a href="http://www.chongyangma.com/publications/st/index.html" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/cafe236c5c7975d42b08eb88bcfde118b58f6ac4014af78e49bde8cbc5273127/687474703a2f2f7777772e63686f6e6779616e676d612e636f6d2f7075626c69636174696f6e732f73742f323031345f73745f7465617365722e706e67"><img width="50%" src="https://camo.githubusercontent.com/cafe236c5c7975d42b08eb88bcfde118b58f6ac4014af78e49bde8cbc5273127/687474703a2f2f7777772e63686f6e6779616e676d612e636f6d2f7075626c69636174696f6e732f73742f323031345f73745f7465617365722e706e67" data-canonical-src="http://www.chongyangma.com/publications/st/2014_st_teaser.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">风格元素：学习感知形状风格相似度（2015）</font></font></b> <a href="http://people.cs.umass.edu/~zlun/papers/StyleSimilarity/StyleSimilarity.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/happylun/StyleSimilarity"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/f76e214a270185b703b38cf3f4a1e2429797aa32b8af09651e0bff63dfc6b9cb/68747470733a2f2f70656f706c652e63732e756d6173732e6564752f7e7a6c756e2f7061706572732f5374796c6553696d696c61726974792f5374796c6553696d696c61726974795f7465617365722e6a7067"><img width="50%" src="https://camo.githubusercontent.com/f76e214a270185b703b38cf3f4a1e2429797aa32b8af09651e0bff63dfc6b9cb/68747470733a2f2f70656f706c652e63732e756d6173732e6564752f7e7a6c756e2f7061706572732f5374796c6553696d696c61726974792f5374796c6553696d696c61726974795f7465617365722e6a7067" data-canonical-src="https://people.cs.umass.edu/~zlun/papers/StyleSimilarity/StyleSimilarity_teaser.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">保留形状风格迁移的功能 (2016) </font></font></b> <a href="http://people.cs.umass.edu/~zlun/papers/StyleTransfer/StyleTransfer.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/happylun/StyleTransfer"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/7d48fc3a127a4748bcd74fee67bab3914c2f6035ab16df16e03527577d8b5065/68747470733a2f2f70656f706c652e63732e756d6173732e6564752f7e7a6c756e2f7061706572732f5374796c655472616e736665722f5374796c655472616e736665725f7465617365722e6a7067"><img width="50%" src="https://camo.githubusercontent.com/7d48fc3a127a4748bcd74fee67bab3914c2f6035ab16df16e03527577d8b5065/68747470733a2f2f70656f706c652e63732e756d6173732e6564752f7e7a6c756e2f7061706572732f5374796c655472616e736665722f5374796c655472616e736665725f7465617365722e6a7067" data-canonical-src="https://people.cs.umass.edu/~zlun/papers/StyleTransfer/StyleTransfer_teaser.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">从图像到模型集合的无监督纹理传输（2016）</font></font></b> <a href="http://ai.stanford.edu/~haosu/papers/siga16_texture_transfer_small.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/4e1f1836c3938656f5108b571d668c21be062b621fe4fa671a4a28274592c338/687474703a2f2f67656f6d657472792e63732e75636c2e61632e756b2f70726f6a656374732f323031362f746578747572655f7472616e736665722f70617065725f646f63732f7465617365722e706e67"><img width="50%" src="https://camo.githubusercontent.com/4e1f1836c3938656f5108b571d668c21be062b621fe4fa671a4a28274592c338/687474703a2f2f67656f6d657472792e63732e75636c2e61632e756b2f70726f6a656374732f323031362f746578747572655f7472616e736665722f70617065725f646f63732f7465617365722e706e67" data-canonical-src="http://geometry.cs.ucl.ac.uk/projects/2016/texture_transfer/paper_docs/teaser.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">基于几何特征的学习细节迁移（2017）</font></font></b> <a href="http://surfacedetails.cs.princeton.edu/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/2ed70fdd9f889c5bbd1511e747c6aa281af9b0e6bd9f3c2d4c0fc4c31dcea22f/687474703a2f2f7375726661636564657461696c732e63732e7072696e6365746f6e2e6564752f696d616765732f7465617365722e706e67"><img width="50%" src="https://camo.githubusercontent.com/2ed70fdd9f889c5bbd1511e747c6aa281af9b0e6bd9f3c2d4c0fc4c31dcea22f/687474703a2f2f7375726661636564657461696c732e63732e7072696e6365746f6e2e6564752f696d616765732f7465617365722e706e67" data-canonical-src="http://surfacedetails.cs.princeton.edu/images/teaser.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">在 3D 形状上共同定位样式定义元素 (2017) </font></font></b> <a href="http://people.scs.carleton.ca/~olivervankaick/pubs/style_elem.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/469393dfab0c1fbf78dea43399ad59623074bef5dac238e483e7a7fff1628e53/687474703a2f2f73323031372e73696767726170682e6f72672f73697465732f64656661756c742f66696c65732f7374796c65732f6c617267652f7075626c69632f696d616765732f6576656e74732f633131382d653130302d7075626c6963696d6167655f302d69746f6b3d794f384f6567514f2e706e67"><img width="50%" src="https://camo.githubusercontent.com/469393dfab0c1fbf78dea43399ad59623074bef5dac238e483e7a7fff1628e53/687474703a2f2f73323031372e73696767726170682e6f72672f73697465732f64656661756c742f66696c65732f7374796c65732f6c617267652f7075626c69632f696d616765732f6576656e74732f633131382d653130302d7075626c6963696d6167655f302d69746f6b3d794f384f6567514f2e706e67" data-canonical-src="http://s2017.siggraph.org/sites/default/files/styles/large/public/images/events/c118-e100-publicimage_0-itok=yO8OegQO.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">神经 3D 网格渲染器 (2017) </font></font></b> <a href="http://hiroharu-kato.com/projects_en/neural_renderer.html" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/hiroharu-kato/neural_renderer.git"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/bbdebc8e16e4bc216858b2858815b7303aa593f0def9083151f016b158cd08f9/68747470733a2f2f7062732e7477696d672e636f6d2f6d656469612f4450536d2d3448576b414170455a642e6a7067"><img width="50%" src="https://camo.githubusercontent.com/bbdebc8e16e4bc216858b2858815b7303aa593f0def9083151f016b158cd08f9/68747470733a2f2f7062732e7477696d672e636f6d2f6d656469612f4450536d2d3448576b414170455a642e6a7067" data-canonical-src="https://pbs.twimg.com/media/DPSm-4HWkAApEZd.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">通过代理到图像对齐进行外观建模（2018）</font></font></b> <a href="http://vcc.szu.edu.cn/research/2018/AppMod" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/3D-Machine-Learning/blob/master/imgs/Appearance%20Modeling%20via%20Proxy-to-Image%20Alignment.png"><img width="50%" src="https://github.com/timzhang642/3D-Machine-Learning/raw/master/imgs/Appearance%20Modeling%20via%20Proxy-to-Image%20Alignment.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">💎 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pixel2Mesh：从单个 RGB 图像生成 3D 网格模型 (2018) </font></font></b> <a href="http://bigvid.fudan.edu.cn/pixel2mesh/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/880b4f9ce804c65b4ebe40eced40f6110ed441db714b0599612cf39d60c4f2d8/68747470733a2f2f7062732e7477696d672e636f6d2f6d656469612f44614975456e6655304141716573412e6a7067"><img width="50%" src="https://camo.githubusercontent.com/880b4f9ce804c65b4ebe40eced40f6110ed441db714b0599612cf39d60c4f2d8/68747470733a2f2f7062732e7477696d672e636f6d2f6d656469612f44614975456e6655304141716573412e6a7067" data-canonical-src="https://pbs.twimg.com/media/DaIuEnfU0AAqesA.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">自动不成对形状变形传输（SIGGRAPH Asia 2018）</font></font></b> <a href="http://geometrylearning.com/ausdt/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/20c602f2242c664a765fb9f181abef59ced009c43911448b707a7f167b25e163/687474703a2f2f67656f6d657472796c6561726e696e672e636f6d2f61757364742f696d67732f7465617365722e706e67"><img width="50%" src="https://camo.githubusercontent.com/20c602f2242c664a765fb9f181abef59ced009c43911448b707a7f167b25e163/687474703a2f2f67656f6d657472796c6561726e696e672e636f6d2f61757364742f696d67732f7465617365722e706e67" data-canonical-src="http://geometrylearning.com/ausdt/imgs/teaser.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3DSNet：无监督形状到形状 3D 风格迁移 (2020) </font></font></b> <a href="https://arxiv.org/abs/2011.13388" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/ethz-asl/3dsnet"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/ethz-asl/3dsnet/blob/main/docs/chairs.jpg"><img width="50%" src="https://github.com/ethz-asl/3dsnet/raw/main/docs/chairs.jpg" style="max-width: 100%;"></a></p>
<a name="user-content-scene_synthesis">
</a><div class="markdown-heading" dir="auto"><a><h2 tabindex="-1" class="heading-element" dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">场景合成/重建</font></font></h2></a><a id="user-content-scene-synthesisreconstruction" class="anchor" aria-label="永久链接：场景合成/重建" href="#scene-synthesisreconstruction"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Make It Home：家具排列的自动优化（2011，SIGGRAPH）</font></font></b> <a href="http://people.sutd.edu.sg/~saikit/projects/furniture/index.html" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><animated-image data-catalyst="" style="width: 40%;"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/a986c343cd8af2e2c6f357dddf8bd8648f2764afc91ede5fbca8af36d5f670d0/68747470733a2f2f7777772e63732e756d622e6564752f7e637261696779752f696d672f7061706572732f6675726e69747572652e676966" data-target="animated-image.originalLink"><img src="https://camo.githubusercontent.com/a986c343cd8af2e2c6f357dddf8bd8648f2764afc91ede5fbca8af36d5f670d0/68747470733a2f2f7777772e63732e756d622e6564752f7e637261696779752f696d672f7061706572732f6675726e69747572652e676966" data-canonical-src="https://www.cs.umb.edu/~craigyu/img/papers/furniture.gif" style="max-width: 100%; display: inline-block;" data-target="animated-image.originalImage"></a>
      <span class="AnimatedImagePlayer" data-target="animated-image.player" hidden="">
        <a data-target="animated-image.replacedLink" class="AnimatedImagePlayer-images" href="https://camo.githubusercontent.com/a986c343cd8af2e2c6f357dddf8bd8648f2764afc91ede5fbca8af36d5f670d0/68747470733a2f2f7777772e63732e756d622e6564752f7e637261696779752f696d672f7061706572732f6675726e69747572652e676966" target="_blank">
          
        <span data-target="animated-image.imageContainer">
            <img data-target="animated-image.replacedImage" alt="68747470733a2f2f7777772e63732e756d622e6564752f7e637261696779752f696d672f7061706572732f6675726e69747572652e676966" class="AnimatedImagePlayer-animatedImage" src="https://camo.githubusercontent.com/a986c343cd8af2e2c6f357dddf8bd8648f2764afc91ede5fbca8af36d5f670d0/68747470733a2f2f7777772e63732e756d622e6564752f7e637261696779752f696d672f7061706572732f6675726e69747572652e676966" style="display: block; opacity: 1;">
          <canvas class="AnimatedImagePlayer-stillImage" aria-hidden="true" width="319" height="252"></canvas></span></a>
        <button data-target="animated-image.imageButton" class="AnimatedImagePlayer-images" tabindex="-1" aria-label="播放 68747470733a2f2f7777772e63732e756d622e6564752f7e637261696779752f696d672f7061706572732f6675726e69747572652e676966" hidden=""></button>
        <span class="AnimatedImagePlayer-controls" data-target="animated-image.controls" hidden="">
          <button data-target="animated-image.playButton" class="AnimatedImagePlayer-button" aria-label="播放 68747470733a2f2f7777772e63732e756d622e6564752f7e637261696779752f696d672f7061706572732f6675726e69747572652e676966">
            <svg aria-hidden="true" focusable="false" class="octicon icon-play" width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg">
              <path d="M4 13.5427V2.45734C4 1.82607 4.69692 1.4435 5.2295 1.78241L13.9394 7.32507C14.4334 7.63943 14.4334 8.36057 13.9394 8.67493L5.2295 14.2176C4.69692 14.5565 4 14.1739 4 13.5427Z">
            </path></svg>
            <svg aria-hidden="true" focusable="false" class="octicon icon-pause" width="16" height="16" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg">
              <rect x="4" y="2" width="3" height="12" rx="1"></rect>
              <rect x="9" y="2" width="3" height="12" rx="1"></rect>
            </svg>
          </button>
          <a data-target="animated-image.openButton" aria-label="在新窗口中打开 68747470733a2f2f7777772e63732e756d622e6564752f7e637261696779752f696d672f7061706572732f6675726e69747572652e676966" class="AnimatedImagePlayer-button" href="https://camo.githubusercontent.com/a986c343cd8af2e2c6f357dddf8bd8648f2764afc91ede5fbca8af36d5f670d0/68747470733a2f2f7777772e63732e756d622e6564752f7e637261696779752f696d672f7061706572732f6675726e69747572652e676966" target="_blank">
            <svg aria-hidden="true" class="octicon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" width="16" height="16">
              <path fill-rule="evenodd" d="M10.604 1h4.146a.25.25 0 01.25.25v4.146a.25.25 0 01-.427.177L13.03 4.03 9.28 7.78a.75.75 0 01-1.06-1.06l3.75-3.75-1.543-1.543A.25.25 0 0110.604 1zM3.75 2A1.75 1.75 0 002 3.75v8.5c0 .966.784 1.75 1.75 1.75h8.5A1.75 1.75 0 0014 12.25v-3.5a.75.75 0 00-1.5 0v3.5a.25.25 0 01-.25.25h-8.5a.25.25 0 01-.25-.25v-8.5a.25.25 0 01.25-.25h3.5a.75.75 0 000-1.5h-3.5z"></path>
            </svg>
          </a>
        </span>
      </span></animated-image></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">使用室内设计指南的交互式家具布局（2011）</font></font></b> <a href="http://graphics.stanford.edu/~pmerrell/furnitureLayout.htm" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/ae5afa8367d0c06cadef6ac26588508fd5ba25330a04de08003ff9e341e65a86/687474703a2f2f7669732e6265726b656c65792e6564752f7061706572732f6675726e69747572654c61796f75742f6675726e69747572654269672e6a7067"><img width="50%" src="https://camo.githubusercontent.com/ae5afa8367d0c06cadef6ac26588508fd5ba25330a04de08003ff9e341e65a86/687474703a2f2f7669732e6265726b656c65792e6564752f7061706572732f6675726e69747572654c61796f75742f6675726e69747572654269672e6a7067" data-canonical-src="http://vis.berkeley.edu/papers/furnitureLayout/furnitureBig.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">使用局部退火可逆跳跃 MCMC 合成具有约束的开放世界 (2012) </font></font></b> <a href="http://graphics.stanford.edu/~lfyg/owl.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/3D-Machine-Learning/blob/master/imgs/Synthesizing%20Open%20Worlds%20with%20Constraints%20using%20Locally%20Annealed%20Reversible%20Jump%20MCMC%20(2012).jpeg"><img width="50%" src="https://github.com/timzhang642/3D-Machine-Learning/raw/master/imgs/Synthesizing%20Open%20Worlds%20with%20Constraints%20using%20Locally%20Annealed%20Reversible%20Jump%20MCMC%20(2012).jpeg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">基于示例的 3D 对象排列合成（2012 SIGGRAPH Asia）</font></font></b> <a href="http://graphics.stanford.edu/projects/scenesynth/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/756d67c865644f4ec70cc0a57a58d77c288e40e4c4ecceea5ca9f7cb0a1838cf/687474703a2f2f67726170686963732e7374616e666f72642e6564752f70726f6a656374732f7363656e6573796e74682f696d672f7465617365722e6a7067"><img width="60%" src="https://camo.githubusercontent.com/756d67c865644f4ec70cc0a57a58d77c288e40e4c4ecceea5ca9f7cb0a1838cf/687474703a2f2f67726170686963732e7374616e666f72642e6564752f70726f6a656374732f7363656e6573796e74682f696d672f7465617365722e6a7067" data-canonical-src="http://graphics.stanford.edu/projects/scenesynth/img/teaser.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sketch2Scene：基于草图的 3D 模型联合检索和共置（2013）</font></font></b> <a href="http://sweb.cityu.edu.hk/hongbofu/projects/sketch2scene_sig13/#.WWWge__ysb0" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/a063ce5dbc16d2111a3329e3cdfd97e2f2a16d710335306f072096af9d1b8b7f/687474703a2f2f73756e7765696c756e2e6769746875622e696f2f696d616765732f70617065722f736b65746368327363656e655f7468756d622e6a7067"><img width="40%" src="https://camo.githubusercontent.com/a063ce5dbc16d2111a3329e3cdfd97e2f2a16d710335306f072096af9d1b8b7f/687474703a2f2f73756e7765696c756e2e6769746875622e696f2f696d616765732f70617065722f736b65746368327363656e655f7468756d622e6a7067" data-canonical-src="http://sunweilun.github.io/images/paper/sketch2scene_thumb.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">动作驱动的 3D 室内场景演变 (2016) </font></font></b> <a href="https://www.cs.sfu.ca/~haoz/pubs/ma_siga16_action.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/8dde484b5d1b056e984379398df94959903389403aa0ef9315640fac74796ac3/68747470733a2f2f6d6172756974782e6769746875622e696f2f70726f6a6563742f61646973652f7465617365722e6a7067"><img width="50%" src="https://camo.githubusercontent.com/8dde484b5d1b056e984379398df94959903389403aa0ef9315640fac74796ac3/68747470733a2f2f6d6172756974782e6769746875622e696f2f70726f6a6563742f61646973652f7465617365722e6a7067" data-canonical-src="https://maruitx.github.io/project/adise/teaser.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Clutterpalette：用于详细描述室内场景的交互式工具（2015）</font></font></b> <a href="https://www.cs.umb.edu/~craigyu/papers/clutterpalette.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/3D-Machine-Learning/blob/master/imgs/The%20Clutterpalette-%20An%20Interactive%20Tool%20for%20Detailing%20Indoor%20Scenes.png"><img width="50%" src="https://github.com/timzhang642/3D-Machine-Learning/raw/master/imgs/The%20Clutterpalette-%20An%20Interactive%20Tool%20for%20Detailing%20Indoor%20Scenes.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Image2Scene：转变 3D 房间的风格 (2015) </font></font></b> <a href="https://dl.acm.org/doi/abs/10.1145/2733373.2806274" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="/timzhang642/3D-Machine-Learning/blob/master/imgs/Image2Scene.jpg"><img width="60%" src="/timzhang642/3D-Machine-Learning/raw/master/imgs/Image2Scene.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">用于创建场景变化的关系模板 (2016) </font></font></b> <a href="http://geometry.cs.ucl.ac.uk/projects/2016/relationship-templates/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/d495fdc4a7656708ce1da68a4bf2fdcd0548e5cdac289cc4fec38f10499f9907/687474703a2f2f67656f6d657472792e63732e75636c2e61632e756b2f70726f6a656374732f323031362f72656c6174696f6e736869702d74656d706c617465732f70617065725f646f63732f7465617365722e706e67"><img width="50%" src="https://camo.githubusercontent.com/d495fdc4a7656708ce1da68a4bf2fdcd0548e5cdac289cc4fec38f10499f9907/687474703a2f2f67656f6d657472792e63732e75636c2e61632e756b2f70726f6a656374732f323031362f72656c6174696f6e736869702d74656d706c617465732f70617065725f646f63732f7465617365722e706e67" data-canonical-src="http://geometry.cs.ucl.ac.uk/projects/2016/relationship-templates/paper_docs/teaser.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">IM2CAD (2017) </font></font></b> <a href="http://homes.cs.washington.edu/~izadinia/im2cad.html" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/a420c58adeb285e3c950e4ef3af9e6d65cd1e1d4a9deeb33da277b5d5d1f5aa8/687474703a2f2f692e696d6775722e636f6d2f4b68744f6575422e6a7067"><img width="50%" src="https://camo.githubusercontent.com/a420c58adeb285e3c950e4ef3af9e6d65cd1e1d4a9deeb33da277b5d5d1f5aa8/687474703a2f2f692e696d6775722e636f6d2f4b68744f6575422e6a7067" data-canonical-src="http://i.imgur.com/KhtOeuB.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">预测室内场景的完整 3D 模型 (2017) </font></font></b> <a href="https://arxiv.org/pdf/1504.02437.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/3D-Machine-Learning/blob/master/imgs/Predicting%20Complete%203D%20Models%20of%20Indoor%20Scenes.png"><img width="50%" src="https://github.com/timzhang642/3D-Machine-Learning/raw/master/imgs/Predicting%20Complete%203D%20Models%20of%20Indoor%20Scenes.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">从单个 RGBD 图像完成 3D 场景解析 (2017) </font></font></b> <a href="https://arxiv.org/pdf/1710.09490.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/3D-Machine-Learning/blob/master/imgs/Complete%203D%20Scene%20Parsing%20from%20Single%20RGBD%20Image.jpeg"><img width="50%" src="https://github.com/timzhang642/3D-Machine-Learning/raw/master/imgs/Complete%203D%20Scene%20Parsing%20from%20Single%20RGBD%20Image.jpeg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">光栅到矢量：重新审视平面图转换（2017，ICCV）</font></font></b> <a href="http://www.cse.wustl.edu/~chenliu/floorplan-transformation.html" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/art-programmer/FloorplanTransformation"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/cb70387979a9678fae6e5143729b68a3f3633a578c0e826c2f6b7be271cf3769/68747470733a2f2f7777772e6373652e777573746c2e6564752f7e6368656e6c69752f666c6f6f72706c616e2d7472616e73666f726d6174696f6e2f7465617365722e706e67"><img width="50%" src="https://camo.githubusercontent.com/cb70387979a9678fae6e5143729b68a3f3633a578c0e826c2f6b7be271cf3769/68747470733a2f2f7777772e6373652e777573746c2e6564752f7e6368656e6c69752f666c6f6f72706c616e2d7472616e73666f726d6174696f6e2f7465617365722e706e67" data-canonical-src="https://www.cse.wustl.edu/~chenliu/floorplan-transformation/teaser.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">适用于 3D 多对象场景的全卷积精炼自动编码生成对抗网络（2017）</font></font></b> <a href="https://becominghuman.ai/3d-multi-object-gan-7b7cee4abf80" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[博客]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/6e32751613a73cda66233818112fc5c702e18e895163e5246549ed17a0e5a6cb/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f313630302f312a4e636b573268666762486845503350385a355a4c6a512e706e67"><img width="50%" src="https://camo.githubusercontent.com/6e32751613a73cda66233818112fc5c702e18e895163e5246549ed17a0e5a6cb/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f313630302f312a4e636b573268666762486845503350385a355a4c6a512e706e67" data-canonical-src="https://cdn-images-1.medium.com/max/1600/1*NckW2hfgbHhEP3P8Z5ZLjQ.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">通过活动相关对象关系图自适应合成室内场景（2017 SIGGRAPH Asia）</font></font></b> <a href="http://arts.buaa.edu.cn/projects/sa17/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/1f0f0d7f4d8896d7ba2e1a1d00d24d552a41fff0ee8cc2d3b2e1149f5dc00f32/68747470733a2f2f7361323031372e73696767726170682e6f72672f696d616765732f6576656e74732f633132312d6534352d7075626c6963696d6167652e6a7067"><img width="50%" src="https://camo.githubusercontent.com/1f0f0d7f4d8896d7ba2e1a1d00d24d552a41fff0ee8cc2d3b2e1149f5dc00f32/68747470733a2f2f7361323031372e73696767726170682e6f72672f696d616765732f6576656e74732f633132312d6534352d7075626c6963696d6167652e6a7067" data-canonical-src="https://sa2017.siggraph.org/images/events/c121-e45-publicimage.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">使用遗传算法的自动化室内设计（2017）</font></font></b> <a href="https://publik.tuwien.ac.at/files/publik_262718.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/6ef50a6254072ab97b247443dd557415044141d92083d9bb382746f9f23ad009/687474703a2f2f7777772e70657465726b616e2e636f6d2f70696374757265732f746561736572712e6a7067"><img width="50%" src="https://camo.githubusercontent.com/6ef50a6254072ab97b247443dd557415044141d92083d9bb382746f9f23ad009/687474703a2f2f7777772e70657465726b616e2e636f6d2f70696374757265732f746561736572712e6a7067" data-canonical-src="http://www.peterkan.com/pictures/teaserq.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SceneSuggest：上下文驱动的 3D 场景设计 (2017) </font></font></b> <a href="https://arxiv.org/pdf/1703.00061.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/3D-Machine-Learning/blob/master/imgs/SceneSuggest%20-Context-driven%203D%20Scene%20Design%20(2017).png"><img width="50%" src="https://github.com/timzhang642/3D-Machine-Learning/raw/master/imgs/SceneSuggest%20-Context-driven%203D%20Scene%20Design%20(2017).png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">用于实时同步 3D 重建和材料识别的完全端到端深度学习方法（2017）</font></font></b> <a href="https://arxiv.org/pdf/1703.04699v1.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/3D-Machine-Learning/blob/master/imgs/A%20fully%20end-to-end%20deep%20learning%20approach%20for%20real-time%20simultaneous%203D%20reconstruction%20and%20material%20recognition%20(2017).png"><img width="50%" src="https://github.com/timzhang642/3D-Machine-Learning/raw/master/imgs/A%20fully%20end-to-end%20deep%20learning%20approach%20for%20real-time%20simultaneous%203D%20reconstruction%20and%20material%20recognition%20(2017).png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">使用随机语法的以人为中心的室内场景合成（2018，CVPR）</font></font></b><a href="http://web.cs.ucla.edu/~syqi/publications/cvpr2018synthesis/cvpr2018synthesis.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="http://web.cs.ucla.edu/~syqi/publications/cvpr2018synthesis/cvpr2018synthesis_supplementary.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[补充] </font></font></a> <a href="https://github.com/SiyuanQi/human-centric-scene-synthesis"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><animated-image data-catalyst="" style="width: 50%;"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/31eb89a15a2684ece0bf5ea745afd00422fe8fb52ec5dd002cd2857890420742/687474703a2f2f7765622e63732e75636c612e6564752f7e737971692f7075626c69636174696f6e732f7468756d626e61696c732f637670723230313873796e7468657369732e676966" data-target="animated-image.originalLink" hidden=""><img src="https://camo.githubusercontent.com/31eb89a15a2684ece0bf5ea745afd00422fe8fb52ec5dd002cd2857890420742/687474703a2f2f7765622e63732e75636c612e6564752f7e737971692f7075626c69636174696f6e732f7468756d626e61696c732f637670723230313873796e7468657369732e676966" data-canonical-src="http://web.cs.ucla.edu/~syqi/publications/thumbnails/cvpr2018synthesis.gif" style="max-width: 100%;" data-target="animated-image.originalImage" hidden=""></a>
      <span class="AnimatedImagePlayer" data-target="animated-image.player">
        <a data-target="animated-image.replacedLink" class="AnimatedImagePlayer-images" href="https://camo.githubusercontent.com/31eb89a15a2684ece0bf5ea745afd00422fe8fb52ec5dd002cd2857890420742/687474703a2f2f7765622e63732e75636c612e6564752f7e737971692f7075626c69636174696f6e732f7468756d626e61696c732f637670723230313873796e7468657369732e676966" target="_blank">
          <span data-target="animated-image.imageContainer">
            <img data-target="animated-image.replacedImage" alt="" class="AnimatedImagePlayer-animatedImage" src="https://camo.githubusercontent.com/31eb89a15a2684ece0bf5ea745afd00422fe8fb52ec5dd002cd2857890420742/687474703a2f2f7765622e63732e75636c612e6564752f7e737971692f7075626c69636174696f6e732f7468756d626e61696c732f637670723230313873796e7468657369732e676966">
          </span>
        </a>
        <button data-target="animated-image.imageButton" class="AnimatedImagePlayer-images" tabindex="-1"></button>
        <span class="AnimatedImagePlayer-controls" data-target="animated-image.controls">
          <button data-target="animated-image.playButton" class="AnimatedImagePlayer-button">
            <svg aria-hidden="true" focusable="false" class="octicon icon-play" width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg">
              <path d="M4 13.5427V2.45734C4 1.82607 4.69692 1.4435 5.2295 1.78241L13.9394 7.32507C14.4334 7.63943 14.4334 8.36057 13.9394 8.67493L5.2295 14.2176C4.69692 14.5565 4 14.1739 4 13.5427Z">
            </path></svg>
            <svg aria-hidden="true" focusable="false" class="octicon icon-pause" width="16" height="16" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg">
              <rect x="4" y="2" width="3" height="12" rx="1"></rect>
              <rect x="9" y="2" width="3" height="12" rx="1"></rect>
            </svg>
          </button>
          <a data-target="animated-image.openButton" aria-label="在新窗口中打开" class="AnimatedImagePlayer-button" href="https://camo.githubusercontent.com/31eb89a15a2684ece0bf5ea745afd00422fe8fb52ec5dd002cd2857890420742/687474703a2f2f7765622e63732e75636c612e6564752f7e737971692f7075626c69636174696f6e732f7468756d626e61696c732f637670723230313873796e7468657369732e676966" target="_blank">
            <svg aria-hidden="true" class="octicon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" width="16" height="16">
              <path fill-rule="evenodd" d="M10.604 1h4.146a.25.25 0 01.25.25v4.146a.25.25 0 01-.427.177L13.03 4.03 9.28 7.78a.75.75 0 01-1.06-1.06l3.75-3.75-1.543-1.543A.25.25 0 0110.604 1zM3.75 2A1.75 1.75 0 002 3.75v8.5c0 .966.784 1.75 1.75 1.75h8.5A1.75 1.75 0 0014 12.25v-3.5a.75.75 0 00-1.5 0v3.5a.25.25 0 01-.25.25h-8.5a.25.25 0 01-.25-.25v-8.5a.25.25 0 01.25-.25h3.5a.75.75 0 000-1.5h-3.5z"></path>
            </svg>
          </a>
        </span>
      </span></animated-image></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">📷🎲 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">FloorNet：从 3D 扫描重建平面图的统一框架（2018）</font></font></b> <a href="https://arxiv.org/pdf/1804.00090.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="http://art-programmer.github.io/floornet.html" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/e918ac16654468abecde912d4fd6a7179b505ea69fa8bb31c417d5922968e704/687474703a2f2f6172742d70726f6772616d6d65722e6769746875622e696f2f666c6f6f726e65742f7465617365722e706e67"><img width="50%" src="https://camo.githubusercontent.com/e918ac16654468abecde912d4fd6a7179b505ea69fa8bb31c417d5922968e704/687474703a2f2f6172742d70726f6772616d6d65722e6769746875622e696f2f666c6f6f726e65742f7465617365722e706e67" data-canonical-src="http://art-programmer.github.io/floornet/teaser.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">👾 </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ScanComplete：3D 扫描的大规模场景补全和语义分割（2018）</font></font></b> <a href="https://arxiv.org/pdf/1712.10215.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/bcfeab64b230d031e41df2911607e8fb2f763273fbb7c72428eb5aa0d4054420/68747470733a2f2f6e696573736e65726c61622e6f72672f7061706572732f323031382f337363616e636f6d706c6574652f7465617365722e6a7067"><img width="50%" src="https://camo.githubusercontent.com/bcfeab64b230d031e41df2911607e8fb2f763273fbb7c72428eb5aa0d4054420/68747470733a2f2f6e696573736e65726c61622e6f72672f7061706572732f323031382f337363616e636f6d706c6574652f7465617365722e6a7067" data-canonical-src="https://niessnerlab.org/papers/2018/3scancomplete/teaser.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">用于室内场景合成的深度卷积先验（2018）</font></font></b> <a href="https://kwang-ether.github.io/pdf/deepsynth.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/88fe92f4ffa0815f01c20fec2527ab882df78ac778b3e7edc90535f1d495014c/687474703a2f2f6d73617676612e6769746875622e696f2f66696c65732f6465657073796e74682e706e67"><img width="50%" src="https://camo.githubusercontent.com/88fe92f4ffa0815f01c20fec2527ab882df78ac778b3e7edc90535f1d495014c/687474703a2f2f6d73617676612e6769746875622e696f2f66696c65732f6465657073796e74682e706e67" data-canonical-src="http://msavva.github.io/files/deepsynth.png" style="max-width: 100%;"></a></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">📷</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">通过深度卷积生成模型快速灵活的室内场景合成（2018）</font></font></b> <a href="https://arxiv.org/pdf/1811.12463.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/brownvc/fast-synth"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="/timzhang642/3D-Machine-Learning/blob/master/imgs/Fast%20and%20Flexible%20Indoor%20scene%20synthesis%20via%20Deep%20Convolutional%20Generative%20Models.jpg"><img width="80%" src="/timzhang642/3D-Machine-Learning/raw/master/imgs/Fast%20and%20Flexible%20Indoor%20scene%20synthesis%20via%20Deep%20Convolutional%20Generative%20Models.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">使用随机语法的每像素地面实况的可配置 3D 场景合成和 2D 图像渲染（2018）</font></font></b> <a href="https://arxiv.org/pdf/1704.00112.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/366af0b58267dc2df19a21869caf3491ceb6d3268a5c708fcbc98742b7ec1354/68747470733a2f2f6d656469612e737072696e6765726e61747572652e636f6d2f6f726967696e616c2f737072696e6765722d7374617469632f696d6167652f61727425334131302e313030372532467331313236332d3031382d313130332d352f4d656469614f626a656374732f31313236335f323031385f313130335f466967355f48544d4c2e6a7067"><img width="50%" src="https://camo.githubusercontent.com/366af0b58267dc2df19a21869caf3491ceb6d3268a5c708fcbc98742b7ec1354/68747470733a2f2f6d656469612e737072696e6765726e61747572652e636f6d2f6f726967696e616c2f737072696e6765722d7374617469632f696d6167652f61727425334131302e313030372532467331313236332d3031382d313130332d352f4d656469614f626a656374732f31313236335f323031385f313130335f466967355f48544d4c2e6a7067" data-canonical-src="https://media.springernature.com/original/springer-static/image/art%3A10.1007%2Fs11263-018-1103-5/MediaObjects/11263_2018_1103_Fig5_HTML.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">从单个 RGB 图像解析和重建整体 3D 场景 (ECCV 2018) </font></font></b> <a href="http://siyuanhuang.com/holistic_parsing/main.html" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/a2efbe8a0a570ebb9328cbe0775b6c8da44445e3cbb2625db1bbb9b5cbc18645/687474703a2f2f7765622e63732e75636c612e6564752f7e737971692f7075626c69636174696f6e732f7468756d626e61696c732f65636376323031387363656e652e706e67"><img width="50%" src="https://camo.githubusercontent.com/a2efbe8a0a570ebb9328cbe0775b6c8da44445e3cbb2625db1bbb9b5cbc18645/687474703a2f2f7765622e63732e75636c612e6564752f7e737971692f7075626c69636174696f6e732f7468756d626e61696c732f65636376323031387363656e652e706e67" data-canonical-src="http://web.cs.ucla.edu/~syqi/publications/thumbnails/eccv2018scene.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">来自场景数据库的 3D 场景的语言驱动合成（SIGGRAPH Asia 2018）</font></font></b> <a href="http://www.sfu.ca/~agadipat/publications/2018/T2S/project_page.html" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/75cbd73ca907d677ebe8a868f075c436fa91b1456dc9ab19948a06b2fcbca87b/687474703a2f2f7777772e7366752e63612f7e61676164697061742f7075626c69636174696f6e732f323031382f5432532f7465617365722e706e67"><img width="50%" src="https://camo.githubusercontent.com/75cbd73ca907d677ebe8a868f075c436fa91b1456dc9ab19948a06b2fcbca87b/687474703a2f2f7777772e7366752e63612f7e61676164697061742f7075626c69636174696f6e732f323031382f5432532f7465617365722e706e67" data-canonical-src="http://www.sfu.ca/~agadipat/publications/2018/T2S/teaser.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">通过混合表示进行场景合成的深度生成建模（2018）</font></font></b> <a href="https://arxiv.org/pdf/1808.02084.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/3D-Machine-Learning/blob/master/imgs/Deep%20Generative%20Modeling%20for%20Scene%20Synthesis%20via%20Hybrid%20Representations%20(2018).jpeg"><img width="50%" src="https://github.com/timzhang642/3D-Machine-Learning/raw/master/imgs/Deep%20Generative%20Modeling%20for%20Scene%20Synthesis%20via%20Hybrid%20Representations%20(2018).jpeg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">GRAINS：室内场景的生成递归自动编码器（2018）</font></font></b> <a href="https://arxiv.org/pdf/1807.09193.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/b4648970b79aebe60baa430070212f0cff52fd0878159c5f432b51b73ea4566a/68747470733a2f2f7777772e67726f756e6461692e636f6d2f6d656469612f61727869765f70726f6a656374732f3337333530332f6e65775f706963732f7465617365726669672e6a70672e37353078305f7137355f63726f702e6a7067"><img width="50%" src="https://camo.githubusercontent.com/b4648970b79aebe60baa430070212f0cff52fd0878159c5f432b51b73ea4566a/68747470733a2f2f7777772e67726f756e6461692e636f6d2f6d656469612f61727869765f70726f6a656374732f3337333530332f6e65775f706963732f7465617365726669672e6a70672e37353078305f7137355f63726f702e6a7067" data-canonical-src="https://www.groundai.com/media/arxiv_projects/373503/new_pics/teaserfig.jpg.750x0_q75_crop.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SEETHROUGH：在严重遮挡的室内场景图像中查找对象（2018）</font></font></b> <a href="http://www.vovakim.com/papers/18_3DVOral_SeeThrough.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/2052a02f14fde462bff753813fa67986b1b88240c39392bb61a2f75f0f0a75f8/687474703a2f2f67656f6d657472792e63732e75636c2e61632e756b2f70726f6a656374732f323031382f7365657468726f7567682f70617065725f646f63732f726573756c745f706c6174652e706e67"><img width="50%" src="https://camo.githubusercontent.com/2052a02f14fde462bff753813fa67986b1b88240c39392bb61a2f75f0f0a75f8/687474703a2f2f67656f6d657472792e63732e75636c2e61632e756b2f70726f6a656374732f323031382f7365657468726f7567682f70617065725f646f63732f726573756c745f706c6174652e706e67" data-canonical-src="http://geometry.cs.ucl.ac.uk/projects/2018/seethrough/paper_docs/result_plate.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">👾 Scan2CAD：学习 RGB-D 扫描中的 CAD 模型对齐（CVPR 2019）</font></font></b> <a href="https://arxiv.org/pdf/1811.11187.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/skanti/Scan2CAD"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/6d305eff9e6717654d2ee1ab7106c0ce8ba8780a481d78e13ce97570be2622ad/687474703a2f2f7777772e6e696573736e65726c61622e6f72672f7061706572732f323031392f357363616e326361642f7465617365722e6a7067"><img width="50%" src="https://camo.githubusercontent.com/6d305eff9e6717654d2ee1ab7106c0ce8ba8780a481d78e13ce97570be2622ad/687474703a2f2f7777772e6e696573736e65726c61622e6f72672f7061706572732f323031392f357363616e326361642f7465617365722e6a7067" data-canonical-src="http://www.niessnerlab.org/papers/2019/5scan2cad/teaser.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">💎 Scan2Mesh：从非结构化范围扫描到 3D 网格（CVPR 2019）</font></font></b> <a href="https://arxiv.org/pdf/1811.10464.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/57fc7ab064b48d2515b0e3a98da448ce9804cdf7f7459dd92263191580257408/687474703a2f2f7777772e6e696573736e65726c61622e6f72672f7061706572732f323031392f347363616e326d6573682f7465617365722e6a7067"><img width="50%" src="https://camo.githubusercontent.com/57fc7ab064b48d2515b0e3a98da448ce9804cdf7f7459dd92263191580257408/687474703a2f2f7777772e6e696573736e65726c61622e6f72672f7061706572732f323031392f347363616e326d6573682f7465617365722e6a7067" data-canonical-src="http://www.niessnerlab.org/papers/2019/4scan2mesh/teaser.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">👾 3D-SIC：RGB-D 扫描的 3D 语义实例完成（arXiv 2019）</font></font></b> <a href="https://arxiv.org/pdf/1904.12012.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/9637ca67df3b6d08eb21fde7fc874637afad48c05dbafdc1524e6a10d9ef304c/687474703a2f2f7777772e6e696573736e65726c61622e6f72672f7061706572732f323031392f7a317369632f7465617365722e6a7067"><img width="50%" src="https://camo.githubusercontent.com/9637ca67df3b6d08eb21fde7fc874637afad48c05dbafdc1524e6a10d9ef304c/687474703a2f2f7777772e6e696573736e65726c61622e6f72672f7061706572732f323031392f7a317369632f7465617365722e6a7067" data-canonical-src="http://www.niessnerlab.org/papers/2019/z1sic/teaser.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">👾 3D 扫描中的端到端 CAD 模型检索和 9DoF 对齐（arXiv 2019）</font></font></b> <a href="https://arxiv.org/abs/1906.04201" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/b800219ffbb069b7cc06e5888e757ca9fb249f6aa6e8c59451cf83ec084af842/687474703a2f2f7777772e6e696573736e65726c61622e6f72672f7061706572732f323031392f7a32656e6432656e642f7465617365722e6a7067"><img width="50%" src="https://camo.githubusercontent.com/b800219ffbb069b7cc06e5888e757ca9fb249f6aa6e8c59451cf83ec084af842/687474703a2f2f7777772e6e696573736e65726c61622e6f72672f7061706572732f323031392f7a32656e6432656e642f7465617365722e6a7067" data-canonical-src="http://www.niessnerlab.org/papers/2019/z2end2end/teaser.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3D 室内场景合成综述 (2020) </font></font></b> <a href="https://www.researchgate.net/profile/Shao_Kui_Zhang/publication/333135099_A_Survey_of_3D_Indoor_Scene_Synthesis/links/5ce13a5492851c4eabad4de0/A-Survey-of-3D-Indoor-Scene-Synthesis.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/julyrashchenko/3D-Machine-Learning/blob/master/imgs/A%20Survey%20of%203D%20Indoor%20Scene%20Synthesis.jpg"><img width="60%" src="https://github.com/julyrashchenko/3D-Machine-Learning/raw/master/imgs/A%20Survey%20of%203D%20Indoor%20Scene%20Synthesis.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">💊 📷 PlanIT：使用关系图和空间先验网络规划和实例化室内场景（2019）</font></font></b> <a href="https://kwang-ether.github.io/pdf/planit.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/brownvc/planit"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="/timzhang642/3D-Machine-Learning/blob/master/imgs/PlanIT.jpg"><img src="/timzhang642/3D-Machine-Learning/raw/master/imgs/PlanIT.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">👾 特征度量配准：一种无需对应的鲁棒点云配准的快速半监督方法（CVPR 2020）</font></font></b> <a href="https://arxiv.org/abs/2005.01014" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a><a href="https://github.com/XiaoshuiHuang/fmr"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/XiaoshuiHuang/xiaoshuihuang.github.io/blob/master/research/2020-feature-metric.png?raw=true"><img width="50%" src="https://github.com/XiaoshuiHuang/xiaoshuihuang.github.io/raw/master/research/2020-feature-metric.png?raw=true" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">💊 以人为中心的室内场景评估和合成指标（2020）</font></font></b> <a href="/timzhang642/3D-Machine-Learning/blob/master/sciencedirect.com/science/article/abs/pii/S1524070320300175"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="/timzhang642/3D-Machine-Learning/blob/master/imgs/Human-centric%20metrics%20for%20indoor%20scene%20assessment%20and%20synthesis.jpg"><img width="60%" src="/timzhang642/3D-Machine-Learning/raw/master/imgs/Human-centric%20metrics%20for%20indoor%20scene%20assessment%20and%20synthesis.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SceneCAD：预测 RGB-D 扫描中的对象对齐和布局（2020）</font></font></b> <a href="https://arxiv.org/pdf/2003.12622.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="/timzhang642/3D-Machine-Learning/blob/master/imgs/SceneCAD.jpg"><img width="60%" src="/timzhang642/3D-Machine-Learning/raw/master/imgs/SceneCAD.jpg" style="max-width: 100%;"></a></p>
<a name="user-content-scene_understanding">
</a><div class="markdown-heading" dir="auto"><h2 tabindex="-1" class="heading-element" dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">场景理解（另一个更详细的</font></font><a href="https://github.com/bertjiazheng/awesome-scene-understanding"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">存储库</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">）</font></font></h2><a id="user-content-scene-understanding-another-more-detailed-repository" class="anchor" aria-label="永久链接：场景理解（另一个更详细的存储库）" href="#scene-understanding-another-more-detailed-repository"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">恢复杂乱房间的空间布局（2009）</font></font></b> <a href="http://dhoiem.cs.illinois.edu/publications/iccv2009_hedau_indoor.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/3D-Machine-Learning/blob/master/imgs/Recovering%20the%20Spatial%20Layout%20of%20Cluttered%20Rooms.png"><img width="60%" src="https://github.com/timzhang642/3D-Machine-Learning/raw/master/imgs/Recovering%20the%20Spatial%20Layout%20of%20Cluttered%20Rooms.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">使用图内核表征场景中的结构关系 (2011 SIGGRAPH) </font></font></b> <a href="https://graphics.stanford.edu/~mdfisher/graphKernel.html" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/f56e7971a0d69889a3a27bf3fd20cb757106604b770406d3dfeef82e8c430cfb/68747470733a2f2f67726170686963732e7374616e666f72642e6564752f7e6d646669736865722f7061706572732f67726170684b65726e656c5465617365722e706e67"><img width="60%" src="https://camo.githubusercontent.com/f56e7971a0d69889a3a27bf3fd20cb757106604b770406d3dfeef82e8c430cfb/68747470733a2f2f67726170686963732e7374616e666f72642e6564752f7e6d646669736865722f7061706572732f67726170684b65726e656c5465617365722e706e67" data-canonical-src="https://graphics.stanford.edu/~mdfisher/papers/graphKernelTeaser.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">使用 3D 几何短语了解室内场景 (2013) </font></font></b> <a href="http://cvgl.stanford.edu/projects/3dgp/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/bd4fb2fb50fb5fa3e9d2812725701cd342d7397e3f379fbddf7764315776f8bc/687474703a2f2f6376676c2e7374616e666f72642e6564752f70726f6a656374732f336467702f696d616765732f7469746c652e706e67"><img width="30%" src="https://camo.githubusercontent.com/bd4fb2fb50fb5fa3e9d2812725701cd342d7397e3f379fbddf7764315776f8bc/687474703a2f2f6376676c2e7374616e666f72642e6564752f70726f6a656374732f336467702f696d616765732f7469746c652e706e67" data-canonical-src="http://cvgl.stanford.edu/projects/3dgp/images/title.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">通过上下文焦点组织异构场景集合（2014 SIGGRAPH）</font></font></b> <a href="http://kevinkaixu.net/projects/focal.html" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/cf9ed269c94078fcd7e6b6283fdbc8ce068ae9bc08ec21992340fdf2a1c7cfff/687474703a2f2f6b6576696e6b616978752e6e65742f70726f6a656374732f666f63616c2f6f7665726c617070696e675f636c7573746572732e6a7067"><img width="60%" src="https://camo.githubusercontent.com/cf9ed269c94078fcd7e6b6283fdbc8ce068ae9bc08ec21992340fdf2a1c7cfff/687474703a2f2f6b6576696e6b616978752e6e65742f70726f6a656374732f666f63616c2f6f7665726c617070696e675f636c7573746572732e6a7067" data-canonical-src="http://kevinkaixu.net/projects/focal/overlapping_clusters.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SceneGrok：在 3D 环境中推断动作地图（2014 年，SIGGRAPH）</font></font></b> <a href="http://graphics.stanford.edu/projects/scenegrok/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/78316bfbe9ae99774935abb4cbd11da8d019aa51c273fb191b7b2855becf7e8b/687474703a2f2f67726170686963732e7374616e666f72642e6564752f70726f6a656374732f7363656e6567726f6b2f7363656e6567726f6b2e706e67"><img width="50%" src="https://camo.githubusercontent.com/78316bfbe9ae99774935abb4cbd11da8d019aa51c273fb191b7b2855becf7e8b/687474703a2f2f67726170686963732e7374616e666f72642e6564752f70726f6a656374732f7363656e6567726f6b2f7363656e6567726f6b2e706e67" data-canonical-src="http://graphics.stanford.edu/projects/scenegrok/scenegrok.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">PanoContext：用于全景场景理解的全房间 3D 上下文模型 (2014) </font></font></b> <a href="http://panocontext.cs.princeton.edu/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/5f22779bb938a5f344f9d7702580b128851642ff73c08f349fe8baad7c40edfe/687474703a2f2f70616e6f636f6e746578742e63732e7072696e6365746f6e2e6564752f7465617365722e6a7067"><img width="50%" src="https://camo.githubusercontent.com/5f22779bb938a5f344f9d7702580b128851642ff73c08f349fe8baad7c40edfe/687474703a2f2f70616e6f636f6e746578742e63732e7072696e6365746f6e2e6564752f7465617365722e6a7067" data-canonical-src="http://panocontext.cs.princeton.edu/teaser.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">学习用于室内场景布局预测的信息丰富的边缘图（2015）</font></font></b> <a href="http://slazebni.cs.illinois.edu/publications/iccv15_informative.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/3D-Machine-Learning/blob/master/imgs/Learning%20Informative%20Edge%20Maps%20for%20Indoor%20Scene%20Layout%20Prediction.png"><img width="50%" src="https://github.com/timzhang642/3D-Machine-Learning/raw/master/imgs/Learning%20Informative%20Edge%20Maps%20for%20Indoor%20Scene%20Layout%20Prediction.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Rent3D：单目布局估计的平面图先验（2015）</font></font></b> <a href="http://www.cs.toronto.edu/~fidler/projects/rent3D.html" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/a045713786e38b77e5ed8782f3310d76b2f54442b12b208cb430e9bd15c9e44c/687474703a2f2f7777772e63732e746f726f6e746f2e6564752f7e6669646c65722f70726f6a656374732f6c61796f75742d7265732e6a7067"><img width="50%" src="https://camo.githubusercontent.com/a045713786e38b77e5ed8782f3310d76b2f54442b12b208cb430e9bd15c9e44c/687474703a2f2f7777772e63732e746f726f6e746f2e6564752f7e6669646c65722f70726f6a656374732f6c61796f75742d7265732e6a7067" data-canonical-src="http://www.cs.toronto.edu/~fidler/projects/layout-res.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">一种从粗到细的室内布局估计（CFILE）方法（2016）</font></font></b> <a href="https://pdfs.semanticscholar.org/7024/a92186b81e6133dc779f497d06877b48d82b.pdf?_ga=2.54181869.497995160.1510977308-665742395.1510465328" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/3D-Machine-Learning/blob/master/imgs/A%20Coarse-to-Fine%20Indoor%20Layout%20Estimation%20(CFILE)%20Method%20(2016).png"><img width="50%" src="https://github.com/timzhang642/3D-Machine-Learning/raw/master/imgs/A%20Coarse-to-Fine%20Indoor%20Layout%20Estimation%20(CFILE)%20Method%20(2016).png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">DeLay：杂乱室内场景的鲁棒空间布局估计（2016）</font></font></b> <a href="http://deeplayout.stanford.edu/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/3D-Machine-Learning/blob/master/imgs/DeLay-Robust%20Spatial%20Layout%20Estimation%20for%20Cluttered%20Indoor%20Scenes.png"><img width="30%" src="https://github.com/timzhang642/3D-Machine-Learning/raw/master/imgs/DeLay-Robust%20Spatial%20Layout%20Estimation%20for%20Cluttered%20Indoor%20Scenes.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">大规模室内空间的3D语义解析（2016）</font></font></b> <a href="http://buildingparser.stanford.edu/method.html" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/alexsax/2D-3D-Semantics"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/b2af6b7e4d2b2251bfdf77acc13ed560626b6f29c6fea297e88c65803cae9fcd/687474703a2f2f6275696c64696e677061727365722e7374616e666f72642e6564752f696d616765732f7465617365722e706e67"><img width="50%" src="https://camo.githubusercontent.com/b2af6b7e4d2b2251bfdf77acc13ed560626b6f29c6fea297e88c65803cae9fcd/687474703a2f2f6275696c64696e677061727365722e7374616e666f72642e6564752f696d616765732f7465617365722e706e67" data-canonical-src="http://buildingparser.stanford.edu/images/teaser.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">单图像 3D 解释器网络 (2016) </font></font></b> <a href="http://3dinterpreter.csail.mit.edu/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/jiajunwu/3dinn"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/16a85ad12c30ae686479dd38a006075349f28b2b31e2736df09f2842f0f29904/687474703a2f2f3364696e7465727072657465722e637361696c2e6d69742e6564752f696d616765732f73706f746c696768745f3364696e6e5f6c617267652e6a7067"><img width="50%" src="https://camo.githubusercontent.com/16a85ad12c30ae686479dd38a006075349f28b2b31e2736df09f2842f0f29904/687474703a2f2f3364696e7465727072657465722e637361696c2e6d69742e6564752f696d616765732f73706f746c696768745f3364696e6e5f6c617267652e6a7067" data-canonical-src="http://3dinterpreter.csail.mit.edu/images/spotlight_3dinn_large.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">深度多模态图像对应学习（2016）</font></font></b> <a href="http://www.cse.wustl.edu/~chenliu/floorplan-matching.html" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/57877776209fdfaaeed913ba16c49695685df5e6ae689289686c80a9b068271d/687474703a2f2f6172742d70726f6772616d6d65722e6769746875622e696f2f666c6f6f72706c616e2d6d61746368696e672f7465617365722e706e67"><img width="50%" src="https://camo.githubusercontent.com/57877776209fdfaaeed913ba16c49695685df5e6ae689289686c80a9b068271d/687474703a2f2f6172742d70726f6772616d6d65722e6769746875622e696f2f666c6f6f72706c616e2d6d61746368696e672f7465617365722e706e67" data-canonical-src="http://art-programmer.github.io/floorplan-matching/teaser.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">使用卷积神经网络进行室内场景理解的基于物理的渲染 (2017) </font></font></b> <a href="http://3dvision.princeton.edu/projects/2016/PBRS/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/yindaz/pbrs"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码] </font></font></a> <a href="https://github.com/yindaz/surface_normal"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码] </font></font></a> <a href="https://github.com/fyu/dilation"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码] </font></font></a> <a href="https://github.com/s9xie/hed"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/d73166660e7d7b082412db2a0e7893961e3ce549c3f4ffc72eae251d735c7e3a/68747470733a2f2f7062732e7477696d672e636f6d2f6d656469612f43305945524a4f584541413639784e2e6a7067"><img width="50%" src="https://camo.githubusercontent.com/d73166660e7d7b082412db2a0e7893961e3ce549c3f4ffc72eae251d735c7e3a/68747470733a2f2f7062732e7477696d672e636f6d2f6d656469612f43305945524a4f584541413639784e2e6a7067" data-canonical-src="https://pbs.twimg.com/media/C0YERJOXEAA69xN.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">RoomNet：端到端房间布局估计（2017）</font></font></b> <a href="https://arxiv.org/pdf/1703.06241.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/20794825ed8282e3f255bdacfdca6f1a40dc428fc1f586010c65e8bb80e28877/68747470733a2f2f7062732e7477696d672e636f6d2f6d656469612f43375a3239477356304141534576522e6a7067"><img width="50%" src="https://camo.githubusercontent.com/20794825ed8282e3f255bdacfdca6f1a40dc428fc1f586010c65e8bb80e28877/68747470733a2f2f7062732e7477696d672e636f6d2f6d656469612f43375a3239477356304141534576522e6a7067" data-canonical-src="https://pbs.twimg.com/media/C7Z29GsV0AASEvR.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SUN RGB-D：RGB-D 场景理解基准套件（2017）</font></font></b> <a href="http://rgbd.cs.princeton.edu/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/f122922f0dd2d5019a89bf04c9689c28e41399e77543d982968951e0bad5a4d1/687474703a2f2f726762642e63732e7072696e6365746f6e2e6564752f7465617365722e6a7067"><img width="50%" src="https://camo.githubusercontent.com/f122922f0dd2d5019a89bf04c9689c28e41399e77543d982968951e0bad5a4d1/687474703a2f2f726762642e63732e7072696e6365746f6e2e6564752f7465617365722e6a7067" data-canonical-src="http://rgbd.cs.princeton.edu/teaser.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">单深度图像的语义场景完成 (2017) </font></font></b> <a href="http://sscnet.cs.princeton.edu/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/shurans/sscnet"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/90a5f813a10efe05bcda5795290c3b376c36e22fe981f65250d3870c24e370df/687474703a2f2f7373636e65742e63732e7072696e6365746f6e2e6564752f7465617365722e6a7067"><img width="50%" src="https://camo.githubusercontent.com/90a5f813a10efe05bcda5795290c3b376c36e22fe981f65250d3870c24e370df/687474703a2f2f7373636e65742e63732e7072696e6365746f6e2e6564752f7465617365722e6a7067" data-canonical-src="http://sscnet.cs.princeton.edu/teaser.jpg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">从 3D 场景的 2D 图像中分解形状、姿势和布局 (2018 CVPR) </font></font></b> <a href="https://arxiv.org/pdf/1712.01812.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://shubhtuls.github.io/factored3d/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/f56fe5f699ad53d82dc8b5883c05fc65faeb1f388d4caef08a85c17001bf602d/68747470733a2f2f736875626874756c732e6769746875622e696f2f666163746f72656433642f7265736f75726365732f696d616765732f7465617365722e706e67"><img width="50%" src="https://camo.githubusercontent.com/f56fe5f699ad53d82dc8b5883c05fc65faeb1f388d4caef08a85c17001bf602d/68747470733a2f2f736875626874756c732e6769746875622e696f2f666163746f72656433642f7265736f75726365732f696d616765732f7465617365722e706e67" data-canonical-src="https://shubhtuls.github.io/factored3d/resources/images/teaser.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">LayoutNet：从单个 RGB 图像重建 3D 房间布局 (2018 CVPR) </font></font></b> <a href="https://arxiv.org/pdf/1803.08999.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/zouchuhang/LayoutNet"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/63a9635fb0e2d39d5dd70034e4762717fa202d79dca88ab41240698267d42bb2/687474703a2f2f70302e6966656e67696d672e636f6d2f706d6f702f323031382f303430342f413144304341453438313330433931384645363234464136303439354632333743363731373246365f73697a6536335f773739375f683735352e6a706567"><img width="50%" src="https://camo.githubusercontent.com/63a9635fb0e2d39d5dd70034e4762717fa202d79dca88ab41240698267d42bb2/687474703a2f2f70302e6966656e67696d672e636f6d2f706d6f702f323031382f303430342f413144304341453438313330433931384645363234464136303439354632333743363731373246365f73697a6536335f773739375f683735352e6a706567" data-canonical-src="http://p0.ifengimg.com/pmop/2018/0404/A1D0CAE48130C918FE624FA60495F237C67172F6_size63_w797_h755.jpeg" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">PlaneNet：从单个 RGB 图像进行分段平面重建 (2018 CVPR) </font></font></b> <a href="http://art-programmer.github.io/planenet/paper.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="http://art-programmer.github.io/planenet.html" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a></p>
<p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/e73a6d7615c1cfb5353308f4d6ccd3983d949e4e644c5b12e8e031d2c1ad2782/687474703a2f2f6172742d70726f6772616d6d65722e6769746875622e696f2f696d616765732f706c616e656e65742e706e67"><img width="50%" src="https://camo.githubusercontent.com/e73a6d7615c1cfb5353308f4d6ccd3983d949e4e644c5b12e8e031d2c1ad2782/687474703a2f2f6172742d70726f6772616d6d65722e6769746875622e696f2f696d616765732f706c616e656e65742e706e67" data-canonical-src="http://art-programmer.github.io/images/planenet.png" style="max-width: 100%;"></a></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">使用合成图像的跨域自监督多任务特征学习（2018 CVPR）</font></font></b> <a href="http://web.cs.ucdavis.edu/~yjlee/projects/cvpr2018.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a> </p><p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/6cd36088633707d0e3975ef7411223a490f8e061467f9cb52a718e88f3adbc17/68747470733a2f2f6a61736f6e3731382e6769746875622e696f2f70726f6a6563742f6376707231382f66696c65732f636f6e636570745f7069632e706e67"><img width="50%" src="https://camo.githubusercontent.com/6cd36088633707d0e3975ef7411223a490f8e061467f9cb52a718e88f3adbc17/68747470733a2f2f6a61736f6e3731382e6769746875622e696f2f70726f6a6563742f6376707231382f66696c65732f636f6e636570745f7069632e706e67" data-canonical-src="https://jason718.github.io/project/cvpr18/files/concept_pic.png" style="max-width: 100%;"></a></p><p dir="auto"></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pano2CAD：来自单个全景图像的房间布局（2018 CVPR）</font></font></b> <a href="http://bjornstenger.github.io/papers/xu_wacv2017.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a> </p><p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/0fc35770f2a5242f7f86519ad3b1554188efadaaf96dcde87ea261d4caf95e8c/68747470733a2f2f7777772e67726f756e6461692e636f6d2f6d656469612f61727869765f70726f6a656374732f35383932342f666967757265732f436f6d706172655f32622e706e67"><img width="50%" src="https://camo.githubusercontent.com/0fc35770f2a5242f7f86519ad3b1554188efadaaf96dcde87ea261d4caf95e8c/68747470733a2f2f7777772e67726f756e6461692e636f6d2f6d656469612f61727869765f70726f6a656374732f35383932342f666967757265732f436f6d706172655f32622e706e67" data-canonical-src="https://www.groundai.com/media/arxiv_projects/58924/figures/Compare_2b.png" style="max-width: 100%;"></a></p><p dir="auto"></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">单全景自动 3D 室内场景建模 (2018 CVPR) </font></font></b> <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_Automatic_3D_Indoor_CVPR_2018_paper.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a> </p><p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timzhang642/3D-Machine-Learning/blob/master/imgs/Automatic%203D%20Indoor%20Scene%20Modeling%20from%20Single%20Panorama%20(2018%20CVPR).jpeg"><img width="50%" src="https://github.com/timzhang642/3D-Machine-Learning/raw/master/imgs/Automatic%203D%20Indoor%20Scene%20Modeling%20from%20Single%20Panorama%20(2018%20CVPR).jpeg" style="max-width: 100%;"></a></p><p dir="auto"></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">通过关联嵌入进行单图像分段平面 3D 重建 (2019 CVPR) </font></font></b> <a href="https://arxiv.org/pdf/1902.09777.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/svip-lab/PlanarReconstruction"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a> </p><p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/svip-lab/PlanarReconstruction/blob/master/misc/pipeline.jpg"><img width="50%" src="https://github.com/svip-lab/PlanarReconstruction/raw/master/misc/pipeline.jpg" style="max-width: 100%;"></a></p><p dir="auto"></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">通过反向图形进行 3D 感知场景操作 (NeurIPS 2018) </font></font></b> <a href="http://3dsdn.csail.mit.edu/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文] </font></font></a> <a href="https://github.com/svip-lab/PlanarReconstruction"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[代码]</font></font></a> </p><p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/ed00812492a481fa74c261e308e2ba77905f6957ef939efc7666cea23f9722a6/687474703a2f2f336473646e2e637361696c2e6d69742e6564752f696d616765732f7465617365722e706e67"><img width="50%" src="https://camo.githubusercontent.com/ed00812492a481fa74c261e308e2ba77905f6957ef939efc7666cea23f9722a6/687474703a2f2f336473646e2e637361696c2e6d69742e6564752f696d616765732f7465617365722e706e67" data-canonical-src="http://3dsdn.csail.mit.edu/images/teaser.png" style="max-width: 100%;"></a></p><p dir="auto"></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">💎</font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">使用多层深度和极线变压器进行 3D 场景重建（ICCV 2019）</font></font></b> <a href="https://research.dshin.org/iccv19/multi-layer-depth/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a> </p><p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/4703f50f2ad2c1eb61663016b5b009387928ea73fdbaa0b2424144eafde373ba/68747470733a2f2f72657365617263682e647368696e2e6f72672f6963637631392f6d756c74692d6c617965722d64657074682f666967757265732f6f766572766965775f312e706e67"><img width="50%" src="https://camo.githubusercontent.com/4703f50f2ad2c1eb61663016b5b009387928ea73fdbaa0b2424144eafde373ba/68747470733a2f2f72657365617263682e647368696e2e6f72672f6963637631392f6d756c74692d6c617965722d64657074682f666967757265732f6f766572766965775f312e706e67" data-canonical-src="https://research.dshin.org/iccv19/multi-layer-depth/figures/overview_1.png" style="max-width: 100%;"></a><br><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/b041c0bdea40204a2004bbfa1c674bcc97d5b3d30bb3d2a10b17997de8e6c7be/68747470733a2f2f72657365617263682e647368696e2e6f72672f6963637631392f6d756c74692d6c617965722d64657074682f666967757265732f766f78656c697a6174696f6e30302e6a7067"><img width="50%" src="https://camo.githubusercontent.com/b041c0bdea40204a2004bbfa1c674bcc97d5b3d30bb3d2a10b17997de8e6c7be/68747470733a2f2f72657365617263682e647368696e2e6f72672f6963637631392f6d756c74692d6c617965722d64657074682f666967757265732f766f78656c697a6174696f6e30302e6a7067" data-canonical-src="https://research.dshin.org/iccv19/multi-layer-depth/figures/voxelization00.jpg" style="max-width: 100%;"></a></p><p dir="auto"></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">PerspectiveNet：通过透视点从单个 RGB 图像检测 3D 对象（NIPS 2019）</font></font></b> <a href="https://papers.nips.cc/paper/9093-perspectivenet-3d-object-detection-from-a-single-rgb-image-via-perspective-points.pdf" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文]</font></font></a> </p><p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/1e5083a1dfffc64c2ed076901d692193daf96b406d76a8db60474e48a8fa085e/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f67726f756e6461692d7765622d70726f642f6d656469612f75736572732f757365725f3238383033362f70726f6a6563745f3430323335382f696d616765732f78312e706e67"><img width="50%" src="https://camo.githubusercontent.com/1e5083a1dfffc64c2ed076901d692193daf96b406d76a8db60474e48a8fa085e/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f67726f756e6461692d7765622d70726f642f6d656469612f75736572732f757365725f3238383033362f70726f6a6563745f3430323335382f696d616765732f78312e706e67" data-canonical-src="https://storage.googleapis.com/groundai-web-prod/media/users/user_288036/project_402358/images/x1.png" style="max-width: 100%;"></a></p><p dir="auto"></p>
<p dir="auto"><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Holistic++ 场景理解：单视图 3D 整体场景解析和人体姿势估计以及人与物体交互和物理常识（ICCV 2019）</font></font></b> <a href="https://github.com/yixchen/holistic_scene_human"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[论文和代码]</font></font></a> </p><p align="center" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/e1d6606312660fa897e19cc39114b598c5d2b1933e56ec8377db5cd7f1dd48af/68747470733a2f2f7969786368656e2e6769746875622e696f2f686f6c697374696370702f66696c652f70672e706e67"><img width="50%" src="https://camo.githubusercontent.com/e1d6606312660fa897e19cc39114b598c5d2b1933e56ec8377db5cd7f1dd48af/68747470733a2f2f7969786368656e2e6769746875622e696f2f686f6c697374696370702f66696c652f70672e706e67" data-canonical-src="https://yixchen.github.io/holisticpp/file/pg.png" style="max-width: 100%;"></a></p><p dir="auto"></p>
</article></div>
